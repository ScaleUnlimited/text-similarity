<CACHeegPaGjTMDrdjLrshfU9KHMC0iMY1F9MbYDk==ThvoM97Dw@mail.gmail.com>	Agata Filiana 	a.filiana87@gmail.com	Evaluating Recommender System	2013-06-01T18:17:30Z		Hi, I'm trying to evaluate my recommender system in Mahout.\nI have boolean data and I'm using *GenericBooleanPrefUserBasedRecommender*\nI used precision and recall and it's returning *throwing\njava.lang.IllegalArgumentException: Illegal nDCG: NaN**\n*\n*\n*\nDoes this mean I don't have enough data to evaluate? Is there any way to do\nevaluation on this data?\n\nThank you\n*\nAgata Filiana\n**http://about.me/dewtraveller* <http://about.me/dewtraveller>*\n*\n \n\n
<3E5344DE-E18E-4B23-B49C-13570EBB8248@dhuebner.com>	Dominik Hübner 	contact@dhuebner.com	Re: Blending initial recommendations for cross recommendation	2013-06-01T19:19:52Z	<608B35DF-14C5-431F-B8FD-2D2365BDA1C7@gmail.com>	Thanks for those detailed responses!\nSo I assume that the problem of scaling the initial recommendations can be implicitly solved by learning the weights for the linear combination. The log rank Ted mentioned seems to be useful as well!\n\nWhat metric do you use to evaluate your recommendations? I currently setup a cooccurrence matrix to predict based on purchases only, a Purchase X View matrix and a view only matrix.\nI am trying to recommend products leading people to purchases. So I somehow need to find a way, to distinct between types recommendations and prefer "purchase" recommendations. Is there any common approach to evaluate recommendations in such settings? Maybe I just didn't get what you meant with: "In my case I weight the entire row vector, add the two, resort, and calculate precision @ some number of recs." \nUsing a percentile rank as shown in the paper for implicit ALS might be useful here. The problem nevertheless is, that most users only have a single purchase.\n\nFurthermore, do you really perform gradient descent learning of the weights using hadoop/mahout? Isn't this too costly to perform due to the overheads of the JVM and hadoop?\n\n\n\nOn Jun 1, 2013, at 1:21 AM, Pat Ferrel <pat.ferrel@gmail.com> wrote:\n\n> I've got a cross-recommender too. It was originally conceived to do a multi-action ensemble from Ted's notes. I'm now gathering a new data set and building the meta-model learner.\n> \n> Even with the same scale you need to learn the weighting factors. Consider a simple ensemble case:\n> \n> R_p is the matrix of all recommendations from an item-base recommender using history for any single action (purchase?)\n> S_p is the matrix of all similar items for a given item as measured by user actions (purchases?)\n> \n> It is highly likely that you can improve your ranking (also agree ratings are seldom useful) by incorporating both sets of data. So R = R_p + aS_p This is the linear combo Ted mentions. But what is "a"? The scale of the strengths here are very similar but their importance is not. In my case I found that S_p was far more predictive of a future purchase than R_p but without R_p every user sees the same recommendations for a given item. So some combo of the two may prove far stronger in practice than either alone. The combo certainly has produced a better precision score.\n> \n> Once you learn "a" things are much simpler but to learn it you need an objective function like improving precision. Then you must find what weighting "a" gives the highest precision. This requires virtually the entire of R_p and S_p. Since you don't know "a" you don't know how much of the sorted R or S you need to combine. Using the Solr idea you would combine as many results as you can get from the queries trying different weightings and measuring precision each time. In my case I weight the entire row vector, add the two, resort, and calculate precision @ some number of recs.\n> \n> Implementing a simple form of gradient ascent seems like a good approach because the full ensemble is going to have more than just "a" and therefore need help converging across multiple dimensions.\n> \n> The version of the cross-recommender I have considers item similarities as well as user history based recommendations we have an ensemble something like:\n> \n> R_p + aS_p + bR_v + cS_v = R \n> here R_v and S_v are from the cooccurrences of views with purchases, not just views. \n> \n> Ted's idea below about using the rank or log rank instead of whatever the similarity metric returns as the "strength" seems like a really good one. It evens out the scales and so may converge quicker if nothing else. Pretty easy to implement too.\n> \n> \n> Repetition alert! Does anyone have a data set to try this on?\n> \n> \n> \n> On May 31, 2013, at 1:15 PM, Koobas <koobas@gmail.com> wrote:\n> \n> Ted,\n> Thank you very much. This is very insightful.\n> The log scaling is definitely an intuitive way of building the meta model.\n> Not much disagreement about the uselessness of predicting ratings.\n> \n> \n> On Fri, May 31, 2013 at 4:00 PM, Ted Dun ning <ted.dunning@gmail.com> wrote:\n> \n>> In my case, I put all the indicators from all different sources in the same\n>> Solr/Lucene index.  Recommendations consists of making a single query to\n>> Solr/Lucene with as much data as I have or want to include.\n>> \n>> At the point that this query is done, there are no weights on the\n>> indicators ... merely presence or absence in a field or query.  The weights\n>> that I typically use are computed on the fly by Lucene's default similarity\n>> score and the results tend to be very good.  There is no issue of combining\n>> scores on different scales since there is only one composite score.\n>> \n>> If you *really* want to build multiple models using different technologies\n>> and combine them, you need a so-called meta-model.  There are many ways to\n>> build such a beast.  A very simple way is to reduce all scores to quantiles\n>> then to a log-odds scale (taking care not to ever estimate a quantile as\n>> either 0 or 1).  A linear combination of these rescaled scores can work\n>> pretty well although you do have to learn the linear weights.\n>> \n>> Sometimes scores vary strongly from query to query.  In such cases,\n>> reducing a score to being some kind of rank statistic can be helpful. For\n>> instance, you may want to have a score that is the log of the rank that an\n>> item appears at in the results list.  You might also be able to normalize\n>> scores based on properties of the query. Such rank-based or normalized\n>> scores can then often be combined by any meta-model, including the one I\n>> mentioned above.\n>> \n>> You should also look at the netflix papers, especially the one describing\n>> the winning entry for more ideas on model combination.  The major\n>> difference there is that they were trying to predict a rating which is a\n>> task that I find essentially useless since ranking is so much more\n>> important in most real-world applications.  Others may dispute my\n>> assessment on this, of course.\n>> \n>> There are many ways of building the meta-model that you need, but one\n>> over-riding thought that I have is that the deviations from ideal in all\n>> real cases will be large enough that theory should not be taken too\n>> literally here, but rather should be used as a weak, though still useful,\n>> inspirational guide.\n>> \n>> \n>> On Fri, May 31, 2013 at 3:18 PM, Koobas <koobas@gmail.com> wrote:\n>> \n>>> I am also very interested in the answer to this question.\n>>> Just to reiterate, if you use different recommenders, e.g.,\n>>> kNN user-based, kNN item-based, ALS, each one produces\n>>> recommendations on a different scale. So how do you combine them?\n>>> \n>>> \n>>> On Fri, May 31, 2013 at 3:07 PM, Dominik Hübner <contact@dhuebner.com\n>>>> wrote:\n>>> \n>>>> Hey,\n>>>> I have implemented a cross recommender based on the approach Ted\n>> Dunning\n>>>> proposed (cannot find the original post, but here is a follow up\n>>>> http://www.mail-archive.com/user@mahout.apache.org/msg12983.html).\n>>>> Currently I am struggling with the last step of blending the initial\n>>>> recommendations.\n>>>> \n>>>> My current approach:\n>>>> 1. Compute a cooccurrence matrix for each useful combination of\n>>>> user-product interaction (e.g. which product views and purchased do\n>>> appear\n>>>> in common …)\n>>>> 2. Perform initial recommendation based on each matrix and the required\n>>>> type of user vector (e.g. a user's history of views OR purchases) (like\n>>> the\n>>>> item-based recommender implemented in Mahout)\n>>>> \n>>>> In step 2, I adapted the AggregateAndRecommendReducer of Mahout, which\n>>>> normalizes vectors while building the sum of weighted similarities or\n>> in\n>>>> this case => cooccurrences.\n>>>> \n>>>> Now I end up with multiple recommendations for each product, but all of\n>>>> them are on a different scale.\n>>>> How can I convert them to have the same scale, in order to be able to\n>>>> weight them and build the linear combinations of initial\n>> recommendations\n>>> as\n>>>> Ted proposed?\n>>>> Would it make sense to normalize use r vectors (before multiplying) as\n>>> well?\n>>>> \n>>>> Otherwise views would have a much higher influence than purchases due\n>> to\n>>>> their plain characteristics (they just appear way more frequently). Or\n>> is\n>>>> this the reason for weighting purchases higher and views lower? If so,\n>> I\n>>>> think it's sort of inconvenient. Wouldn't it be much more favorable to\n>>> get\n>>>> each type of interaction within the same scale and use the weights just\n>>> to\n>>>> control each types influence on the final recommendation?\n>>>> \n>>>> Thanks in advance for any suggestions!\n>>>> \n>>>> \n>>>> \n>>>> Regards\n>>>> Dominik\n>>>> \n>>>> Sent from my iPhone\n>>> \n>> \n> \n\n \n\n
<CAEccTyzf6n6ZEfvrPPpQhDdQMVPmLPokoPBOyCwFm35EXKM1-g@mail.gmail.com>	Sean Owen 	srowen@gmail.com	Re: evaluating recommender with boolean prefs	2013-06-07T21:38:24Z	<CADe2zZqJ+bxCy+zObKjQ+OB7XyGeoyguKy1m_Fnbg945hxwH+Q@mail.gmail.com>	Yes it makes sense in the case of for example ALS.\nWith or without this idea, the more general point is that this result\nis still problematic. It is somewhat useful in comparing in a relative\nsense; I'd rather have a recommender that stacks my input values\nsomewhere near the top than bottom. But metrics like precision@5 get\nhard to interpret -- because they are often near 0 even when things\nare working reasonably well. Mean average precision considers the\nresults in a more complete sense, as would AUC.\n\nOn Fri, Jun 7, 2013 at 10:04 PM, Koobas <koobas@gmail.com> wrote:\n> On Fri, Jun 7, 2013 at 4:50 PM, Sean Owen <srowen@gmail.com> wrote:\n>\n>> It depends on the algorithm I suppose. In some cases, the\n>> already-known items would always be top recommendations and the test\n>> would tell you nothing. Just like in an RMSE test -- if you already\n>> know the right answers your score is always a perfect 0.\n>>\n>> It's very much to the point.\n> ALS works by constructing a low-rank approximation of the original matrix.\n> We check how good that approximation it by comparing it against the\n> original.\n>\n> I see an analogy here, in the case of kNN.\n> The suggestions are a model of your interests, in a sense can be used to\n> reconstruct\n> your original set.\n>\n>\n>> But in some cases I agree you could get some of use out of observing\n>> where the algorithm ranks known associations, because they won't in\n>> some cases all be the very first ones.\n>>\n>> it raises an interesting question: if the top recommendation wasn't an\n>> already known association, how do we know it's "wrong"? We don't. You\n>> rate Star Trek, Star Trek V, and Star Trek IV. Say Star Trek II is\n>> your top recommendation. That's actually probably right, and should be\n>> ranked higher than all your observed associations. (It's a good\n>> movie.) But the test would consider it wrong. In fact anything that\n>> you haven't interacted with before is "wrong".\n>>\n>> You can look at it from the other side.\n> It's not about the ones that are not in your original set.\n> It's about how good the recommender is in putting back the original, if\n> they were removed.\n> Except we would not actually be removing them.\n> It's the same approach, simply without splitting the input into the\n> training set and the validation set.\n> In a sense the whole set is the training set and the validation set.\n> Again, I am not coming from the ML background.\n> Am I making sense here?\n>\n>\n>> This sort of explains why precision/recall can be really low in these\n>> tests. I would not be surprised if you get 0 in some cases, on maybe\n>> small input. Is it a bad predictor? maybe, but it's not clear.\n>>\n>>\n>>\n>> On Fri, Jun 7, 2013 at 8:06 PM, Koobas <koobas@gmail.com> wrote:\n>> > Since I am primarily an HPC person, probably a naive question from the ML\n>> > perspective.\n>> > What if, when computing recommendations, we don't exclude what the user\n>> > already has,\n>> > and then see if the items he has end up being recommended to him (compute\n>> > some appropriate metric / ratio)?\n>> > Wouldn't that be the ultimate evaluator?\n>> >\n>> >\n>> > On Fri, Jun 7, 2013 at 2:58 PM, Sean Owen <srowen@gmail.com> wrote:\n>> >\n>> >> In point 1, I don't think I'd say it that way. It's not true that\n>> >> test/training is divided by user, because every user would either be\n>> >> 100% in the training or 100% in the test data. Instead you hold out\n>> >> part of the data for each user, or at least, for some subset of users.\n>> >> Then you can see whether recs for those users match the held out data.\n>> >>\n>> >> Yes then you see how the held-out set matches the predictions by\n>> >> computing ratios that give you precision/recall.\n>> >>\n>> >> The key question is really how you choose the test data. It's implicit\n>> >> data; one is as good as the next. In the framework I think it just\n>> >> randomly picks a subset of the data. You could also split by time;\n>> >> that's a defensible way to do it. Training data up to time t and test\n>> >> data after time t.\n>> >>\n>> >> On Fri, Jun 7, 2013 at 7:51 PM, Michael Sokolov\n>> >> <msokolov@safaribo oksonline.com> wrote:\n>> >> > I'm trying to evaluate a few different recommenders based on boolean\n>> >> > preferences.  The in action book suggests using an precision/recall\n>> >> metric,\n>> >> > but I'm not sure I understand what that does, and in particular how\n>> it is\n>> >> > dividing my data into test/train sets.\n>> >> >\n>> >> > What I think I'd like to do is:\n>> >> >\n>> >> > 1. Divide the test data by user: identify a set of training data with\n>> >> data\n>> >> > from 80% of the users, and test using the remaining 20% (say).\n>> >> >\n>> >> > 2. Build a similarity model from the training data\n>> >> >\n>> >> > 3. For the test users, divide their data in half; a "training" set\n>> and an\n>> >> > evaluation set.  Then for each test user, use their training data as\n>> >> input\n>> >> > to the recommender, and see if it recommends the data in the\n>> evaluation\n>> >> set\n>> >> > or not.\n>> >> >\n>> >> > Is this what the precision/recall test is actually doing?\n>> >> >\n>> >> > --\n>> >> > Michael Sokolov\n>> >> > Senior Architect\n>> >> > Safari Books Online\n>> >> >\n>> >>\n>>\n\n \n
<CAEccTyzrSru=2dA4kScAtqSpVRkRAY3fZiVy0SzeCOH2nPC5sg@mail.gmail.com>	Sean Owen 	srowen@gmail.com	Re: evaluating recommender with boolean prefs	2013-06-07T21:41:33Z	<86024921ECCEE245B5F041F56B954F96148CB79122@EMV05-UKBR.domain1.systemhost.net>	I believe the suggestion is just for purposes of evaluation. You would\nnot return these items in practice, yes.\n\nAlthough there are cases where you do want to return known items. For\nexample, maybe you are modeling user interaction with restaurant\ncategories. This could be useful, because as soon as you see I\ninteract with "Chinese" and "Indian" you may recommend "Thai"; it\nmight even be a stronger recommendation than the two known categories.\nBut I may not want to actually exclude Chinese and Indian from the\nlist entirely.\n\nOn Fri, Jun 7, 2013 at 10:36 PM,  <simon.2.thompson@bt.com> wrote:\n> But why would she want the things she has?\n\n \n
<CA+mgvOjRF7gNfUnrsuzRX4qD014WJPOcvm9onHMQX=dG0wkdAw@mail.gmail.com>	Peter Holland 	d99991048@mydit.ie	Social Network Link Prediction in Mahout	2013-06-08T13:15:32Z		Hi All,\nI am trying to use Mahout for Link Prediction in a Social Network.\n\nThe data I have is an edges list with 9.4 million rows. The edge list is a\ncsv vile where each node is an integer value and a row represents a edge\nbetween two nodes. For example;\n\n3432, 5098\n3423, 6710\n4490, 5843\n4490, 2039\n.....\n\nThis is a directed graph so row 1 means that node 3432 follows node 5098.\n\nI would like to build a recommender to calculate the top 10 nodes a user\nmight like to connect to next. The problem I have is that the recommender\nclasses needs input in the form (user, item, value).  So, how can I first\ncalculate a value to represent the 'weight' of an edge? For example\nEdgeRank?\n\nAny help would be greatly appreciated.\nThank you,\nPeter\n \n\n
<CAEccTyyNj1H4zCcP__zMC41ciC6mr94WN-OjWAzqJ_DJYWuTxQ@mail.gmail.com>	Sean Owen 	srowen@gmail.com	Re: Social Network Link Prediction in Mahout	2013-06-08T13:20:57Z	<CA+mgvOjRF7gNfUnrsuzRX4qD014WJPOcvm9onHMQX=dG0wkdAw@mail.gmail.com>	Use an implementation that doesn't expect a rating. These are\nso-called 'boolean' implementations, like GenericBooleanPrefDataModel.\nFor example you can build and item-based recommender with the boolean\nversion of item based recommender and a log-likelihood similarity.\n\nOr, yes you can calculate some meaningful edge weight to add more info\nto your model. Maybe the number of times the two users interacted? the\nresulting number can be used as a 'rating' although I don't know if\nyou will get great results since it doesn't act a lot like a rating.\nInstead, use the log of this number.\n\nOr, use an algorithm that is comfortable with count-like input, like\nALS with the "implicit data" option turned on.\n\nSean\n\nOn Sat, Jun 8, 2013 at 2:15 PM, Peter Holland <d99991048@mydit.ie> wrote:\n> Hi All,\n> I am trying to use Mahout for Link Prediction in a Social Network.\n>\n> The data I have is an edges list with 9.4 million rows. The edge list is a\n> csv vile where each node is an integer value and a row represents a edge\n> between two nodes. For example;\n>\n> 3432, 5098\n> 3423, 6710\n> 4490, 5843\n> 4490, 2039\n> .....\n>\n> This is a directed graph so row 1 means that node 3432 follows node 5098.\n>\n> I would like to build a recommender to calculate the top 10 nodes a user\n> might like to connect to next. The problem I have is that the recommender\n> classes needs input in the form (user, item, value).  So, how can I first\n> calculate a value to represent the 'weight' of an edge? For example\n> EdgeRank?\n>\n> Any help would be greatly appreciated.\n> Thank you,\n> Peter\n\n \n
<548ACFBE-46C9-40B6-A43D-028D9C1823E4@gmail.com>	Ted Dunning 	ted.dunning@gmail.com	Re: Why are clustering emails not clustering similar stuff?	2013-06-08T13:25:08Z	<CAD5r=q3nBdgEQ_qUoFmw9buRrMn=k62c13gkhUewcZkrGf=NQw@mail.gmail.com>	How are you verifying your vectorization?\n\nWhat do you use for weighting of words?\n\nHave you tested the distance between the notifications and other documents?  Are closely duplicate documents close to each other? \n\nSent from my iPhone\n\nOn Jun 6, 2013, at 7:47, Jesvin Jose <frank.einstien@gmail.com> wrote:\n\n> I tried to cluster 1000 emails of a person using Kmeans, but clusters are\n> not forming okay. For example if Facebook sends notifications about James\n> Doe and 5 other people, I get 5 clusters like:\n> \n> :VL-858{n=7\n>    Top Terms:\n>        doe                                   =>  10.066998481750488\n>        james                                =>  10.066998481750488\n> \n> Why are notifications for all 5 people not getting clustered together? I\n> used variants of the commands used in Mahout in Action, Sean Owen et al as\n> follows:\n> \n> Vectorizing uses lowercasing, stop words and length filter:\n> \n> bin/hadoop jar\n> /home/jesvin/dev/hadoop/mahout-distribution-0.7/examples/target/mahout-examples-0.7-job.jar\n> org.apache.mahout.driver.MahoutDriver seq2sparse -i mymail-seqfiles -o\n> mymail-vectors-bigram -ow  -a mia.clustering.ch10.MyAnalyzer -chunk 200 -wt\n> tfidf -s 5 -md 3 -x 90 -ng 2 -ml 50 -seq\n> \n> Its for 1000 emails, but I tried 100 clusters. If I tried 50, I still get\n> similar results but half the number of emails "get into" any cluster.\n> \n> bin/hadoop jar\n> /home/jesvin/dev/hadoop/mahout-distribution-0.7/examples/target/mahout-examples-0.7-job.jar\n> org.apache.mahout.driver.MahoutDriver kmeans -i\n> mymail-vectors-bigram/tfidf-vectors -c mymail-initial-clusters -o\n> mymail-kmeans-clusters-from-bigrams -dm\n> org.apache.mahout.common.distance.CosineDistanceMeasure -cd 0.1 -k 100 -x\n> 20 -cl\n> \n> -- \n> We dont beat the reaper by living longer. We beat the reaper by living well\n> and living fully. The reaper will come for all of us. Question is, what do\n> we do between the time we are born and the time he shows up? -Randy Pausch\n\n \n
<CAEccTywtkSPALeMjDXZu7M8s33unba_Y6=ugX_0XMY+w1j3H+Q@mail.gmail.com>	Sean Owen 	srowen@gmail.com	Re: Error Running mahout-core-0.5-job.jar	2012-03-21T18:19:35Z	<1332353507358-3846385.post@n3.nabble.com>	It's -Dmapred.output.dir=output not --Dmapred.output.dir=output (one dash),\nbut, that's not even the problem.\nI don't think you can specify -D options this way, as they are JVM\narguments. You need to configure these in Hadoop's config files.\nThis is not specific to Mahout.\n\nOn Wed, Mar 21, 2012 at 6:11 PM, jeanbabyxu <jessica.xu@aexp.com> wrote:\n\n> I tried to run mahout in Hadoop using the following command,\n>\n> [jxu13@lppma692 hadoop-0.20.2]$ bin/hadoop jar\n> /opt/mapr/mahout/mahout-0.5/core/target/mahout-core-0.5-job.jar\n> org.apache.mahout.cf.taste.hadoop.ite\n> m.RecommenderJob -Dmapred.input.dir=input/input.txt\n> --Dmapred.output.dir=output --usersFile input/users.txt --booleanData\n>\n> But got the following error msg:\n> 12/03/21 10:55:59 ERROR common.AbstractJob: Unexpected\n> --Dmapred.output.dir=output while processing Job-Specific Options:\n> usage: <command> [Generic Options] [Job-Specific Options]\n>\n> I have copied in the input and users files to HDFS:\n> [jxu13@lppma692 hadoop-0.20.2]$ bin/hadoop fs -ls /user/jxu13/input\n> Found 2 items\n> -rwxrwxrwx   3 jxu13 rimegg          1 2012-03-20 12:08\n> /user/jxu13/input/users.txt\n> -rwxrwxrwx   3 jxu13 rimegg       1732 2012-03-20 11:58\n> /user/jxu13/input/input.txt\n>\n> What is the default output directory: is it /user/jxu13/output?\n>\n> [jxu13@lppma692 hadoop-0.20.2]$ bin/hadoop fs -ls /user/jxu13/output\n> ls: Cannot access /user/jxu13/output: No such file or directory.\n>\n> Is this a Hadoop set-up/configuration problem?\n>\n> Any advice would be appreciated.\n>\n>\n> --\n> View this message in context:\n> http://lucene.472066.n3.nabble.com/Error-Running-mahout-core-0-5-job-jar-tp3846385p3846385.html\n> Sent from the Mahout User List mailing list archive at Nabble.com.\n>\n \n\n
<CACpbbiJC=TX79Ks0eKZAtEgX7rZ95PB3Up5j0PHnzfbO8HxaSQ@mail.gmail.com>	Baoqiang Cao 	bqcaomail@gmail.com	Re: can't get <point-id, cluster-id> thru "-p"	2012-03-21T19:06:02Z	<4F68A3B8.7080905@occamsmachete.com>	This is extremely helpful! Thanks a lot.\nIndeed, after seqdumper I got such:\n\nKey: 1774184: Value: wt: 1.0distance: 0.8839410915753125  vec: [123426:1.000]\nKey: 1705919: Value: wt: 1.0distance: 0.0  vec: []\nKey: 1705919: Value: wt: 1.0distance: 0.0  vec: []\nKey: 1705919: Value: wt: 1.0distance: 0.0  vec: []\n\nAnd after redid the seq2sparse part with "-nv", I got the postid as\nwell. So, it is clear that due to stop words and rare occurrence,\nsome documents end up with no words left--empty documents. That is why\nI got "vec: []". Is there any suggestion dealing with them? Leave as\nis so always be aware of one trivial cluster (empty vectors)? Or...\n\nThanks again for wonderful help!\n\n\nOn Tue, Mar 20, 2012 at 10:35 AM, Pat Ferrel <pat@occamsmachete.com> wrote:\n> What I have done is to use "seq2sparse -nv" to get named vectors.\n>\n>   *mahout seq2sparse \\n>       -i reuters-seqfiles/ \\n>       -o reuters-vectors/ \\n>       -ow -chunk 100 \\n>       -x 90 \\n>       -seq \\n>       -a com.finderbots.analyzers.LuceneStemmingAnalyzer \\n>       -ml 50 \\n>       -n 2 \\n>       -nv*\n>\n> This will use the filename as the key in the vector sequence file. The keys\n> will remain the same through the clustering phase.\n>\n> Run "kmeans -cl" to get the clusteredPoint dir created and in it you will\n> find a part-m-00000\n>\n>   *mahout kmeans \\n>       -i reuters-vectors/tfidf-vectors/ \\n>       -c reuters-kmeans-centroids \\n>       -cl \\n>       -o reuters-kmeans-clusters \\n>       -k 20 \\n>       -ow \\n>       -x 10 \\n>       -dm org.apache.mahout.common.distance.CosineDistanceMeasure *\n>\n> Use seqdumper:\n>\n>   mahout seqdumper -s\n>   reuters-kmeans-clusters/clusteredPoints/part-m-00000 | more\n>\n> You will see that the file contains\n>\n>   key: clusterid, value: wt = % likelihood the vector is in cluster,\n>   distance from centroid, named vector belonging to the cluster,\n>   vector data.\n>\n> For kmeans the likelihood will be 1.0 or 0. For example:\n>\n>   Key: 21477: Value: wt: 1.0distance: 0.9420744909793364  vec:\n>   /-tmp/reut2-000.sgm-158.txt = [372:0.318, 966:0.396, 3027:0.230,\n>   8816:0.452, 8868:0.308, 13639:0.278, 13648:0.264, 14334:0.270,\n>   14371:0.413]\n>\n> Clusters, of course, cannot have names. A simple solution is to construct a\n> name from the top terms in the centroid output from clusterdump.\n>\n> I also recommend /Mahout in Action/ from Manning Publishing. You can buy it\n> here:\n> http://manning.com/owen/\n>\n>\n> On 3/19/12 7:45 PM, Baoqiang Cao wrote:\n>>\n>> Thanks again for the reference!\n>>\n>> One more question if you don't mind. How do I get the text keys for\n>> each cluster? I mean, so at beginning we have input file for\n>> seq2sparse, and we knew the format is\n>>\n>> textkey1 text1\n>> textkey2 text2\n>> ..\n>>\n>> after mahout kmeans, and clusterdump, how do I know, for example,\n>> which texts belong to cluster 1 from command line? I thought the\n>> outputs from clusterdump have what I want, but so far, it is still\n>> elusive.\n>>\n>> Best,\n>> Baoqiang\n>>\n>>\n>>\n>> On Mon, Mar 19, 2012 at 4:01 PM, Pat Ferrel<pat@occamsmachete.com>  wrote:\n>>>\n>>> I guess you figured this out but the cluster drivers take "-cl", which\n>>> tells\n>>> them to put points into the calculated clusters and output to the\n>>> clusterPoints directory. Then you pass that in to clusterdump.\n>>>\n>>> instructions here:\n>>> https://cwiki.apache.org/confluence/display/MAHOUT/K-Means+Clustering\n>>>\n>>> the --help for the mahout cluster drivers is incomplete, check cwiki for\n>>> differences\n>>>\n>>>\n>>>\n>>> On 3/14/12 3:18 PM, Baoqiang Cao wrote:\n>>>>\n>>>> Thanks a lot. But I don't know if I miss anything in front of my teary\n>>>> eyes because of Wednesday afternoon or ? I have equivalent inputs as\n>>>> yours:\n>>>>\n>>>> mahout clusterdump -s /mahout/kmeans/clusters-15-final -d\n>>>> /mahout/sparse/dictionary.file-0 -dt sequencefile   -p /mahout/points\n>>>>\n>>>> the cluster files after 15 iterations are\n>>>> /mahout/kmeans/clusters-15-final. /mahout/points is  a directory I\n>>>> created in prior. On screen, the output are something like\n>>>> "VL-1721020{n=186 c=[...". It just is no any output files under that\n>>>> directory.\n>>>>\n>>>> Any help , please\n>>>>\n>>>> On Wed, Mar 14, 2012 at 2:13 PM, Pat Ferrel<pat@occamsmachete.com>\n>>>>  wrote:\n>>>>>\n>>>>> The -p parameter is an input. You should pass in the clusterPoints/\n>>>>> directory that was generated by the cluster driver you used.\n>>>>>\n>>>>> My use of fkmeans might be an example:\n>>>>>\n>>>>>   mahout fkmeans -i wikipedia-vectors/tfidf-vectors/ -c\n>>>>>   wikipedia-fkmeans-centroids -o wikipedia-fkmeans-clusters -k 100 -m\n>>>>>   2 -ow -x 10 -dm\n>>>>> org.apache.mahout.common.distance.CosineDistanceMeasure\n>>>>>\n>>>>> This will create\n>>>>> wikipedia-clusters/clusters/clusteredPoints/part-m-00000\n>>>>> which is the file with the clustered points. I then did a clusterdump\n>>>>>\n>>>>>   mahout clusterdump -s\n>>>>>   wikipedia-fkmeans-clusters/clusters/clusters-1/part-r-00000 -p\n>>>>>   wikipedia-fkmeans-clusters/clusteredPoints/ -d\n>>>>>  wikipedia-fkmeans-clusters/dictionary.file-0 -dt sequencefile -dm\n>>>>>   org.apache.mahout.common.distance.CosineDistanceMeasure\n>>>>>\n>>>>> This will output to the screen. Use -o to specify an output file.\n>>>>>\n>>>>> Good advice for any user of mahout is read the output of the help very\n>>>>> carefully. IMHO it is very easy to misunderstand the parameters,\n>>>>> inputs,\n>>>>> and\n>>>>> outputs. I think I only understand about 10%. Try:\n>>>>>\n>>>>>   mahout fkmeans --help\n>>>>>\n>>>>>\n>>>>>\n>>>>> On 3/14/12 10:52 AM, Baoqiang Cao wrote:\n>>>>>>\n>>>>>> Hi,\n>>>>>>\n>>>>>> Very sorry for such a trivial question but ran out of luck. I'm trying\n>>>>>> to see which points (thru point-ids) belong to which cluster center.\n>>>>>> Here is what I did:\n>>>>>>\n>>>>>> mahout clusterdump -s /mahout/kmeans/clusters-15-final -d\n>>>>>> /mahout/sparse/dictionary.file-0 -dt sequencefile   -p /mahout/points\n>>>>>>>\n>>>>>>> out\n>>>>>>\n>>>>>> The onscreen output is:\n>>>>>>\n>>>>>> 12/03/14 12:39:52 INFO common.AbstractJob: Command line arguments:\n>>>>>> {--dictionary=/mahout/sparse/dictionary.file-0,\n>>>>>> --dictionaryType=sequencefile,\n>>>>>>\n>>>>>>\n>>>>>>\n>>>>>> --distanceMeasure=org.apache.mahout.common.distance.SquaredEuclideanDistanceMeasure,\n>>>>>> --endPhase=2147483647, --outputFormat=TEXT,\n>>>>>> --pointsDir=/mahout/points,\n>>>>>> --seqFileDir=/mahout/kmeans/clusters-15-final, --startPhase=0,\n>>>>>> --tempDir=temp}\n>>>>>> 12/03/14 12:39:55 WARN snappy.LoadSnappy: Snappy native library is\n>>>>>> available\n>>>>>> 12/03/14 12:39:55 INFO util.NativeCodeLoader: Loaded the native-hadoop\n>>>>>> library\n>>>>>> 12/03/14 12:39:55 INFO snappy.LoadSnappy: Snappy native library loaded\n>>>>>> 12/03/14 12:39:55 INFO compress.CodecPool: Got brand-new decompressor\n>>>>>> 12/03/14 12:39:55 INFO compress.CodecPool: Got brand-new decompressor\n>>>>>> 12/03/14 12:39:55 INFO compress.CodecPool: Got brand-new decompressor\n>>>>>> 12/03/14 12:39:55 INFO compress.CodecPool: Got brand-new decompressor\n>>>>>> 12/03/14 12:42:07 INFO clustering.ClusterDumper: Wrote 5188 clusters\n>>>>>> 12/03/14 12:42:07 INFO driver.MahoutDriver: Program took 135276 ms\n>>>>>> (Minutes: 2.2546)\n>>>>>>\n>>>>>>\n>>>>>> There is nothing under "/mahout/points". Any help on why and how?\n>>>>>>\n>>>>>> Thanks in advance.\n>>>>>> Baoqiang\n>>>>>>\n>\n\n \n
<4F6A3673.3050100@occamsmachete.com>	Pat Ferrel 	pat@occamsmachete.com	What to do with empty docs	2012-03-21T20:13:39Z	<CACpbbiJC=TX79Ks0eKZAtEgX7rZ95PB3Up5j0PHnzfbO8HxaSQ@mail.gmail.com>	You may want to tune your analyzer to let more tokens through. The empty \ndocs may be used in the TFIDF part of the analysis to calculate IDF, not \nsure?\n\nAs to clusters with empty docs, I don't see how you can avoid that \nunless you drop them before running clustering. I can't think of any \nuseful information they add to clustering.\n\nOn 3/21/12 12:06 PM, Baoqiang Cao wrote:\n> This is extremely helpful! Thanks a lot.\n> Indeed, after seqdumper I got such:\n>\n> Key: 1774184: Value: wt: 1.0distance: 0.8839410915753125  vec: [123426:1.000]\n> Key: 1705919: Value: wt: 1.0distance: 0.0  vec: []\n> Key: 1705919: Value: wt: 1.0distance: 0.0  vec: []\n> Key: 1705919: Value: wt: 1.0distance: 0.0  vec: []\n>\n> And after redid the seq2sparse part with "-nv", I got the postid as\n> well. So, it is clear that due to stop words and rare occurrence,\n> some documents end up with no words left--empty documents. That is why\n> I got "vec: []". Is there any suggestion dealing with them? Leave as\n> is so always be aware of one trivial cluster (empty vectors)? Or...\n>\n> Thanks again for wonderful help!\n>\n>\n> On Tue, Mar 20, 2012 at 10:35 AM, Pat Ferrel<pat@occamsmachete.com>  wrote:\n>> What I have done is to use "seq2sparse -nv" to get named vectors.\n>>\n>>    *mahout seq2sparse \\n>>        -i reuters-seqfiles/ \\n>>        -o reuters-vectors/ \\n>>        -ow -chunk 100 \\n>>        -x 90 \\n>>        -seq \\n>>        -a com.finderbots.analyzers.LuceneStemmingAnalyzer \\n>>        -ml 50 \\n>>        -n 2 \\n>>        -nv*\n>>\n>> This will use the filename as the key in the vector sequence file. The keys\n>> will remain the same through the clustering phase.\n>>\n>> Run "kmeans -cl" to get the clusteredPoint dir created and in it you will\n>> find a part-m-00000\n>>\n>>    *mahout kmeans \\n>>        -i reuters-vectors/tfidf-vectors/ \\n>>        -c reuters-kmeans-centroids \\n>>        -cl \\n>>        -o reuters-kmeans-clusters \\n>>        -k 20 \\n>>        -ow \\n>>        -x 10 \\n>>        -dm org.apache.mahout.common.distance.CosineDistanceMeasure *\n>>\n>> Use seqdumper:\n>>\n>>    mahout seqdumper -s\n>>    reuters-kmeans-clusters/clusteredPoints/part-m-00000 | more\n>>\n>> You will see that the file contains\n>>\n>>    key: clusterid, value: wt = % likelihood the vector is in cluster,\n>>    distance from centroid, named vector belonging to the cluster,\n>>    vector data.\n>>\n>> For kmeans the likelihood will be 1.0 or 0. For example:\n>>\n>>    Key: 21477: Value: wt: 1.0distance: 0.9420744909793364  vec:\n>>    /-tmp/reut2-000.sgm-158.txt = [372:0.318, 966:0.396, 3027:0.230,\n>>    8816:0.452, 8868:0.308, 13639:0.278, 13648:0.264, 14334:0.270,\n>>    14371:0.413]\n>>\n>> Clusters, of course, cannot have names. A simple solution is to construct a\n>> name from the top terms in the centroid output from clusterdump.\n>>\n>> I also recommend /Mahout in Action/ from Manning Publishing. You can buy it\n>> here:\n>> http://manning.com/owen/\n>>\n>>\n>> On 3/19/12 7:45 PM, Baoqiang Cao wrote:\n>>> Thanks again for the reference!\n>>>\n>>> One more question if you don't mind. How do I get the text keys for\n>>> each cluster? I mean, so at beginning we have input file for\n>>> seq2sparse, and we knew the format is\n>>>\n>>> textkey1 text1\n>>> textkey2 text2\n>>> ..\n>>>\n>>> after mahout kmeans, and clusterdump, how do I know, for example,\n>>> which texts belong to cluster 1 from command line? I thought the\n>>> outputs from clusterdump have what I want, but so far, it is still\n>>> elusive.\n>>>\n>>> Best,\n>>> Baoqiang\n>>>\n>>>\n>>>\n>>> On Mon, Mar 19, 2012 at 4:01 PM, Pat Ferrel<pat@occamsmachete.com>    wrote:\n>>>> I guess you figured this out but the cluster drivers take "-cl", which\n>>>> tells\n>>>> them to put points into the calculated clusters and output to the\n>>>> clusterPoints directory. Then you pass that in to clusterdump.\n>>>>\n>>>> instructions here:\n>>>> https://cwiki.apache.org/confluence/display/MAHOUT/K-Means+Clustering\n>>>>\n>>>> the --help for the mahout cluster drivers is incomplete, check cwiki for\n>>>> differences\n>>>>\n>>>>\n>>>>\n>>>> On 3/14/12 3:18 PM, Baoqiang Cao wrote:\n>>>>> Thanks a l ot. But I don't know if I miss anything in front of my teary\n>>>>> eyes because of Wednesday afternoon or ? I have equivalent inputs as\n>>>>> yours:\n>>>>>\n>>>>> mahout clusterdump -s /mahout/kmeans/clusters-15-final -d\n>>>>> /mahout/sparse/dictionary.file-0 -dt sequencefile   -p /mahout/points\n>>>>>\n>>>>> the cluster files after 15 iterations are\n>>>>> /mahout/kmeans/clusters-15-final. /mahout/points is a directory I\n>>>>> created in prior. On screen, the output are something like\n>>>>> "VL-1721020{n=186 c=[...". It just is no any output files under that\n>>>>> directory.\n>>>>>\n>>>>> Any help , please\n>>>>>\n>>>>> On Wed, Mar 14, 2012 at 2:13 PM, Pat Ferrel<pat@occamsmachete.com>\n>>>>>   wrote:\n>>>>>> The -p parameter is an input. You should pass in the clusterPoints/\n>>>>>> directory that was generated by the cluster driver you used.\n>>>>>>\n>>>>>> My use of fkmeans might be an example:\n>>>>>>\n>>>>>>    mahout fkmeans -i wikipedia-vectors/tfidf-vectors/ -c\n>>>>>>    wikipedia-fkmeans-centroids -o wikipedia-fkmeans-clusters -k 100 -m\n>>>>>>    2 -ow -x 10 -dm\n>>>>>> org.apache.mahout.common.distance.CosineDistanceMeasure\n>>>>>>\n>>>>>> This will create\n>>>>>> wikipedia-clusters/clusters/clusteredPoints/part-m-00000\n>>>>>> which is the file with the clustered points. I then did a clusterdump\n>>>>>>\n>>>>>>    mahout clusterdump -s\n>>>>>>    wikipedia-fkmeans-clusters/clusters/clusters-1/part-r-00000 -p\n>>>>>>    wikipedia-fkmeans-clusters/clusteredPoints/ -d\n>>>>>>   wikipedia-fkmeans-clusters/dictionary.file-0 -dt sequencefile -dm\n>>>>>>    org.apache.mahout.common.distance.CosineDistanceMeasure\n>>>>>>\n>>>>>> This will output to the screen. Use -o to specify an output file.\n>>>>>>\n>>>>>> Good advice for any user of mahout is read the output of the help very\n>>>>>> carefully. IMHO it is very easy to misunderstand the parameters,\n>>>>>> inputs,\n>>>>>> and\n>>>>>> outputs. I think I only understand about 10%. Try:\n>>>>>>\n>>>>>>    mahout fkmeans --help\n>>>>>>\n>>>>>>\n>>>>>>\n>>>>>> On 3/14/12 10:52 AM, Baoqiang Cao wrote:\n>>>>>>> Hi,\n>>>>>>>\n>>>>>>> Very sorry for such a trivial question but ran out of luck. I'm trying\n>>>>>>> to see which points (thru point-ids) belong to which cluster center.\n>>>>>>> Here is what I did:\n>>>>>>>\n>>>>>>> mahout clusterdump -s /mahout/kmeans/clusters-15-final -d\n>>>>>>> /mahout/sparse/dictionary.file-0 -dt sequencefile   -p /mahout/points\n>>>>>>>> out\n>>>>>>> The onscreen output is:\n>>>>>>>\n>>>>>>> 12/03/14 12:39:52 INFO common.AbstractJob: Command line arguments:\n>>>>>>> {--dictionary=/mahout/sparse/dictionary.file-0,\n>>>>>>> --dictionaryType=sequencefile,\n>>>>>>>\n>>>>>>>\n>>>>>>>\n>>>>>>> --distanceMeasure=org.apache.mahout.common.distance.SquaredEuclideanDistanceMeasure,\n>>>>>>> --endPhase=2147483647, --outputFormat=TEXT,\n>>>>>>> --pointsDir=/mahout/points,\n>>>>>>> --seqFileDir=/mahout/kmeans/clusters-15-final, --startPhase=0,\n>>>>>>> --tempDir=temp}\n>>>>>>> 12/03/14 12:39:55 WARN snappy.LoadSnappy: Snappy native library is\n>>>>>>> available\n>>>>>>> 12/03/14 12:39:55 INFO util.NativeCodeLoader: Loaded the native-hadoop\n>>>>>>> library\n>>>>>>> 12/03/14 12:39:55 INFO snappy.LoadSnappy: Snappy native library loaded\n>>>>>>> 12/03/14 12:39:55 INFO compress.CodecPool: Got brand-new decompressor\n>>>>>>> 12/03/14 12:39:55 INFO compress.CodecPool: Got brand-new decompressor\n>>>>>>> 12/03/14 12:39:55 INFO compress.CodecPool: Got brand-new decompressor\n>>>>>>> 12/03/14 12:39:55 INFO compress.CodecPool: Got brand-new decompressor\n>>>>>>> 12/03/14 12:42:07 INFO clustering.ClusterDumper: Wrote 5188 clusters\n>>>>>>> 12/03/14 12:42:07 INFO driver.MahoutDriver: Program took 135276 ms\n>>>>>>> (Minutes: 2.2546)\n>>>>>>>\n>>>>>>>\n>>>>>>> There is nothing under "/mahout/points". Any help on why and how?\n>>>>>>>\n>>>>>>> Thanks in advance.\n>>>>>>> Baoqiang\n>>>>>>>\n\n \n
<46aec0c.23042.1363985f728.Coremail.beneo_7@163.com>	beneo_7 	beneo_7@163.com	why DoubleMatrix2D.java in mahout trunk is deprecated?	2012-03-22T08:29:02Z		why DoubleMatrix2D.java in mahout trunk is deprecated? \n\n\nif i want to use the function in DoubleMatrix2D, where should i find ? \n\n
<1332406303277-3847880.post@n3.nabble.com>	dionyasos 	bahadiryilmz@gmail.com	is hadoop necessary for clustering in mahout?	2012-03-22T08:51:43Z		Hi everyone,\nis there a way to run clustering algorithms in mahout without using hadoop ?\nthanks.\n\n--\nView this message in context: http://lucene.472066.n3.nabble.com/is-hadoop-necessary-for-clustering-in-mahout-tp3847880p3847880.html\nSent from the Mahout User List mailing list archive at Nabble.com.\n\n \n
<1332402120126-3847789.post@n3.nabble.com>	tianwild 	tianwild_22@hotmail.com	Re: Error Running mahout-core-0.5-job.jar	2012-03-22T07:42:00Z	<1332353507358-3846385.post@n3.nabble.com>	the correct Dmapred is -Dmapred.dir=output, not --Dmapred.dir=output\n\n--\nView this message in context: http://lucene.472066.n3.nabble.com/Error-Running-mahout-core-0-5-job-jar-tp3846385p3847789.html\nSent from the Mahout User List mailing list archive at Nabble.com.\n\n \n
<1326751540852-3664574.post@n3.nabble.com>	tdguest 	abhilasha@querydynamics.com	Re: Twitter Classification	2012-01-16T22:05:40Z	<cd6fa9681002120251i2b68d74fi1a8680aa1fe9eca1@mail.gmail.com>	\n   Very Interesting discussion! Are there are any tweet classification\nsystems existing right now?  Would love to check them out!\n\n   I have a site called http://www.tweetdynamics.com that categorizes tweets\nfrom users all over the world into topics. The classification is based on a\nsemi-supervised classification method. Will be great if you can check this\nsite out and send me feedback at - abhilasha@querydynamics.com\n\nRegards,\nAbhilasha\n\n--\nView this message in context: http://lucene.472066.n3.nabble.com/Twitter-Classification-tp641884p3664574.html\nSent from the Mahout User List mailing list archive at Nabble.com.\n\n \n
<1326765276.99527.YahooMailNeo@web39405.mail.mud.yahoo.com>	Suneel Marthi 	suneel_marthi@yahoo.com	Help with RowSimilarityJob CLI	2012-01-17T01:54:36Z		I am looking to use RowSimilarityJob for determining the similarity of 2 documents (using Cosine Distance Measure).\n\nI am working off of Mahout trunk and don't see the CLI for rowsimilarity (for /bin/mahout). I do see that the Mahout 0.5 has a rowsimilarity CLI.\n\nWhat am I doing wrong or am I missing something?\n\nRegards,\nSuneel\n \n\n
<CAHPpb6aKRHXGBB+_4a9Cpo90jkkL4Z3Ss4PJK9N_Hdk=6DVoVg@mail.gmail.com>	tanzek 	tanzek@gmail.com	Re: Help in vectorizing features	2012-01-17T03:45:20Z	<CAJwFCa3DiTW+gWAqWjNWo=HJAnP=CLsrh_pQCLD4f4A3kkWSMQ@mail.gmail.com>	Oh. very thank you. I think I can understand the usage of Vector.\nIn your words, is the thing called "feature hashing"? Are there any\nmaterials or papers in which the meaning is illustrated? Is what the paper\n<Feature hashing for large scale multitask learning> written by Weinberger\net al. shows similar with the "feature hashing" in your book?\n\n\n2012/1/17 Ted Dunning <ted.dunning@gmail.com>\n\n> What you have is almost correct.  Usually, however, you don't want to\n> encode your class in a single slot, but rather allocate k slots for a\n> nominal variable that can have k values and set one of those values to 1\n> with all others set to 0.\n>\n> If you do this, then you don't need the *Encoder stuff at all.\n>\n> On the other hand, if k is very large or even not known or you have a bag\n> of nominals with large or unknown k, then you need the Encoder framework.\n>  In that case, you will need to have a large vector to encode into, but not\n> as large as k (which is good since you don't even know how big that is).\n>\n> On Mon, Jan 16, 2012 at 3:22 PM, tanzek <tanzek@gmail.com> wrote:\n>\n> > Hello, Ted, I really need a help. Are there any problems with my\n> questions?\n> >\n> > 2012/1/14 tanzek <tanzek@gmail.com>\n> >\n> > > I have a file in which these are some features and each row is a record\n> > > except the head, they are like a relational table. All features are\n> > > numeric, and the last feature is a nominal. Now I need to vectorize\n> them\n> > to\n> > > feed the logistic regression or other classification algorithms. But\n> > after\n> > > I have read chapters from 13 to 16 in <Mahout in Action>, I was puzzled\n> > by\n> > > the feature encoder, especially when I used the ContinuousValueEncoder.\n> > The\n> > > following code is from my real program:\n> > >\n> > > FeatureVectorEncoder enc = new ContinuousValueEncoder("test");\n> > > Vector v1 = new DenseVector(20);  // 19 features + 1 class\n> > > String[] ftStr = fileReader.getLine[].split(",");\n> > > for(int i=0; i<19; ++i){\n> > >     enc.addToVector(ftStr[i], v1);\n> > >     // enc.addToVector((byte[])null, Double.parseDouble(ftStr[i]), v1);\n> > > }\n> > > System.out.println(v1);   // *** I can't get the result I am familiar\n> > with.\n> > >\n> > > Should I use ContinusousValueEncoder to finish this job? The feature\n> > > encoder or feature hashing seems to be hard for me to understand. I\n> have\n> > > also dropped the feature encoder in this code.\n> > >\n> > > Vector v1 = new DenseVector(20);  // 19 features + 1 class\n> > > String[] ftStr = fileReader.getLine[].split(",");\n> > > for(int i=0; i<19; ++i){\n> > >     v1.set(i, Double.parseDouble(ftStr[i]));\n> > > }\n> > > System.out.println(v1);   // *** now I can understand my code\n> > >\n> > > Is this the right way to use Vector?\n> > >\n> > > So, in all I have three questions:\n> > > 1. What is the relationship between Vector and Encoder?\n> > > 2. Is the Encoder essential to vectorize my features?\n> > > 3. Why the encoder work in an unfamiliar way or how does it work?\n> > >\n> > > Any helps, discussions, materials or papers would be highly\n> appreciated.\n> > > Thank you!\n> > >\n> > >\n> >\n>\n \n\n
<CAJwFCa26NPKF--JzMW102HwHj2Z6y8ksXxjMcv9eSQi+cvPyqg@mail.gmail.com>	Ted Dunning 	ted.dunning@gmail.com	Re: Help in vectorizing features	2012-01-17T05:37:50Z	<CAHPpb6aKRHXGBB+_4a9Cpo90jkkL4Z3Ss4PJK9N_Hdk=6DVoVg@mail.gmail.com>	On Tue, Jan 17, 2012 at 3:45 AM, tanzek <tanzek@gmail.com> wrote:\n\n> Oh. very thank you. I think I can understand the usage of Vector.\n> In your words, is the thing called "feature hashing"?\n\n\nYes.\n\n\n> Are there any materials or papers in which the meaning is illustrated? Is\n> what the paper\n> <Feature hashing for large scale multitask learning> written by Weinberger\n> et al. shows similar with the "feature hashing" in your book?\n>\n\nYes.  This is a good reference.\n\nThe feature hashing Mahout differs slightly in that we have multiple fields\nand support multiple probes so that we can use smaller vectors.\n\nThe basic idea remains the same, however.\n \n\n
<1326788081814-3665439.post@n3.nabble.com>	mikaza 	michael.kazekin@mediainsight.info	Extending mahout lucene.vector driver	2012-01-17T08:14:41Z		Hi!\n\nI am trying to extend "mahout lucene.vector" driver, so that it can be\nfeeded with arbitrary\nkey-value constraints on solr schema fields (and generate only a subset for\nmahout vectors,\nwhich seems to be a regular use case).\n\nSo the best (easiest) way I see, is to create an IndexReader implementation\nthat would allow\nto read the subset.\n\nThe problem is that I don't know the correct way to do this.\n\nMaybe, subclassing the FilterIndexReader would solve the problem, but I\ndon't know which\nmethods to override to get a consistent object representation.\n\nThe driver code includes the following:\n\n IndexReader reader = IndexReader.open(dir, true);\n\n    Weight weight;\n    if ("tf".equalsIgnoreCase(weightType)) {\n      weight = new TF();\n    } else if ("tfidf".equalsIgnoreCase(weightType)) {\n      weight = new TFIDF();\n    } else {\n      throw new IllegalArgumentException("Weight type " + weightType + " is\nnot supported");\n    }\n\n    TermInfo termInfo = new CachedTermInfo(reader, field, minDf,\nmaxDFPercent);\n    VectorMapper mapper = new TFDFMapper(reader, weight, termInfo);\n\n    LuceneIterable iterable;\n\n    if (norm == LuceneIterable.NO_NORMALIZING) {\n      iterable = new LuceneIterable(reader, idField, field, mapper,\nLuceneIterable.NO_NORMALIZING, maxPercentErrorDocs);\n    } else {\n      iterable = new LuceneIterable(reader, idField, field, mapper, norm,\nmaxPercentErrorDocs);\n    }\n\nIt creates a SequenceFile.Writer class then and writes the "iterable"\nvariable.\n\n\nDo you have any thoughts on how to inject the code in a most simple way?\n\n--\nView this message in context: http://lucene.472066.n3.nabble.com/Extending-mahout-lucene-vector-driver-tp3665439p3665439.html\nSent from the Mahout User List mailing list archive at Nabble.com.\n\n \n
<1326790060.60643.YahooMailNeo@web121204.mail.ne1.yahoo.com>	Harry Potter 	harry123great@yahoo.com	MR Vectorization	2012-01-17T08:47:40Z		How to do vectorization for the fields extracted from a network packet using MR ?(i.e.,) how to convert the network packet to a vector format? \n\n
<9f7fc60.454612fb.4f154f0a.40e93@o2.pl>	Szymon Chojnacki 	sajmmon@o2.pl	Re: Recommender system - feedback request	2012-01-17T10:35:54Z	<E5D4A64F-3972-45FC-813C-12C64F2C74B8@gmail.com>	Thank you Ted,\nCheers\nDnia 16 stycznia 2012 22:36 Ted Dunning &lt;ted.dunning@gmail.com&gt; napisał(a):\nSounds like a natural application of nearest neighbor techniques. My guess is that if the size of your set A is moderate then the mahout recommendation engine will work with the addition of a specialized distance function. Data sets of 100,000 examples or more are probably just fine. \nSent from my iPhone\nOn Jan 16, 2012, at 11:35, Szymon Chojnacki &lt;sajmmon@o2.pl&gt; wrote:\n&gt; Hi,\n&gt; \n&gt; my request is not directly connected to Mahout software. I would like to ask for a feedback from ML practitioners in the Mahout community. I am looking for a recommender algorithm that could be used in the following situation:\n&gt; \n&gt; 1. As input we have only positive examples mapping points from N-dimensional space A to other N-dimensional space B\n&gt; 2. We have a generator that creates plausible points in B (around 60) for any given point in A\n&gt; 3. We would like to select the best 5 points in B (from generated ones, the points are usually unique)\n&gt; \n&gt; The recommender system is used to automatically arrange offices layouts with furniture, \n&gt; some dimensions from A are: area, number of doors, area of windows. Some dimensions from B are: area occupied by desks / total area, price of all furniture, the variance of the distribution of mass centers.\n&gt; \n&gt; Currently a simple algorithm is implemented in a tool called Reterio ( http://www.reterio.com ), but I am looking for any publications describing a general approach to this problem.\n&gt; \n&gt; Will be greatful for any feedback\n&gt; \n&gt; Szymon Chojnacki\n&gt; \n&gt; \n&gt; \n--\nSzymon Chojnacki http://www.ipipan.eu/~sch/ \n\n
<4F15510A.8000607@gmail.com>	Ioan Eugen Stan 	stan.ieugen@gmail.com	Re: Help with RowSimilarityJob CLI	2012-01-17T10:44:26Z	<1326765276.99527.YahooMailNeo@web39405.mail.mud.yahoo.com>	Pe 17.01.2012 03:54, Suneel Marthi a scris:\n> I am looking to use RowSimilarityJob for determining the similarity of 2 documents (using Cosine Distance Measure).\n>\n> I am working off of Mahout trunk and don't see the CLI for rowsimilarity (for /bin/mahout). I do see that the Mahout 0.5 has a rowsimilarity CLI.\n>\n> What am I doing wrong or am I missing something?\n>\n> Regards,\n> Suneel\n>\n\n\nHello Suneel,\n\nI'm also working on trunk and I see it. It's defined in \ndriver.classes.props. The class itself is in package \norg.apache.mahout.math.hadoop.similarity.cooccurrence .\n\nMaybe mahout cli doesn't find the props file on the class path. Do you \nalso have mahout math package?\n\nHope this helps,\n\n-- \nIoan Eugen Stan\nhttp://ieugen.blogspot.com\n\n \n
<CACx7p4CjfGZz5e6r8fDNrb6O6urOUxToNtwabfKQq08RxcdzAA@mail.gmail.com>	Raviv Pavel 	raviv@gigya-inc.com	Re: Twitter Classification	2012-01-17T10:52:22Z	<1326751540852-3664574.post@n3.nabble.com>	Where are you taking the list of topics from ?\n\n*\n*\n*--*Raviv\n\n\n\nOn Tue, Jan 17, 2012 at 12:05 AM, tdguest <abhilasha@querydynamics.com>wrote:\n\n>\n>   Very Interesting discussion! Are there are any tweet classification\n> systems existing right now?  Would love to check them out!\n>\n>   I have a site called http://www.tweetdynamics.com that categorizes\n> tweets\n> from users all over the world into topics. The classification is based on a\n> semi-supervised classification method. Will be great if you can check this\n> site out and send me feedback at - abhilasha@querydynamics.com\n>\n> Regards,\n> Abhilasha\n>\n> --\n> View this message in context:\n> http://lucene.472066.n3.nabble.com/Twitter-Classification-tp641884p3664574.html\n> Sent from the Mahout User List mailing list archive at Nabble.com.\n>\n \n\n
<CACx7p4CjfGZz5e6r8fDNrb6O6urOUxToNtwabfKQq08RxcdzAA@mail.gmail.com>	Raviv Pavel 	raviv@gigya-inc.com	Re: Twitter Classification	2012-01-17T10:52:22Z	<1326751540852-3664574.post@n3.nabble.com>	Where are you taking the list of topics from ?\n\n*\n*\n*--*Raviv\n\n\n\nOn Tue, Jan 17, 2012 at 12:05 AM, tdguest <abhilasha@querydynamics.com>wrote:\n\n>\n>   Very Interesting discussion! Are there are any tweet classification\n> systems existing right now?  Would love to check them out!\n>\n>   I have a site called http://www.tweetdynamics.com that categorizes\n> tweets\n> from users all over the world into topics. The classification is based on a\n> semi-supervised classification method. Will be great if you can check this\n> site out and send me feedback at - abhilasha@querydynamics.com\n>\n> Regards,\n> Abhilasha\n>\n> --\n> View this message in context:\n> http://lucene.472066.n3.nabble.com/Twitter-Classification-tp641884p3664574.html\n> Sent from the Mahout User List mailing list archive at Nabble.com.\n>\n \n\n
<4F155317.1070703@gmail.com>	Ioan Eugen Stan 	stan.ieugen@gmail.com	Re: MR Vectorization	2012-01-17T10:53:11Z	<1326790060.60643.YahooMailNeo@web121204.mail.ne1.yahoo.com>	Pe 17.01.2012 10:47, Harry Potter a scris:\n> How to do vectorization for the fields extracted from a network packet using MR ?(i.e.,) how to convert the network packet to a vector format?\n\nWell there is a good example in Mahout in Action. Basically you have to \ndefine your list of features. For a network packet possible features \ncould be:\n\n- packet size (easy to compare packets with similar sizes)\n- does it come from our network (kind of binary feature)\n- is it on an encrypted channel (packet is ssl??)\n- does it go to sensitive destination\n- the network address in a notation that can group packets by networks \nconsidering that they are similar if they are in the same network,\n\netc.\n\nIt really depends on what are you trying to achieve. A feature is \nsomething that matters to you in whatever are you trying to achieve.\n\nYou will probably spend a lot of time tuning this so don't go for \nperfect the first time. Give it some spins.\n\nThen you assign each feature an index in your feature vector.\nThen you create vectors by assigning NUMERIC wights to those features.\nThen you compute similarity between two vectors (packets) by using your \nown or one of mahout implemented distance metrics.\n\nI hope this gives you a start,\n\n-- \nIoan Eugen Stan\nhttp://ieugen.blogspot.com\n\n \n
<1326798618.75219.YahooMailNeo@web121201.mail.ne1.yahoo.com>	Harry Potter 	harry123great@yahoo.com	Re: MR Vectorization	2012-01-17T11:10:18Z	<4F155317.1070703@gmail.com>	thanks sir...  that was really helpful..\n\n\n________________________________\n From: Ioan Eugen Stan <stan.ieugen@gmail.com>\nTo: user@mahout.apache.org \nSent: Tuesday, January 17, 2012 4:23 PM\nSubject: Re: MR Vectorization\n \nPe 17.01.2012 10:47, Harry Potter a scris:\n> How to do vectorization for the fields extracted from a network packet using MR ?(i.e.,) how to convert the network packet to a vector format?\n\nWell there is a good example in Mahout in Action. Basically you have to define your list of features. For a network packet possible features could be:\n\n- packet size (easy to compare packets with similar sizes)\n- does it come from our network (kind of binary feature)\n- is it on an encrypted channel (packet is ssl??)\n- does it go to sensitive destination\n- the network address in a notation that can group packets by networks considering that they are similar if they are in the same network,\n\netc.\n\nIt really depends on what are you trying to achieve. A feature is something that matters to you in whatever are you trying to achieve.\n\nYou will probably spend a lot of time tuning this so don't go for perfect the first time. Give it some spins.\n\nThen you assign each feature an index in your feature vector.\nThen you create vectors by assigning NUMERIC wights to those features.\nThen you compute similarity between two vectors (packets) by using your own or one of mahout implemented distance metrics.\n\nI hope this gives you a start,\n\n-- Ioan Eugen Stan\nhttp://ieugen.blogspot.com \n\n
<CA+fkeAESXzxCBNtuACpQRGoGOd__5_izxPXtTNKdT2BRKcSa0A@mail.gmail.com>	jams gost 	jamsgost@gmail.com	Help for optimizing Mahout Recommender Code	2012-01-17T13:57:52Z	<CA+fkeAHLQ2m2oEMMeWLSOy_tw8uSkCAgLCiy6+K7+2k6JSVz1Q@mail.gmail.com>	HI Mahout Group,\n\n\n  I am student and new to mahout. I have reviewed Mahout code For\nrecommender system and implemented in my system.I have implemented belowed\nJAVA FILE.\n\nimport java.io.File;\nimport java.io.FileNotFoundException;\nimport java.util.List;\nimport java.io.IOException;\nimport org.apache.mahout.cf.taste.impl.model.file.FileDataModel;\nimport org.apache.mahout.cf.taste.impl.recommender.CachingRecommender;\nimport org.apache.commons.cli2.OptionException;\nimport org.apache.mahout.cf.taste.common.TasteException;\nimport org.apache.mahout.cf.taste.impl.common.LongPrimitiveIterator;\nimport\norg.apache.mahout.cf.taste.impl.recommender.slopeone.SlopeOneRecommender;\nimport org.apache.mahout.cf.taste.model.DataModel;\nimport org.apache.mahout.cf.taste.recommender.RecommendedItem;\n\n\npublic class allrecommender {\n\n    public static void main(String... args) throws FileNotFoundException,\nTasteException, IOException, OptionException {\n\n\n        File ratingsFile = new File("data.csv");\n        DataModel model = new FileDataModel(ratingsFile);\n\n        CachingRecommender cachingRecommender = new CachingRecommender(new\nSlopeOneRecommender(model));\n\n\n        for (LongPrimitiveIterator it = model.getUserIDs(); it.hasNext();){\n            long userId = it.nextLong();\n\n         List<RecommendedItem> recommendations =\ncachingRecommender.recommend(userId, 10);\n\n\n            if (recommendations.size() == 0){\n                System.out.print("User ");\n                System.out.print(userId);\n                System.out.println(": no recommendations");\n            }\n\n\n            for (RecommendedItem recommendedItem : recommendations) {\n                System.out.print("User ");\n                System.out.print(userId);\n                System.out.print(": ");\n                System.out.println(recommendedItem);\n            }\n        }\n    }\n}\n\n\nAt console side I am getting recommendation like\n\nUser 1188 : RecommendedItem[item:58804, value:1.0]\nUser 1188 : RecommendedItem[item:6616, value:1.0]\nUser 1188 : RecommendedItem[item:60158, value:1.0]\nUser 1188 : RecommendedItem[item:9262, value:1.0]\nUser 1188 : RecommendedItem[item:39936, value:1.0]\nUser 1188 : RecommendedItem[item:29095, value:1.0]\nUser 1188 : RecommendedItem[item:3317, value:1.0]\nUser 1188 : RecommendedItem[item:35707, value:1.0]\nUser 1188 : RecommendedItem[item:54217, value:1.0]\nUser 1188 : RecommendedItem[item:25450, value:1.0]\n\n\nInstead this, I want recommendation in this manner:\n\nfor User 1188 : 58804,6616,60158,9262,39936,29095,3317,35707,54217,25450\n\nor only ids of item.so how can i get only item ids on my console..\n\n\nThanks,\nJams\n \n\n
<CAEccTyzR=_AhZEsiky+aokee01NHY1yjqn370wyzzBQipO8_bA@mail.gmail.com>	Sean Owen 	srowen@gmail.com	Re: Help for optimizing Mahout Recommender Code	2012-01-17T14:47:28Z	<CA+fkeAESXzxCBNtuACpQRGoGOd__5_izxPXtTNKdT2BRKcSa0A@mail.gmail.com>	I don't think this has anything to do with Mahout per se. You just\nneed to write your loop to print the output as you say you want it.\n\nOn Tue, Jan 17, 2012 at 1:57 PM, jams gost <jamsgost@gmail.com> wrote:\n> HI Mahout Group,\n>\n>\n>  I am student and new to mahout. I have reviewed Mahout code For\n> recommender system and implemented in my system.I have implemented belowed\n> JAVA FILE.\n>\n> import java.io.File;\n> import java.io.FileNotFoundException;\n> import java.util.List;\n> import java.io.IOException;\n> import org.apache.mahout.cf.taste.impl.model.file.FileDataModel;\n> import org.apache.mahout.cf.taste.impl.recommender.CachingRecommender;\n> import org.apache.commons.cli2.OptionException;\n> import org.apache.mahout.cf.taste.common.TasteException;\n> import org.apache.mahout.cf.taste.impl.common.LongPrimitiveIterator;\n> import\n> org.apache.mahout.cf.taste.impl.recommender.slopeone.SlopeOneRecommender;\n> import org.apache.mahout.cf.taste.model.DataModel;\n> import org.apache.mahout.cf.taste.recommender.RecommendedItem;\n>\n>\n> public class allrecommender {\n>\n>    public static void main(String... args) throws FileNotFoundException,\n> TasteException, IOException, OptionException {\n>\n>\n>        File ratingsFile = new File("data.csv");\n>        DataModel model = new FileDataModel(ratingsFile);\n>\n>        CachingRecommender cachingRecommender = new CachingRecommender(new\n> SlopeOneRecommender(model));\n>\n>\n>        for (LongPrimitiveIterator it = model.getUserIDs(); it.hasNext();){\n>            long userId = it.nextLong();\n>\n>         List<RecommendedItem> recommendations =\n> cachingRecommender.recommend(userId, 10);\n>\n>\n>            if (recommendations.size() == 0){\n>                System.out.print("User ");\n>                System.out.print(userId);\n>                System.out.println(": no recommendations");\n>            }\n>\n>\n>            for (RecommendedItem recommendedItem : recommendations) {\n>                System.out.print("User ");\n>                System.out.print(userId);\n>                System.out.print(": ");\n>                System.out.println(recommendedItem);\n>            }\n>        }\n>    }\n> }\n>\n>\n> At console side I am getting recommendation like\n>\n> User 1188 : RecommendedItem[item:58804, value:1.0]\n> User 1188 : RecommendedItem[item:6616, value:1.0]\n> User 1188 : RecommendedItem[item:60158, value:1.0]\n> User 1188 : RecommendedItem[item:9262, value:1.0]\n> User 1188 : RecommendedItem[item:39936, value:1.0]\n> User 1188 : RecommendedItem[item:29095, value:1.0]\n> User 1188 : RecommendedItem[item:3317, value:1.0]\n> User 1188 : RecommendedItem[item:35707, value:1.0]\n> User 1188 : RecommendedItem[item:54217, value:1.0]\n> User 1188 : RecommendedItem[item:25450, value:1.0]\n>\n>\n> Instead this, I want recommendation in this manner:\n>\n> for User 1188 : 58804,6616,60158,9262,39936,29095,3317,35707,54217,25450\n>\n> or only ids of item.so how can i get only item ids on my console..\n>\n>\n> Thanks,\n> Jams\n\n \n
<CA+fkeAHy6jSeg6bR13otATdDXDwzEAPBVmbUqHj1-cKR9r=rhg@mail.gmail.com>	jams gost 	jamsgost@gmail.com	Re: Help for optimizing Mahout Recommender Code	2012-01-17T15:20:43Z	<CAEccTyzR=_AhZEsiky+aokee01NHY1yjqn370wyzzBQipO8_bA@mail.gmail.com>	Hello Sean Owen,\n\n    I have created my own loop.even it gives me same like this.\n\nsuppose i am considering one item 35707,it gives me\n\n RecommendedItem[item:35707, value:1.0]\n\n\n But i need only\n\n35707\n\n\nwithout its value.\n\nSo what can i do for this?\n\n\nThanks,\nJams\n \n\n
<CAEccTyyxH11tTJk+6ENFSfzdSMdgzS8EV7PE4P=iC_CDGLxxpQ@mail.gmail.com>	Sean Owen 	srowen@gmail.com	Re: Help for optimizing Mahout Recommender Code	2012-01-17T15:22:19Z	<CA+fkeAHy6jSeg6bR13otATdDXDwzEAPBVmbUqHj1-cKR9r=rhg@mail.gmail.com>	Print recommendedItem.getItemID() then -- you should read the javadoc.\nhttps://builds.apache.org/job/Mahout-Quality/javadoc/org/apache/mahout/cf/taste/recommender/RecommendedItem.html\n\nOn Tue, Jan 17, 2012 at 3:20 PM, jams gost <jamsgost@gmail.com> wrote:\n> Hello Sean Owen,\n>\n>    I have created my own loop.even it gives me same like this.\n>\n> suppose i am considering one item 35707,it gives me\n>\n>  RecommendedItem[item:35707, value:1.0]\n>\n>\n>  But i need only\n>\n> 35707\n>\n>\n> without its value.\n>\n> So what can i do for this?\n>\n>\n> Thanks,\n> Jams\n\n \n
<CA+fkeAHu1DbB-+F5jO2T4dg2csmd4A1fdwp9dK3qEM4DOX3Sdg@mail.gmail.com>	jams gost 	jamsgost@gmail.com	Re: Help for optimizing Mahout Recommender Code	2012-01-17T15:31:22Z	<CAEccTyyxH11tTJk+6ENFSfzdSMdgzS8EV7PE4P=iC_CDGLxxpQ@mail.gmail.com>	Ya Sean Owen,\n\n    Thank you SO mUCh!! You solved it!!\n\n\nOn Tue, Jan 17, 2012 at 8:52 PM, Sean Owen <srowen@gmail.com> wrote:\n\n> Print recommendedItem.getItemID() then -- you should read the javadoc.\n>\n> https://builds.apache.org/job/Mahout-Quality/javadoc/org/apache/mahout/cf/taste/recommender/RecommendedItem.html\n>\n> On Tue, Jan 17, 2012 at 3:20 PM, jams gost <jamsgost@gmail.com> wrote:\n> > Hello Sean Owen,\n> >\n> >    I have created my own loop.even it gives me same like this.\n> >\n> > suppose i am considering one item 35707,it gives me\n> >\n> >  RecommendedItem[item:35707, value:1.0]\n> >\n> >\n> >  But i need only\n> >\n> > 35707\n> >\n> >\n> > without its value.\n> >\n> > So what can i do for this?\n> >\n> >\n> > Thanks,\n> > Jams\n>\n \n\n
<CAJwFCa2rbmXS1fmxc6cyfQwjtruadcc+dbn4Q_oD+1CJobxVmg@mail.gmail.com>	Ted Dunning 	ted.dunning@gmail.com	Re: MR Vectorization	2012-01-17T16:08:03Z	<1326798618.75219.YahooMailNeo@web121201.mail.ne1.yahoo.com>	Time since the last packet from the same source or to the same destination\nis another interesting feature.\n\nOn Tue, Jan 17, 2012 at 11:10 AM, Harry Potter <harry123great@yahoo.com>wrote:\n\n> thanks sir...  that was really helpful..\n>\n>\n> ________________________________\n>  From: Ioan Eugen Stan <stan.ieugen@gmail.com>\n> To: user@mahout.apache.org\n> Sent: Tuesday, January 17, 2012 4:23 PM\n> Subject: Re: MR Vectorization\n>\n> Pe 17.01.2012 10:47, Harry Potter a scris:\n> > How to do vectorization for the fields extracted from a network packet\n> using MR ?(i.e.,) how to convert the network packet to a vector format?\n>\n> Well there is a good example in Mahout in Action. Basically you have to\n> define your list of features. For a network packet possible features could\n> be:\n>\n> - packet size (easy to compare packets with similar sizes)\n> - does it come from our network (kind of binary feature)\n> - is it on an encrypted channel (packet is ssl??)\n> - does it go to sensitive destination\n> - the network address in a notation that can group packets by networks\n> considering that they are similar if they are in the same network,\n>\n> etc.\n>\n> It really depends on what are you trying to achieve. A feature is\n> something that matters to you in whatever are you trying to achieve.\n>\n> You will probably spend a lot of time tuning this so don't go for perfect\n> the first time. Give it some spins.\n>\n> Then you assign each feature an index in your feature vector.\n> Then you create vectors by assigning NUMERIC wights to those features.\n> Then you compute similarity between two vectors (packets) by using your\n> own or one of mahout implemented distance metrics.\n>\n> I hope this gives you a start,\n>\n> -- Ioan Eugen Stan\n> http://ieugen.blogspot.com\n>\n \n\n
<CA+fkeAFwiTy42bU+qzZvsvxQuAKivqELzT=q641OrbStHSV2Mw@mail.gmail.com>	jams gost 	jamsgost@gmail.com	Re: Help for optimizing Mahout Recommender Code	2012-01-17T17:28:31Z	<CA+fkeAHu1DbB-+F5jO2T4dg2csmd4A1fdwp9dK3qEM4DOX3Sdg@mail.gmail.com>	Hello Sean Owen,\n\n\n    At generation of recommendation every time, i got value 1.0. Is that\nmean 100% as 1.0.?\n\nTHanks,\n\nOn Tue, Jan 17, 2012 at 9:01 PM, jams gost <jamsgost@gmail.com> wrote:\n\n> Ya Sean Owen,\n>\n>     Thank you SO mUCh!! You solved it!!\n>\n>\n> On Tue, Jan 17, 2012 at 8:52 PM, Sean Owen <srowen@gmail.com> wrote:\n>\n>> Print recommendedItem.getItemID() then -- you should read the javadoc.\n>>\n>> https://builds.apache.org/job/Mahout-Quality/javadoc/org/apache/mahout/cf/taste/recommender/RecommendedItem.html\n>>\n>> On Tue, Jan 17, 2012 at 3:20 PM, jams gost <jamsgost@gmail.com> wrote:\n>> > Hello Sean Owen,\n>> >\n>> >    I have created my own loop.even it gives me same like this.\n>> >\n>> > suppose i am considering one item 35707,it gives me\n>> >\n>> >  RecommendedItem[item:35707, value:1.0]\n>> >\n>> >\n>> >  But i need only\n>> >\n>> > 35707\n>> >\n>> >\n>> > without its value.\n>> >\n>> > So what can i do for this?\n>> >\n>> >\n>> > Thanks,\n>> > Jams\n>>\n>\n>\n \n\n
<CAEccTyyA+gLhDUfZUo_4wBYQM+fsr+HXtYCEWoAHzwwrh_=vOg@mail.gmail.com>	Sean Owen 	srowen@gmail.com	Re: Help for optimizing Mahout Recommender Code	2012-01-17T17:55:33Z	<CA+fkeAFwiTy42bU+qzZvsvxQuAKivqELzT=q641OrbStHSV2Mw@mail.gmail.com>	It probably means you are using boolean data, but not using one of the\nboolean pref recommenders.\n\nOn Tue, Jan 17, 2012 at 5:28 PM, jams gost <jamsgost@gmail.com> wrote:\n\n> Hello Sean Owen,\n>\n>\n>    At generation of recommendation every time, i got value 1.0. Is that\n> mean 100% as 1.0.?\n>\n> THanks,\n>\n> On Tue, Jan 17, 2012 at 9:01 PM, jams gost <jamsgost@gmail.com> wrote:\n>\n> > Ya Sean Owen,\n> >\n> >     Thank you SO mUCh!! You solved it!!\n> >\n> >\n> > On Tue, Jan 17, 2012 at 8:52 PM, Sean Owen <srowen@gmail.com> wrote:\n> >\n> >> Print recommendedItem.getItemID() then -- you should read the javadoc.\n> >>\n> >>\n> https://builds.apache.org/job/Mahout-Quality/javadoc/org/apache/mahout/cf/taste/recommender/RecommendedItem.html\n> >>\n> >> On Tue, Jan 17, 2012 at 3:20 PM, jams gost <jamsgost@gmail.com> wrote:\n> >> > Hello Sean Owen,\n> >> >\n> >> >    I have created my own loop.even it gives me same like this.\n> >> >\n> >> > suppose i am considering one item 35707,it gives me\n> >> >\n> >> >  RecommendedItem[item:35707, value:1.0]\n> >> >\n> >> >\n> >> >  But i need only\n> >> >\n> >> > 35707\n> >> >\n> >> >\n> >> > without its value.\n> >> >\n> >> > So what can i do for this?\n> >> >\n> >> >\n> >> > Thanks,\n> >> > Jams\n> >>\n> >\n> >\n>\n \n\n
<1326826237.90124.YahooMailNeo@web39406.mail.mud.yahoo.com>	Suneel Marthi 	suneel_marthi@yahoo.com	Re: Help with RowSimilarityJob CLI	2012-01-17T18:50:37Z	<4F15510A.8000607@gmail.com>	Thanks Stan, I did get past this and the issue was that I had not updated my driver.classes.props in a while.\nI am good now.\n\n\n\n________________________________\n From: Ioan Eugen Stan <stan.ieugen@gmail.com>\nTo: user@mahout.apache.org \nSent: Tuesday, January 17, 2012 5:44 AM\nSubject: Re: Help with RowSimilarityJob CLI\n \nPe 17.01.2012 03:54, Suneel Marthi a scris:\n> I am looking to use RowSimilarityJob for determining the similarity of 2 documents (using Cosine Distance Measure).\n>\n> I am working off of Mahout trunk and don't see the CLI for rowsimilarity (for /bin/mahout). I do see that the Mahout 0.5 has a rowsimilarity CLI.\n>\n> What am I doing wrong or am I missing something?\n>\n> Regards,\n> Suneel\n>\n\n\nHello Suneel,\n\nI'm also working on trunk and I see it. It's defined in \ndriver.classes.props. The class itself is in package \norg.apache.mahout.math.hadoop.similarity.cooccurrence .\n\nMaybe mahout cli doesn't find the props file on the class path. Do you \nalso have mahout math package?\n\nHope this helps,\n\n-- \nIoan Eugen Stan\nhttp://ieugen.blogspot.com \n\n
<728E915F-EBD1-4AB1-B862-8F9564FDE192@semanticartifacts.com>	David Boney 	list@semanticartifacts.com	Austin SIGKDD Hacker's Dojo - Next Meeting Wednesday, January 18, 2012, 7:00 - 8:00 pm	2012-01-18T04:53:54Z		The next meeting of Austin SIGKDD, Big Data Machine Learning Hackers Dojo, is Wednesday, January 11, 2012, 7:00 - 8:00 pm at the Northwest Recreation Center. We have the meeting room from 6-9. We will have a presentation from 7-8, with hacking from 6-7 and 8-9.\n\nWe have been having a great turnout, between 12-20, at every meeting. The Northwest Recreation Center has been a real blessing as a meeting place with a central location, free parking, and free WIFI. We can accommodate up to thirty with the current tables and chairs available. Sukant brought a projector to the last meeting, which provided a better presentation, as you can show actual running examples in addition to slide presentations.\n\nLast week David from PFI made a presentation of Giraph, a graph processing system built on Hadoop. I make a presentation of the source code of the word count program from the tutorial at the Hadoop web site. \n\nThis week David from PFI will continue in his presentation on Giraph, a graph processing system built on Hadoop. If we have time, I will start into a presentation on the Mahout architecture. I will not get to Naive Bayes this week, maybe next week.\n\nThere is now a database for presentation topics, Database > Presentation Topics, at the Austin SIGKDD web site. You can add your name and date to present a topic and/or add additional topics that interest you.\n\nThere are two items for which I would like hold a vote at the meeting. \n\nThe first is a formal vote to be come an ACM SIGKDD chapter. If the majority are in agreement, we will need three or more volunteers who are to be both ACM and SIGKDD members to be the officers, chair, vice chair, and secretary/treasurer. I know there are at least myself and another person attending the meeting who are both ACM and SIGKDD members, and another person has volunteered who does not attend the meetings. We also need ten people total to form the chapter.\n\nThe second item is headhunters and job postings. I would like to see which way the winds are blowing with the group. We had a posting this week by a headhunter to the mailing list. I am not opposed if they are Hadoop or machine learning jobs but others may consider this as just additional spam they have to sort through. I want to keep the group membership open and don't want to review messages before posting, I have better things to do with my time, but I can delete the postings after the fact and kick headhunters off the list if this is what the majority of the group wants.\n\nPlease bring you laptop, a power cord, and a power strip. We need more POWER STRIPS for the meeting.\n\nPlease join Austin SIGKDD to continue receiving announcements.\nHere are the details on austinsigkdd:\nGroup home page: http://groups.yahoo.com/group/austinsigkdd \nGroup email address: austinsigkdd@yahoogroups.com \n\nNorthwest Recreation Center\nhttp://www.ci.austin.tx.us/parks/northwest.htm\n\nP.S. Sukant, please remember to bring your projector. \n\n
<65F573E0-D2B1-462D-8AC4-57136E3FD503@semanticartifacts.com>	David Boney 	list@semanticartifacts.com	TYPO CORRECTION - Austin SIGKDD Hacker's Dojo - Next Meeting Wednesday, January 18, 2012, 7:00 - 8:00 pm	2012-01-18T04:59:46Z		Sorry, I left last week's date in the previous version of this message. This is now corrected.\n-----------------------------------------------------------------\nThe next meeting of Austin SIGKDD, Big Data Machine Learning Hackers Dojo, is Wednesday, January 18, 2012, 7:00 - 8:00 pm at the Northwest Recreation Center. We have the meeting room from 6-9. We will have a presentation from 7-8, with hacking from 6-7 and 8-9.\n\nWe have been having a great turnout, between 12-20, at every meeting. The Northwest Recreation Center has been a real blessing as a meeting place with a central location, free parking, and free WIFI. We can accommodate up to thirty with the current tables and chairs available. Sukant brought a projector to the last meeting, which provided a better presentation, as you can show actual running examples in addition to slide presentations.\n\nLast week David from PFI made a presentation of Giraph, a graph processing system built on Hadoop. I make a presentation of the source code of the word count program from the tutorial at the Hadoop web site. \n\nThis week David from PFI will continue in his presentation on Giraph, a graph processing system built on Hadoop. If we have time, I will start into a presentation on the Mahout architecture. I will not get to Naive Bayes this week, maybe next week.\n\nThere is now a database for presentation topics, Database > Presentation Topics, at the Austin SIGKDD web site. You can add your name and date to present a topic and/or add additional topics that interest you.\n\nThere are two items for which I would like hold a vote at the meeting. \n\nThe first is a formal vote to be come an ACM SIGKDD chapter. If the majority are in agreement, we will need three or more volunteers who are to be both ACM and SIGKDD members to be the officers, chair, vice chair, and secretary/treasurer. I know there are at least myself and another person attending the meeting who are both ACM and SIGKDD members, and another person has volunteered who does not attend the meetings. We also need ten people total to form the chapter.\n\nThe second item is headhunters and job postings. I would like to see which way the winds are blowing with the group. We had a posting this week by a headhunter to the mailing list. I am not opposed if they are Hadoop or machine learning jobs but others may consider this as just additional spam they have to sort through. I want to keep the group membership open and don't want to review messages before posting, I have better things to do with my time, but I can delete the postings after the fact and kick headhunters off the list if this is what the majority of the group wants.\n\nPlease bring you laptop, a power cord, and a power strip. We need more POWER STRIPS for the meeting.\n\nPlease join Austin SIGKDD to continue receiving announcements.\nHere are the details on austinsigkdd:\nGroup home page: http://groups.yahoo.com/group/austinsigkdd \nGroup email address: austinsigkdd@yahoogroups.com \n\nNorthwest Recreation Center\nhttp://www.ci.austin.tx.us/parks/northwest.htm\n\nP.S. Sukant, please remember to bring your projector. \n\n
<CAPbe6MPusbaZ7qsfLnmcYydFWm9jrM+59pLijRQvQcEd_GU5Cw@mail.gmail.com>	刘鎏 	liuliu.tju@gmail.com	About QRDecomposition	2012-01-18T09:39:21Z		Hi,\nWhen I run QRDecomposition in mahout , I find the result of Q or R is\ndifferent from the example in wiki(\nhttp://en.wikipedia.org/wiki/QR_decomposition#Using_Householder_reflections).\nAfter reading the source code, I find the implement of QR decomposition is\nexactly different from the tradition way such as the procedure in wiki.\nCould any one show why mahout implement it in such a way? Thanks for your\nreplies!\n\nLiu Liu\n \n\n
<E3B4FFD6E695EE408BB1945B129CAF58A8ADC0E05C@TMG-E2K7CCR2.group.tradermedia.co.uk>	Martin Wignall 	Martin.Wignall@autotrader.co.uk	error running a RecommenderJob	2012-01-18T12:40:30Z		Hi,\nI am new to mahout, and am working my way through the Mahout In Action book. Now, I'm trying to execute a RecommenderJob, and see the following error:\n\nException in thread "main" java.lang.NoClassDefFoundError: org/apache/commons/cli2/Option\n       at java.lang.Class.forName0(Native Method)\n       at java.lang.Class.forName(Class.java:264)\n       at org.apache.hadoop.util.RunJar.main(RunJar.java:149)\nCaused by: java.lang.ClassNotFoundException: org.apache.commons.cli2.Option\n       at java.net.URLClassLoader$1.run(URLClassLoader.java:217)\n       at java.security.AccessController.doPrivileged(Native Method)\n       at java.net.URLClassLoader.findClass(URLClassLoader.java:205)\n       at java.lang.ClassLoader.loadClass(ClassLoader.java:321)\n       at java.lang.ClassLoader.loadClass(ClassLoader.java:266)\n       ... 3 more\n\nThe command I'm executing is as follows:\n\nhadoop jar target/mahout-core-0.6-SNAPSHOT.jar\norg.apache.mahout.cf.taste.hadoop.item.RecommenderJob\n-Dmapred.input.dir=input/input.txt\n-Dmapred.output.dir=output --usersFile input/users.txt --booleanData\n\nThis differs slightly from the command prescribed in the Mahout In Action book:\n\n\n\nbin/hadoop jar target/mahout-core-0.4-SNAPSHOT.job\n\norg.apache.mahout.cf.taste.hadoop.item.RecommenderJob\n\n-Dmapred.input.dir=input/input.txt\n-Dmapred.output.dir=output --usersFile input/users.txt --booleanData\n\nObvious differences are (i) I'm on Mahout 0.6, and (ii) I don't have a .job file to reference, only a .jar (target/mahout-core-0.6-SNAPSHOT.jar). I don't know if I should be referencing the .jar file in this way.\n\n\nNow, I have built the jar myself, by executing:\n\nmvn -DskipTests clean package\nfrom the core directory of Mahout.\n\nThis appears to have worked fine, and I see the following in the core/target directory\n\ndrwxrwxr-x 6 martin martin    4096 2012-01-18 12:24 ./\ndrwxrwxr-x 8 martin martin    4096 2012-01-18 12:24 ../\ndrwxrwxr-x 3 martin martin    4096 2012-01-18 12:24 classes/\ndrwxrwxr-x 4 martin martin    4096 2012-01-18 12:24 generated-sources/\n-rw-rw-r-- 1 martin martin 1641278 2012-01-18 12:24 mahout-core-0.6-SNAPSHOT.jar\ndrwxrwxr-x 2 martin martin    4096 2012-01-18 12:24 maven-archiver/\ndrwxrwxr-x 3 martin martin    4096 2012-01-18 12:24 test-classes/\n\nNotice that there is no .job file, only a .jar.\n\nI'm not sure if the problem I'm seeing is due to the lack of a .job file, or if it's something else that I've got wrong.\n\nPlease can you help explain why I can't run the RecommenderJob?\n\nMany thanks,\n\nMartin\n\n\n\n________________________________________\n\n\nThis e-mail is sent on behalf of Trader Media Group Limited, Registered Office: Auto Trader House, Cutbush Park Industrial Estate, Danehill, Lower Earley, Reading, Berkshire, RG6 4UT(Registered in England No. 4768833). This email and any files transmitted with it are confidential and may be legally privileged, and intended solely for the use of the individual or entity to whom they are addressed. If you have received this email in error please notify the sender. This email message has been swept for the presence of computer viruses. \n\n \n\n
<CAEccTyykefGRYbAEYgQrzRbQaTaj0Uybc5G8y8WC0CRZLM4PyQ@mail.gmail.com>	Sean Owen 	srowen@gmail.com	Re: error running a RecommenderJob	2012-01-18T12:42:40Z	<E3B4FFD6E695EE408BB1945B129CAF58A8ADC0E05C@TMG-E2K7CCR2.group.tradermedia.co.uk>	You do need a .job file and it's produced the same way, with mvn package.\n\nOn Wed, Jan 18, 2012 at 12:40 PM, Martin Wignall <\nMartin.Wignall@autotrader.co.uk> wrote:\n\n> Hi,\n> I am new to mahout, and am working my way through the Mahout In Action\n> book. Now, I'm trying to execute a RecommenderJob, and see the following\n> error:\n>\n> Exception in thread "main" java.lang.NoClassDefFoundError:\n> org/apache/commons/cli2/Option\n>       at java.lang.Class.forName0(Native Method)\n>       at java.lang.Class.forName(Class.java:264)\n>       at org.apache.hadoop.util.RunJar.main(RunJar.java:149)\n> Caused by: java.lang.ClassNotFoundException: org.apache.commons.cli2.Option\n>       at java.net.URLClassLoader$1.run(URLClassLoader.java:217)\n>       at java.security.AccessController.doPrivileged(Native Method)\n>       at java.net.URLClassLoader.findClass(URLClassLoader.java:205)\n>       at java.lang.ClassLoader.loadClass(ClassLoader.java:321)\n>       at java.lang.ClassLoader.loadClass(ClassLoader.java:266)\n>       ... 3 more\n>\n> The command I'm executing is as follows:\n>\n> hadoop jar target/mahout-core-0.6-SNAPSHOT.jar\n> org.apache.mahout.cf.taste.hadoop.item.RecommenderJob\n> -Dmapred.input.dir=input/input.txt\n> -Dmapred.output.dir=output --usersFile input/users.txt --booleanData\n>\n> This differs slightly from the command prescribed in the Mahout In Action\n> book:\n>\n>\n>\n> bin/hadoop jar target/mahout-core-0.4-SNAPSHOT.job\n>\n> org.apache.mahout.cf.taste.hadoop.item.RecommenderJob\n>\n> -Dmapred.input.dir=input/input.txt\n> -Dmapred.output.dir=output --usersFile input/users.txt --booleanData\n>\n> Obvious differences are (i) I'm on Mahout 0.6, and (ii) I don't have a\n> .job file to reference, only a .jar (target/mahout-core-0.6-SNAPSHOT.jar).\n> I don't know if I should be referencing the .jar file in this way.\n>\n>\n> Now, I have built the jar myself, by executing:\n>\n> mvn -DskipTests clean package\n> from the core directory of Mahout.\n>\n> This appears to have worked fine, and I see the following in the\n> core/target directory\n>\n> drwxrwxr-x 6 martin martin    4096 2012-01-18 12:24 ./\n> drwxrwxr-x 8 martin martin    4096 2012-01-18 12:24 ../\n> drwxrwxr-x 3 martin martin    4096 2012-01-18 12:24 classes/\n> drwxrwxr-x 4 martin martin    4096 2012-01-18 12:24 generated-sources/\n> -rw-rw-r-- 1 martin martin 1641278 2012-01-18 12:24\n> mahout-core-0.6-SNAPSHOT.jar\n> drwxrwxr-x 2 martin martin    4096 2012-01-18 12:24 maven-archiver/\n> drwxrwxr-x 3 martin martin    4096 2012-01-18 12:24 test-classes/\n>\n> Notice that there is no .job file, only a .jar.\n>\n> I'm not sure if the problem I'm seeing is due to the lack of a .job file,\n> or if it's something else that I've got wrong.\n>\n> Please can you help explain why I can't run the RecommenderJob?\n>\n> Many thanks,\n>\n> Martin\n>\n>\n>\n> ________________________________________\n>\n>\n> This e-mail is sent on behalf of Trader Media Group Limited, Registered\n> Office: Auto Trader House, Cutbush Park Industrial Estate, Danehill, Lower\n> Earley, Reading, Berkshire, RG6 4UT(Registered in England No. 4768833).\n> This email and any files transmitted with it are confidential and may be\n> legally privileged, and intended solely for the use of the individual or\n> entity to whom they are addressed. If you have received this email in error\n> please notify the sender. This email message has been swept for the\n> presence of computer viruses.\n>\n>\n \n\n
<E3B4FFD6E695EE408BB1945B129CAF58A8ADC0E069@TMG-E2K7CCR2.group.tradermedia.co.uk>	Martin Wignall 	Martin.Wignall@autotrader.co.uk	RE: error running a RecommenderJob	2012-01-18T12:51:14Z	<CAEccTyykefGRYbAEYgQrzRbQaTaj0Uybc5G8y8WC0CRZLM4PyQ@mail.gmail.com>	Thanks Sean. Any idea why:\n\nmvn -DskipTests clean package\n\n(or\nmvn -DskipTests package\n)\n\nmight not have created the .job file? Is there something else I need to do?\n\nHere's a view from my screen:\n\nmartin@ubuntu:~/mahout/trunk/core$ mvn -DskipTests package\n[INFO] Scanning for projects...\n[INFO] ------------------------------------------------------------------------\n[INFO] Building Mahout Core\n[INFO]    task-segment: [package]\n[INFO] ------------------------------------------------------------------------\n[INFO] [resources:resources {execution: default-resources}]\n[INFO] Using 'UTF-8' encoding to copy filtered resources.\n[INFO] Copying 1 resource\n[INFO] [compiler:compile {execution: default-compile}]\n[INFO] Compiling 703 source files to /home/martin/mahout/trunk/core/target/classes\n[INFO] [resources:testResources {execution: default-testResources}]\n[INFO] Using 'UTF-8' encoding to copy filtered resources.\n[INFO] Copying 5 resources\n[INFO] [compiler:testCompile {execution: default-testCompile}]\n[INFO] Compiling 231 source files to /home/martin/mahout/trunk/core/target/test-classes\n[INFO] [surefire:test {execution: default-test}]\n[INFO] Tests are skipped.\n[INFO] [jar:jar {execution: default-jar}]\n[INFO] Building jar: /home/martin/mahout/trunk/core/target/mahout-core-0.6-SNAPSHOT.jar\n[INFO] ------------------------------------------------------------------------\n[INFO] BUILD SUCCESSFUL\n[INFO] ------------------------------------------------------------------------\n[INFO] Total time: 20 seconds\n[INFO] Finished at: Wed Jan 18 12:48:27 GMT 2012\n[INFO] Final Memory: 38M/388M\n[INFO] ------------------------------------------------------------------------\nmartin@ubuntu:~/mahout/trunk/core$\nmartin@ubuntu:~/mahout/trunk/core$\nmartin@ubuntu:~/mahout/trunk/core$\nmartin@ubuntu:~/mahout/trunk/core$\nmartin@ubuntu:~/mahout/trunk/core$\nmartin@ubuntu:~/mahout/trunk/core$\nmartin@ubuntu:~/mahout/trunk/core$ ll target/\ntotal 1628\ndrwxrwxr-x 6 martin martin    4096 2012-01-18 12:48 ./\ndrwxrwxr-x 8 martin martin    4096 2012-01-18 12:24 ../\ndrwxrwxr-x 3 martin martin    4096 2012-01-18 12:48 classes/\ndrwxrwxr-x 4 martin martin    4096 2012-01-18 12:48 generated-sources/\n-rw-rw-r-- 1 martin martin 1641278 2012-01-18 12:48 mahout-core-0.6-SNAPSHOT.jar\ndrwxrwxr-x 2 martin martin    4096 2012-01-18 12:48 maven-archiver/\ndrwxrwxr-x 3 martin martin    4096 2012-01-18 12:48 test-classes/\n\n\n\nMany thanks in advance!\n\nMartin\n\n-----Original Message-----\nFrom: Sean Owen [mailto:srowen@gmail.com]\nSent: 18 January 2012 12:43\nTo: user@mahout.apache.org\nSubject: Re: error running a RecommenderJob\n\nYou do need a .job file and it's produced the same way, with mvn package.\n\nOn Wed, Jan 18, 2012 at 12:40 PM, Martin Wignall < Martin.Wignall@autotrader.co.uk<mailto:Martin.Wignall@autotrader.co.uk>> wrote:\n\n> Hi,\n> I am new to mahout, and am working my way through the Mahout In Action\n> book. Now, I'm trying to execute a RecommenderJob, and see the\n> following\n> error:\n>\n> Exception in thread "main" java.lang.NoClassDefFoundError:\n> org/apache/commons/cli2/Option\n>       at java.lang.Class.forName0(Native Method)\n>       at java.lang.Class.forName(Class.java:264)\n>       at org.apache.hadoop.util.RunJar.main(RunJar.java:149)\n> Caused by: java.lang.ClassNotFoundException: org.apache.commons.cli2.Option\n>       at java.net.URLClassLoader$1.run(URLClassLoader.java:217)\n>       at java.security.AccessController.doPrivileged(Native Method)\n>       at java.net.URLClassLoader.findClass(URLClassLoader.java:205)\n>       at java.lang.ClassLoader.loadClass(ClassLoader.java:321)\n>       at java.lang.ClassLoader.loadClass(ClassLoader.java:266)\n>       ... 3 more\n>\n> The command I'm executing is as follows:\n>\n> hadoop jar target/mahout-core-0.6-SNAPSHOT.jar\n> org.apache.mahout.cf.taste.hadoop.item.RecommenderJob\n> -Dmapred.input.dir=input/input.txt\n> -Dmapred.output.dir=output --usersFile input/users.txt --booleanData\n>\n> This differs slightly from the command prescribed in the Mahout  In\n> Action\n> book:\n>\n>\n>\n> bin/hadoop jar target/mahout-core-0.4-SNAPSHOT.job\n>\n> org.apache.mahout.cf.taste.hadoop.item.RecommenderJob\n>\n> -Dmapred.input.dir=input/input.txt\n> -Dmapred.output.dir=output --usersFile input/users.txt --booleanData\n>\n> Obvious differences are (i) I'm on Mahout 0.6, and (ii) I don't have a\n> .job file to reference, only a .jar (target/mahout-core-0.6-SNAPSHOT.jar).\n> I don't know if I should be referencing the .jar file in this way.\n>\n>\n> Now, I have built the jar myself, by executing:\n>\n> mvn -DskipTests clean package\n> from the core directory of Mahout.\n>\n> This appears to have worked fine, and I see the following in the\n> core/target directory\n>\n> drwxrwxr-x 6 martin martin    4096 2012-01-18 12:24 ./\n> drwxrwxr-x 8 martin martin    4096 2012-01-18 12:24 ../\n> drwxrwxr-x 3 martin martin    4096 2012-01-18 12:24 classes/\n> drwxrwxr-x 4 martin martin    4096 2012-01-18 12:24 generated-sources/\n> -rw-rw-r-- 1 martin martin 1641278 2012-01-18 12:24\n> mahout-core-0.6-SNAPSHOT.jar\n> drwxrwxr-x 2 martin martin    4096 2012-01-18 12:24 maven-archiver/\n> drwxrwxr-x 3 martin martin    4096 2012-01-18 12:24 test-classes/\n>\n> Notice that there is no .job file, only a .jar.\n>\n> I'm not sure if the problem I'm seeing is due to the lack of a .job\n> file, or if it's something else that I've got wrong.\n>\n> Please can you help explain why I can't run the RecommenderJob?\n>\n> Many thanks,\n>\n> Martin\n>\n>\n>\n> ________________________________________\n>\n>\n> This e-mail is sent on behalf of Trader Media Group Limited,\n> Registered\n> Office: Auto Trader House, Cutbush Park Industrial Estate, Danehill,\n> Lower Earley, Reading, Berkshire, RG6 4UT(Registered in England No. 4768833).\n> This email and any files transmitted with it are confidential and may\n> be legally privileged, and intended solely for the use of the\n> individual or entity to whom they are addressed. If you have received\n> this email in error please notify the sender. This email message has\n> been swept for the presence of computer viruses.\n>\n>\n\n\n________________________________________\n\n\nThis e-mail is sent on behalf of Trader Media Group Limited, Registered Office: Auto Trader House, Cutbush Park Industrial Estate, Danehill, Lower Earley, Reading, Berkshire, RG6 4UT(Registered in England No. 4768833). This email and any files transmitted with it are confidential and may be legally privileged, and intended solely for the use of the individual or entity to whom they are addressed. If you have received this email in error please notify the sender. This email message has been swept for the presence of computer viruses. \n\n \n\n
<CAEccTyy3OBdQCiD1WtH7XAcb6Euy+WGeTqBB2bn4b69xQCYqbA@mail.gmail.com>	Sean Owen 	srowen@gmail.com	Re: error running a RecommenderJob	2012-01-18T13:03:59Z	<E3B4FFD6E695EE408BB1945B129CAF58A8ADC0E069@TMG-E2K7CCR2.group.tradermedia.co.uk>	The exact same comment generates the job file for me... I see a lot more\noutput though, as it does the work of packaging the job file.\n\nI wonder if it's a Maven version thing (I use 3.0.3 on OS X 10.7) or, do\nyou have all the other files in the project, above core/? I'd expect it to\njust outright fail if either of those were at issue though.\n\nRun package from the project root? wild guess. That shouldn't be needed.\n\nOn Wed, Jan 18, 2012 at 12:51 PM, Martin Wignall <\nMartin.Wignall@autotrader.co.uk> wrote:\n\n> Thanks Sean. Any idea why:\n>\n> mvn -DskipTests clean package\n>\n> (or\n> mvn -DskipTests package\n> )\n>\n> might not have created the .job file? Is there something else I need to do?\n>\n>\n \n\n
<E3B4FFD6E695EE408BB1945B129CAF58A8ADC0E0CC@TMG-E2K7CCR2.group.tradermedia.co.uk>	Martin Wignall 	Martin.Wignall@autotrader.co.uk	RE: error running a RecommenderJob	2012-01-18T13:55:08Z	<CAEccTyy3OBdQCiD1WtH7XAcb6Euy+WGeTqBB2bn4b69xQCYqbA@mail.gmail.com>	Thanks for your response Sean. Only other thing I can think of is I had to amend my pom.xml - I removed\n\n<plugin>\n <groupId>org.eclipse.m2e</groupId>\n <artifactId>lifecycle-mapping</artifactId>\n <version>1.0.0</version>\n <configuration>\n <lifecycleMappingMetadata>\n <pluginExecutions>\n <pluginExecution>\n <pluginExecutionFilter>\n <groupId>org.apache.maven.plugins</groupId>\n <artifactId>maven-dependency-plugin</artifactId>\n <versionRange>[2.0,)</versionRange>\n <goals>\n <goal>copy-dependencies</goal>\n </goals>\n </pluginExecutionFilter>\n <action>\n <ignore />\n </action>\n </pluginExecution>\n </pluginExecutions>\n </lifecycleMappingMetadata>\n </configuration>\n </plugin>\n\nfrom <plugins>, because\n\nmvn clean package\n\ndid not work from the command line. I got the following error:\n\nmvn clean package\n[INFO] Scanning for projects...\n[INFO] ------------------------------------------------------------------------\n[INFO] Building Mahout Core\n[INFO]    task-segment: [clean, package]\n[INFO] ------------------------------------------------------------------------\nDownloading: http://repo1.maven.org/maven2/org/eclipse/m2e/lifecycle-mapping/1.0.0/lifecycle-mapping-1.0.0.pom\n[INFO] Unable to find resource 'org.eclipse.m2e:lifecycle-mapping:pom:1.0.0' in repository central (http://repo1.maven.org/maven2)\nDownloading: http://repo1.maven.org/maven2/org/eclipse/m2e/lifecycle-mapping/1.0.0/lifecycle-mapping-1.0.0.pom\n[INFO] Unable to find resource 'org.eclipse.m2e:lifecycle-mapping:pom:1.0.0' in repository central (http://repo1.maven.org/maven2)\n[INFO] ------------------------------------------------------------------------\n[ERROR] BUILD ERROR\n[INFO] ------------------------------------------------------------------------\n[INFO] Error building POM (may not be this project's POM).\n\n\nProject ID: org.eclipse.m2e:lifecycle-mapping\n\nReason: POM 'org.eclipse.m2e:lifecycle-mapping' not found in repository: Unable to download the artifact from any repository\n\n  org.eclipse.m2e:lifecycle-mapping:pom:1.0.0\n\nfrom the specified remote repositories:\n  apache.snapshots (http://repository.apache.org/snapshots),\n  central (http://repo1.maven.org/maven2)\n\n for project org.eclipse.m2e:lifecycle-mapping\n\n\nThe above change to my pom.xml fixed the problem and allowed me to package. I don’t imagine that this is related. Do you? (Just trying to rule it out)\n\n\n\nIn answer to your questions, I’m on maven version 2.2.1, and Ubuntu 11.10\n\nmvn --version\nApache Maven 2.2.1 (rdebian-6)\nJava version: 1.6.0_23\nJava home: /usr/lib/jvm/java-6-openjdk/jre\nDefault locale: en_GB, platform encoding: UTF-8\nOS name: "linux" version: "3.0.0-14-generic-pae" arch: "i386" Family: "unix"\n\n\nAnd in terms of my mahout structure, I just downloaded trunk:\n\nmartin@ubuntu:~/mahout/trunk$ ll\ntotal 168\ndrwxrwxr-x 14 martin martin  4096 2012-01-09 16:53 ./\ndrwxrwxr-x  3 martin martin  4096 2012-01-06 16:10 ../\ndrwxrwxr-x  3 martin martin  4096 2012-01-06 16:11 bin/\ndrwxrwxr-x  6 martin martin  4096 2012-01-09 15:54 buildtools/\n-rw-rw-r--  1 martin martin 14653 2012-01-09 17:48 .classpath\ndrwxrwxr-x  8 martin martin  4096 2012-01-18 12:24 core/\ndrwxrwxr-x  5 martin martin  4096 2012-01-09 15:54 distribution/\n-rw-rw-r--  1 martin martin  2320 2012-01-06 16:11 doap_Mahout.rdf\ndrwxrwxr-x  7 martin martin  4096 2012-01-09 15:54 examples/\n-rw-rw-r--  1 martin martin   345 2012-01-06 16:11 .gitignore\ndrwxrwxr-x  8 martin martin  4096 2012-01-09 15:54 integration/\n-rw-rw-r--  1 martin martin 39588 2012-01-06 16:10 LICENSE.txt\ndrwxrwxr-x  7 martin martin  4096 2012-01-09 16:01 math/\ndrwxrwxr-x  5 martin martin  4096 2012-01-10 11:22 mw-play/\n-rw-rw-r--  1 martin martin  1888 2012-01-06 16:10 NOTICE.txt\n-rw-rw-r--  1 martin martin 32289 2012-01-06 16:11 pom.xml\n-rw-rw-r--  1 martin martin   535 2012-01-09 16:41 .project\n-rw-rw-r--  1 martin martin  1655 2012-01-09 16:00 .project~\n-rw-rw-r--  1 martin martin  1200 2012-01-06 16:11 README.txt\ndrwxrwxr-x  2 martin martin  4096 2012-01-09 15:58 .settings/\ndrwxrwxr-x  5 m artin martin  4096 2012-01-06 16:10 src/\ndrwxrwxr-x  6 martin martin  4096 2012-01-06 16:10 .svn/\ndrwxrwxr-x  3 martin martin  4096 2012-01-06 16:10 target/\n\n\n\nOnly other thing I can think of is I have hadoop version hadoop-0.20.205.0. (I say that because the error I see is when running hadoop after all…\n\nhadoop jar target/mahout-core-0.6-SNAPSHOT.jar\norg.apache.mahout.cf.taste.hadoop.item.RecommenderJob\n-Dmapred.input.dir=input/input.txt\n-Dmapred.output.dir=output --usersFile input/users.txt --booleanData\n\ngives\n\nException in thread "main" java.lang.NoClassDefFoundError: org/apache/commons/cli2/Option\n       at java.lang.Class.forName0(Native Method)\n       at java.lang.Class.forName(Class.java:264)\n       at org.apache.hadoop.util.RunJar.main(RunJar.java:149)\nCaused by: java.lang.ClassNotFoundException: org.apache.commons.cli2.Option\n       at java.net.URLClassLoader$1.run(URLClassLoader.java:217)\n       at java.security.AccessController.doPrivileged(Native Method)\n       at java.net.URLClassLoader.findClass(URLClassLoader.java:205)\n       at java.lang.ClassLoader.loadClass(ClassLoader.java:321)\n       at java.lang.ClassLoader.loadClass(ClassLoader.java:266)\n       ... 3 more\n)\n\n\nAnything in my above setup look incorrect?\n\nThanks,\n\nMartin Wignall\nApplication Architect\nTrader Media Digital\nRedwood House\nWoodlands Business Park\nNewton-le-Willows\nWA12 0HE\n01925 296 245\n07525 405 281\nmartin.wignall@autotrader.co.uk\n\n\n-----Original Message-----\nFrom: Sean Owen [mailto:srowen@gmail.com]\nSent: 18 January 2012 13:04\nTo: user@mahout.apache.org\nSubject: Re: error running a RecommenderJob\n\nThe exact same comment generates the job file for me... I see a lot more output though, as it does the work of packaging the job file.\n\nI wonder if it's a Maven version thing (I use 3.0.3 on OS X 10.7) or, do you have all the other files in the project, above core/? I'd expect it to just outright fail if either of those were at issue though.\n\nRun package from the project root? wild guess. That shouldn't be needed.\n\nOn Wed, Jan 18, 2012 at 12:51 PM, Martin Wignall < Martin.Wignall@autotrader.co.uk<mailto:Martin.Wignall@autotrader.co.uk>> wrote:\n\n> Thanks Sean. Any idea why:\n>\n> mvn -DskipTests clean package\n>\n> (or\n> mvn -DskipTests package\n> )\n>\n> might not have created the .job file? Is there something else I need to do?\n>\n>\n\n\n________________________________________\n\n\nThis e-mail is sent on behalf of Trader Media Group Limited, Registered Office: Auto Trader House, Cutbush Park Industrial Estate, Danehill, Lower Earley, Reading, Berkshire, RG6 4UT(Registered in England No. 4768833). This email and any files transmitted with it are confidential and may be legally privileged, and intended solely for the use of the individual or entity to whom they are addressed. If you have received this email in error please notify the sender. This email message has been swept for the presence of computer viruses. \n\n \n\n
<CAEccTyyW7mkuPtJbU=udZ2dHZ89RywYHp+Rb9Gt7FX5gLk8vMA@mail.gmail.com>	Sean Owen 	srowen@gmail.com	Re: error running a RecommenderJob	2012-01-18T14:14:50Z	<E3B4FFD6E695EE408BB1945B129CAF58A8ADC0E0CC@TMG-E2K7CCR2.group.tradermedia.co.uk>	The Hadoop version isn't the issue as it's a ClassNotFoundException for a\nMahout dependency for sure.\n\nHmm I do think maybe the pom change is relevant. The original error\nconcerns some plugin, org.eclipse.m2e, but that is not present in the\nproject poms. are you sure you have the latest files from SVN, no\npossiblity of other files, no other modifications? This plugin does indeed\nnot exist, but I don't see any use of it when I build. That seems to be the\nsource of the issue.\n\nOn Wed, Jan 18, 2012 at 1:55 PM, Martin Wignall <\nMartin.Wignall@autotrader.co.uk> wrote:\n\n> Thanks for your response Sean. Only other thing I can think of is I had to\n> amend my pom.xml - I removed\n>\n> <plugin>\n>  <groupId>org.eclipse.m2e</groupId>\n>  <artifactId>lifecycle-mapping</artifactId>\n>  <version>1.0.0</version>\n>  <configuration>\n>  <lifecycleMappingMetadata>\n>  <pluginExecutions>\n>  <pluginExecution>\n>  <pluginExecutionFilter>\n>  <groupId>org.apache.maven.plugins</groupId>\n>  <artifactId>maven-dependency-plugin</artifactId>\n>  <versionRange>[2.0,)</versionRange>\n>  <goals>\n>  <goal>copy-dependencies</goal>\n>  </goals>\n>  </pluginExecutionFilter>\n>  <action>\n>  <ignore />\n>  </action>\n>  </pluginExecution>\n>  </pluginExecutions>\n>  </lifecycleMappingMetadata>\n>  </configuration>\n>  </plugin>\n>\n> from <plugins>, because\n>\n> mvn clean package\n>\n> did not work from the command line. I got the following error:\n>\n> mvn clean package\n> [INFO] Scanning for projects...\n> [INFO]\n> ------------------------------------------------------------------------\n> [INFO] Building Mahout Core\n> [INFO]    task-segment: [clean, package]\n> [INFO]\n> ------------------------------------------------------------------------\n> Downloading:\n> http://repo1.maven.org/maven2/org/eclipse/m2e/lifecycle-mapping/1.0.0/lifecycle-mapping-1.0.0.pom\n> [INFO] Unable to find resource\n> 'org.eclipse.m2e:lifecycle-mapping:pom:1.0.0' in repository central (\n> http://repo1.maven.org/maven2)\n> Downloading:\n> http://repo1.maven.org/maven2/org/eclipse/m2e/lifecycle-mapping/1.0.0/lifecycle-mapping-1.0.0.pom\n> [INFO] Unable to find resource\n> 'org.eclipse.m2e:lifecycle-mapping:pom:1.0.0' in repository central (\n> http://repo1.maven.org/maven2)\n> [INFO]\n> ------------------------------------------------------------------------\n> [ERROR] BUILD ERROR\n> [INFO]\n> ------------------------------------------------------------------------\n> [INFO] Error building POM (may not be this project's POM).\n>\n>\n> Project ID: org.eclipse.m2e:lifecycle-mapping\n>\n> Reason: POM 'org.eclipse.m2e:lifecycle-mapping' not found in repository:\n> Unable to download the artifact from any repository\n>\n>  org.eclipse.m2e:lifecycle-mapping:pom:1.0.0\n>\n> from the specified remote repositories:\n>  apache.snapshots (http://repository.apache.org/snapshots),\n>  central (http://repo1.maven.org/maven2)\n>\n>  for project org.eclipse.m2e:lifecycle-mapping\n>\n>\n> The above change to my pom.xml fixed the problem and allowed me to\n> package. I don’t imagine that this is related. Do you? (Just trying to rule\n> it out)\n>\n>\n>\n> In answer to your questions, I’m on maven version 2.2.1, and Ubuntu 11.10\n>\n> mvn --version\n> Apache Maven 2.2.1 (rdebian-6)\n> Java version: 1.6.0_23\n> Java home: /usr/lib/jvm/java-6-openjdk/jre\n> Default locale: en_GB, platform encoding: UTF-8\n> OS name: "linux" version: "3.0.0-14-generic-pae" arch: "i386" Family:\n> "unix"\n>\n>\n> And in terms of my mahout structure, I just downloaded trunk:\n>\n> martin@ubuntu:~/mahout/trunk$ ll\n> total 168\n> drwxrwxr-x 14 martin martin  4096 2012-01-09 16:53 ./\n> drwxrwxr-x  3 martin martin  4096 2012-01-06 16:10 ../\n> drwxrwxr-x  3 martin martin  4096 2012-01-06 16:11 bin/\n> drwxrwxr-x  6 martin martin  4096 2012-01-09 15:54 buildtools/\n> -rw-rw-r--  1 martin martin 14653 2012-01-09 17:48 .classpath\n> drwxrwxr-x  8 martin martin  4096 2012-01-18 12:24 core/\n> drwxrwxr-x  5 martin martin  4096 2012-01-09 15:54 distribution/\n> -rw-rw-r--  1 martin martin  2320 2012-01-06  16:11 doap_Mahout.rdf\n> drwxrwxr-x  7 martin martin  4096 2012-01-09 15:54 examples/\n> -rw-rw-r--  1 martin martin   345 2012-01-06 16:11 .gitignore\n> drwxrwxr-x  8 martin martin  4096 2012-01-09 15:54 integration/\n> -rw-rw-r--  1 martin martin 39588 2012-01-06 16:10 LICENSE.txt\n> drwxrwxr-x  7 martin martin  4096 2012-01-09 16:01 math/\n> drwxrwxr-x  5 martin martin  4096 2012-01-10 11:22 mw-play/\n> -rw-rw-r--  1 martin martin  1888 2012-01-06 16:10 NOTICE.txt\n> -rw-rw-r--  1 martin martin 32289 2012-01-06 16:11 pom.xml\n> -rw-rw-r--  1 martin martin   535 2012-01-09 16:41 .project\n> -rw-rw-r--  1 martin martin  1655 2012-01-09 16:00 .project~\n> -rw-rw-r--  1 martin martin  1200 2012-01-06 16:11 README.txt\n> drwxrwxr-x  2 martin martin  4096 2012-01-09 15:58 .settings/\n> drwxrwxr-x  5 martin martin  4096 2012-01-06 16:10 src/\n> drwxrwxr-x  6 martin martin  4096 2012-01-06 16:10 .svn/\n> drwxrwxr-x  3 martin martin  4096 2012-01-06 16:10 target/\n>\n>\n>\n> Only other thing I can think of is I have hadoop version\n> hadoop-0.20.205.0. (I say that because the error I see is when running\n> hadoop after all…\n>\n> hadoop jar target/mahout-core-0.6-SNAPSHOT.jar\n> org.apache.mahout.cf.taste.hadoop.item.RecommenderJob\n> -Dmapred.input.dir=input/input.txt\n> -Dmapred.output.dir=output --usersFile input/users.txt --booleanData\n>\n> gives\n>\n> Exception in thread "main" java.lang.NoClassDefFoundError:\n> org/apache/commons/cli2/Option\n>       at java.lang.Class.forName0(Native Method)\n>       at java.lang.Class.forName(Class.java:264)\n>       at org.apache.hadoop.util.RunJar.main(RunJar.java:149)\n> Caused by: java.lang.ClassNotFoundException: org.apache.commons.cli2.Option\n>       at java.net.URLClassLoader$1.run(URLClassLoader.java:217)\n>       at java.security.AccessController.doPrivileged(Native Method)\n>       at java.net.URLClassLoader.findClass(URLClassLoader.java:205)\n>       at java.lang.ClassLoader.loadClass(ClassLoader.java:321)\n>       at java.lang.ClassLoader.loadClass(ClassLoader.java:266)\n>       ... 3 more\n> )\n>\n>\n> Anything in my above setup look incorrect?\n>\n> Thanks,\n>\n> Martin Wignall\n> Application Architect\n> Trader Media Digital\n> Redwood House\n> Woodlands Business Park\n> Newton-le-Willows\n> WA12 0HE\n> 01925 296 245\n> 07525 405 281\n> martin.wignall@autotrader.co.uk\n>\n>\n> -----Original Message-----\n> From: Sean Owen [mailto:srowen@gmail.com]\n> Sent: 18 January 2012 13:04\n> To: user@mahout.apache.org\n> Subject: Re: error running a RecommenderJob\n>\n> The exact same comment generates the job file for me... I see a lot more\n> output though, as it does the work of packaging the job file.\n>\n> I wonder if it's a Maven version thing (I use 3.0.3 on OS X 10.7) or, do\n> you have all the other files in the project, above core/? I'd expect it to\n> just outright fail if either of those were at issue though.\n>\n> Run package from the project root? wild guess. That shouldn't be needed.\n>\n> On Wed, Jan 18, 2012 at 12:51 PM, Martin Wignall <\n> Martin.Wignall@autotrader.co.uk<mailto:Martin.Wignall@autotrader.co.uk>>\n> wrote:\n>\n> > Thanks Sean. Any idea why:\n> >\n> > mvn -DskipTests clean package\n> >\n> > (or\n> > mvn -DskipTests package\n> > )\n> >\n> > might not have created the .job file? Is there something else I need to\n> do?\n> >\n> >\n>\n>\n> ________________________________________\n>\n>\n> This e-mail is sent on behalf of Trader Media Group Limited, Registered\n> Office: Auto Trader House, Cutbush Park Industrial Estate, Danehill, Lower\n> Earley, Reading, Berkshire, RG6 4UT(Registered in England No. 4768833).\n> This email and any files transmitted with it are confidential and may be\n> legally privileged, and intended solely for the use of the individual or\n> entity to whom they are addressed. If you have received this email in error\n> please notify the sender. This email message has been swept for the\n> presence of computer viruses.\n>\n>\n \n\n
<CAONrwUHp==UoceS2uCr8iUFqG-DErqXuuKfH1C0AM231evBCHA@mail.gmail.com>	Raphael Bauduin 	rblists@gmail.com	Evaluating a recomender performance	2012-01-18T14:32:52Z		Hi,\n\nI'm looking to the Recomender documentation, and it looks quite\nstraight-forward to use.\nI'm using data from an ecommerce website and would like to build a\nmodel to suggest products based on past purchases ( not on ratings ).\nI'd like to be able to compare several options, however, I found no\nmention of evaluating the performance of the recommender.\nIn my case, the recommendation would be a success if the test user\n(not present in the learning data and thus used for building the\nmodel) actually bought (one of ) the suggested  product(s).\n\nDoes anyone have suggestions on performance measure for this kind of\nRecomender? I'm interested in any and all tips!\n\nThanks\n\nRaphaël\n\n \n
<E3B4FFD6E695EE408BB1945B129CAF58A8ADC0E132@TMG-E2K7CCR2.group.tradermedia.co.uk>	Martin Wignall 	Martin.Wignall@autotrader.co.uk	RE: error running a RecommenderJob	2012-01-18T14:43:27Z	<CAEccTyyW7mkuPtJbU=udZ2dHZ89RywYHp+Rb9Gt7FX5gLk8vMA@mail.gmail.com>	Hmmm, thanks Sean. I'll double check, and possibly re-download Mahout from scratch.\n\nThanks for your help!\n\nMartin \n\n-----Original Message-----\nFrom: Sean Owen [mailto:srowen@gmail.com] \nSent: 18 January 2012 14:15\nTo: user@mahout.apache.org\nSubject: Re: error running a RecommenderJob\n\nThe Hadoop version isn't the issue as it's a ClassNotFoundException for a Mahout dependency for sure.\n\nHmm I do think maybe the pom change is relevant. The original error concerns some plugin, org.eclipse.m2e, but that is not present in the project poms. are you sure you have the latest files from SVN, no possiblity of other files, no other modifications? This plugin does indeed not exist, but I don't see any use of it when I build. That seems to be the source of the issue.\n\nOn Wed, Jan 18, 2012 at 1:55 PM, Martin Wignall < Martin.Wignall@autotrader.co.uk> wrote:\n\n> Thanks for your response Sean. Only other thing I can think of is I \n> had to amend my pom.xml - I removed\n>\n> <plugin>\n>  <groupId>org.eclipse.m2e</groupId>\n>  <artifactId>lifecycle-mapping</artifactId>\n>  <version>1.0.0</version>\n>  <configuration>\n>  <lifecycleMappingMetadata>\n>  <pluginExecutions>\n>  <pluginExecution>\n>  <pluginExecutionFilter>\n>  <groupId>org.apache.maven.plugins</groupId>\n>  <artifactId>maven-dependency-plugin</artifactId>\n>  <versionRange>[2.0,)</versionRange>\n>  <goals>\n>  <goal>copy-dependencies</goal>\n>  </goals>\n>  </pluginExecutionFilter>\n>  <action>\n>  <ignore />\n>  </action>\n>  </pluginExecution>\n>  </pluginExecutions>\n>  </lifecycleMappingMetadata>\n>  </configuration>\n>  </plugin>\n>\n> from <plugins>, because\n>\n> mvn clean package\n>\n> did not work from the command line. I got the following error:\n>\n> mvn clean package\n> [INFO] Scanning for projects...\n> [INFO]\n> ----------------------------------------------------------------------\n> --\n> [INFO] Building Mahout Core\n> [INFO]    task-segment: [clean, package]\n> [INFO]\n> ----------------------------------------------------------------------\n> --\n> Downloading:\n> http://repo1.maven.org/maven2/org/eclipse/m2e/lifecycle-mapping/1.0.0/\n> lifecycle-mapping-1.0.0.pom\n> [INFO] Unable to find resource\n> 'org.eclipse.m2e:lifecycle-mapping:pom:1.0.0' in repository central (\n> http://repo1.maven.org/maven2)\n> Downloading:\n> http://repo1.maven.org/maven2/org/eclipse/m2e/lifecycle-mapping/1.0.0/\n> lifecycle-mapping-1.0.0.pom\n> [INFO] Unable to find resource\n> 'org.eclipse.m2e:lifecycle-mapping:pom:1.0.0' in repository central (\n> http://repo1.maven.org/maven2)\n> [INFO]\n> ----------------------------------------------------------------------\n> --\n> [ERROR] BUILD ERROR\n> [INFO]\n> ----------------------------------------------------------------------\n> -- [INFO] Error building POM (may not be this project's POM).\n>\n>\n> Project ID: org.eclipse.m2e:lifecycle-mapping\n>\n> Reason: POM 'org.eclipse.m2e:lifecycle-mapping' not found in repository:\n> Unable to download the artifact from any repository\n>\n>  org.eclipse.m2e:lifecycle-mapping:pom:1.0.0\n>\n> from the specified remote repositories:\n>  apache.snapshots (http://repository.apache.org/snapshots),\n>  central (http://repo1.maven.org/maven2)\n>\n>  for project org.eclipse.m2e:lifecycle-mapping\n>\n>\n> The above change to my pom.xml fixed the problem and allowed me to \n> package. I don’t imagine that this is related. Do you? (Just trying to \n> rule it out)\n>\n>\n>\n> In answer to your questions, I’m on maven version 2.2.1, and Ubuntu \n> 11.10\n>\n> mvn --version\n> Apache Maven 2.2.1 (rdebian-6)\n> Java version: 1.6.0_23\n> Java home: /usr/lib/jvm/java-6-openjdk/jre Default locale: en_GB, \n> platform encoding: UTF-8 OS name: "linux" version: \n> "3.0.0-14-generic-pae" arch: "i386" Family:\n> "unix"\n>\n>\n> And in terms of my mahout structure, I just downloaded trunk:\n>\n> martin@ubuntu:~/mahout/trunk$ ll\n> total 168\n> drwxrwxr-x 14 martin martin  4096 2012-01-09 16:53 ./ drwxrwxr-x  3 \n> martin martin  4096 2012-01-06 16:10 ../ drwxrwxr-x  3 martin martin  \n> 4096 2012 -01-06 16:11 bin/ drwxrwxr-x  6 martin martin  4096 \n> 2012-01-09 15:54 buildtools/\n> -rw-rw-r--  1 martin martin 14653 2012-01-09 17:48 .classpath \n> drwxrwxr-x  8 martin martin  4096 2012-01-18 12:24 core/ drwxrwxr-x  5 \n> martin martin  4096 2012-01-09 15:54 distribution/\n> -rw-rw-r--  1 martin martin  2320 2012-01-06 16:11 doap_Mahout.rdf \n> drwxrwxr-x  7 martin martin  4096 2012-01-09 15:54 examples/\n> -rw-rw-r--  1 martin martin   345 2012-01-06 16:11 .gitignore\n> drwxrwxr-x  8 martin martin  4096 2012-01-09 15:54 integration/\n> -rw-rw-r--  1 martin martin 39588 2012-01-06 16:10 LICENSE.txt \n> drwxrwxr-x  7 martin martin  4096 2012-01-09 16:01 math/ drwxrwxr-x  5 \n> martin martin  4096 2012-01-10 11:22 mw-play/\n> -rw-rw-r--  1 martin martin  1888 2012-01-06 16:10 NOTICE.txt\n> -rw-rw-r--  1 martin martin 32289 2012-01-06 16:11 pom.xml\n> -rw-rw-r--  1 martin martin   535 2012-01-09 16:41 .project\n> -rw-rw-r--  1 martin martin  1655 2012-01-09 16:00 .project~\n> -rw-rw-r--  1 martin martin  1200 2012-01-06 16:11 README.txt \n> drwxrwxr-x  2 martin martin  4096 2012-01-09 15:58 .settings/ \n> drwxrwxr-x  5 martin martin  4096 2012-01-06 16:10 src/ drwxrwxr-x  6 \n> martin martin  4096 2012-01-06 16:10 .svn/ drwxrwxr-x  3 martin martin  \n> 4096 2012-01-06 16:10 target/\n>\n>\n>\n> Only other thing I can think of is I have hadoop version \n> hadoop-0.20.205.0. (I say that because the error I see is when running \n> hadoop after all…\n>\n> hadoop jar target/mahout-core-0.6-SNAPSHOT.jar\n> org.apache.mahout.cf.taste.hadoop.item.RecommenderJob\n> -Dmapred.input.dir=input/input.txt\n> -Dmapred.output.dir=output --usersFile input/users.txt --booleanData\n>\n> gives\n>\n> Exception in thread "main" java.lang.NoClassDefFoundError:\n> org/apache/commons/cli2/Option\n>       at java.lang.Class.forName0(Native Method)\n>       at java.lang.Class.forName(Class.java:264)\n>       at org.apache.hadoop.util.RunJar.main(RunJar.java:149)\n> Caused by: java.lang.ClassNotFoundException: org.apache.commons.cli2.Option\n>       at java.net.URLClassLoader$1.run(URLClassLoader.java:217)\n>       at java.security.AccessController.doPrivileged(Native Method)\n>       at java.net.URLClassLoader.findClass(URLClassLoader.java:205)\n>       at java.lang.ClassLoader.loadClass(ClassLoader.java:321)\n>       at java.lang.ClassLoader.loadClass(ClassLoader.java:266)\n>       ... 3 more\n> )\n>\n>\n> Anything in my above setup look incorrect?\n>\n> Thanks,\n>\n> Martin Wignall\n> Application Architect\n> Trader Media Digital\n> Redwood House\n> Woodlands Business Park\n> Newton-le-Willows\n> WA12 0HE\n> 01925 296 245\n> 07525 405 281\n> martin.wignall@autotrader.co.uk\n>\n>\n> -----Original Message-----\n> From: Sean Owen [mailto:srowen@gmail.com]\n> Sent: 18 January 2012 13:04\n> To: user@mahout.apache.org\n> Subject: Re: error running a RecommenderJob\n>\n> The exact same comment generates the job file for me... I see a lot \n> more output though, as it does the work of packaging the job file.\n>\n> I wonder if it's a Maven version thing (I use 3.0.3 on OS X 10.7) or, \n> do you have all the other files in the project, above core/? I'd \n> expect it to just outright fail if either of those were at issue though.\n>\n> Run package from the project root? wild guess. That shouldn't be needed.\n>\n> On Wed, Jan 18, 2012 at 12:51 PM, Martin Wignall < \n> Martin.Wignall@autotrader.co.uk<mailto:Martin.Wignall@autotrader.co.uk\n> >>\n> wrote:\n>\n> > Thanks Sean. Any idea why:\n> >\n> > mvn -DskipTests clean package\n> >\n> > (or\n> > mvn -DskipTests package\n> > )\n> >\n> > might not have created the .job file? Is there something else I need \n> > to\n> do?\n> >\n> >\n>\n>\n> ________________________________________\n>\n>\n> This e-mail is sent on behalf of Trader Media Group Limited, \n> Registered\n> Office: Auto Trader House, Cutbush Park Industrial Estate, Danehill, \n> Lower Earley, Reading, Berkshire, RG6 4UT(Registered in England No. 4768833).\n> This email and any files transmitted with it are confidential and may \n> be legally privile ged, and intended solely for the use of the \n> individual or entity to whom they are addressed. If you have received \n> this email in error please notify the sender. This email message has \n> been swept for the presence of computer viruses.\n>\n>\n\n________________________________________\n\n\nThis e-mail is sent on behalf of Trader Media Group Limited, Registered Office: Auto Trader House, Cutbush Park Industrial Estate, Danehill, Lower Earley, Reading, Berkshire, RG6 4UT(Registered in England No. 4768833). This email and any files transmitted with it are confidential and may be legally privileged, and intended solely for the use of the individual or entity to whom they are addressed. If you have received this email in error please notify the sender. This email message has been swept for the presence of computer viruses. \n\n \n
<33EB30D8-F996-4E89-B4AE-DA5E4637336F@gmail.com>	Steven Bourke 	sbourke@gmail.com	Re: Evaluating a recomender performance	2012-01-18T14:55:12Z	<CAONrwUHp==UoceS2uCr8iUFqG-DErqXuuKfH1C0AM231evBCHA@mail.gmail.com>	Here is a link to a simple white paper\n\nhttp://www.contentwise.tv/files/Do-Metrics-Make-Recommender-Algorithms.pdf\n\nThey have an evaluation methodologies section where they explain quite simply how to measure your recommendation quality. \n\nOn 18 Jan 2012, at 14:32, Raphael Bauduin wrote:\n\n> Hi,\n> \n> I'm looking to the Recomender documentation, and it looks quite\n> straight-forward to use.\n> I'm using data from an ecommerce website and would like to build a\n> model to suggest products based on past purchases ( not on ratings ).\n> I'd like to be able to compare several options, however, I found no\n> mention of evaluating the performance of the recommender.\n> In my case, the recommendation would be a success if the test user\n> (not present in the learning data and thus used for building the\n> model) actually bought (one of ) the suggested  product(s).\n> \n> Does anyone have suggestions on performance measure for this kind of\n> Recomender? I'm interested in any and all tips!\n> \n> Thanks\n> \n> Raphaël\n\n\n \n
<7DE61C6E-1529-4F96-B4B1-94BFD4CDD1D4@gmx.de>	Manuel Blechschmidt 	Manuel.Blechschmidt@gmx.de	Re: Evaluating a recomender performance	2012-01-18T15:04:49Z	<CAONrwUHp==UoceS2uCr8iUFqG-DErqXuuKfH1C0AM231evBCHA@mail.gmail.com>	Hi Raphael,\nhere are some measurements by me with mahout for a similar case:\n\nhttp://mail-archives.apache.org/mod_mbox/mahout-user/201111.mbox/%3CDD6555A8-6FAA-4C01-AD1F-D0BEF658BAA8@gmx.de%3E\n\nI have a start up which is building that kind of technology so feel free to reach out to me.\n\n/Manuel\n\nOn 18.01.2012, at 15:32, Raphael Bauduin wrote:\n\n> Hi,\n> \n> I'm looking to the Recomender documentation, and it looks quite\n> straight-forward to use.\n> I'm using data from an ecommerce website and would like to build a\n> model to suggest products based on past purchases ( not on ratings ).\n> I'd like to be able to compare several options, however, I found no\n> mention of evaluating the performance of the recommender.\n> In my case, the recommendation would be a success if the test user\n> (not present in the learning data and thus used for building the\n> model) actually bought (one of ) the suggested  product(s).\n> \n> Does anyone have suggestions on performance measure for this kind of\n> Recomender? I'm interested in any and all tips!\n> \n> Thanks\n> \n> Raphaël\n\n-- \nManuel Blechschmidt\nDortustr. 57\n14467 Potsdam\nMobil: 0173/6322621\nTwitter: http://twitter.com/Manuel_B\n\n \n\n
<CAONrwUGb54G8WF1a-5oz-p00ew1efr6YD-0b64-OBExgnYyNiQ@mail.gmail.com>	Raphael Bauduin 	rblists@gmail.com	Re: Evaluating a recomender performance	2012-01-18T15:26:34Z	<7DE61C6E-1529-4F96-B4B1-94BFD4CDD1D4@gmx.de>	Thanks manuel and Steven for these very interesting links!\n\nI'll experiment further and look at the RecommenderEvaluator.\n\nRaph\n\nOn Wed, Jan 18, 2012 at 4:04 PM, Manuel Blechschmidt\n<Manuel.Blechschmidt@gmx.de> wrote:\n> Hi Raphael,\n> here are some measurements by me with mahout for a similar case:\n>\n> http://mail-archives.apache.org/mod_mbox/mahout-user/201111.mbox/%3CDD6555A8-6FAA-4C01-AD1F-D0BEF658BAA8@gmx.de%3E\n>\n> I have a start up which is building that kind of technology so feel free to reach out to me.\n>\n> /Manuel\n>\n> On 18.01.2012, at 15:32, Raphael Bauduin wrote:\n>\n>> Hi,\n>>\n>> I'm looking to the Recomender documentation, and it looks quite\n>> straight-forward to use.\n>> I'm using data from an ecommerce website and would like to build a\n>> model to suggest products based on past purchases ( not on ratings ).\n>> I'd like to be able to compare several options, however, I found no\n>> mention of evaluating the performance of the recommender.\n>> In my case, the recommendation would be a success if the test user\n>> (not present in the learning data and thus used for building the\n>> model) actually bought (one of ) the suggested  product(s).\n>>\n>> Does anyone have suggestions on performance measure for this kind of\n>> Recomender? I'm interested in any and all tips!\n>>\n>> Thanks\n>>\n>> Raphaël\n>\n> --\n> Manuel Blechschmidt\n> Dortustr. 57\n> 14467 Potsdam\n> Mobil: 0173/6322621\n> Twitter: http://twitter.com/Manuel_B\n>\n\n\n\n-- \nWeb database: http://www.myowndb.com\nFree Software Developers Meeting: http://www.fosdem.org\n\n \n
<E3B4FFD6E695EE408BB1945B129CAF58A8ADCC1BF8@TMG-E2K7CCR2.group.tradermedia.co.uk>	Martin Wignall 	Martin.Wignall@autotrader.co.uk	RE: error running a RecommenderJob	2012-01-18T16:38:44Z	<CAEccTyyW7mkuPtJbU=udZ2dHZ89RywYHp+Rb9Gt7FX5gLk8vMA@mail.gmail.com>	Hi Sean,\nI've downloaded Mahout from scratch and built again. This time all is fine.\n\nOriginally I had changed a pom.xml file in order to get Mahout to build, but for some reason this was not necessary on my second attempt. So all fine, thanks very much for your help.\n\nCheers!\n\nMartin \n\n\n-----Original Message-----\nFrom: Sean Owen [mailto:srowen@gmail.com] \nSent: 18 January 2012 14:15\nTo: user@mahout.apache.org\nSubject: Re: error running a RecommenderJob\n\nThe Hadoop version isn't the issue as it's a ClassNotFoundException for a Mahout dependency for sure.\n\nHmm I do think maybe the pom change is relevant. The original error concerns some plugin, org.eclipse.m2e, but that is not present in the project poms. are you sure you have the latest files from SVN, no possiblity of other files, no other modifications? This plugin does indeed not exist, but I don't see any use of it when I build. That seems to be the source of the issue.\n\nOn Wed, Jan 18, 2012 at 1:55 PM, Martin Wignall < Martin.Wignall@autotrader.co.uk> wrote:\n\n> Thanks for your response Sean. Only other thing I can think of is I \n> had to amend my pom.xml - I removed\n>\n> <plugin>\n>  <groupId>org.eclipse.m2e</groupId>\n>  <artifactId>lifecycle-mapping</artifactId>\n>  <version>1.0.0</version>\n>  <configuration>\n>  <lifecycleMappingMetadata>\n>  <pluginExecutions>\n>  <pluginExecution>\n>  <pluginExecutionFilter>\n>  <groupId>org.apache.maven.plugins</groupId>\n>  <artifactId>maven-dependency-plugin</artifactId>\n>  <versionRange>[2.0,)</versionRange>\n>  <goals>\n>  <goal>copy-dependencies</goal>\n>  </goals>\n>  </pluginExecutionFilter>\n>  <action>\n>  <ignore />\n>  </action>\n>  </pluginExecution>\n>  </pluginExecutions>\n>  </lifecycleMappingMetadata>\n>  </configuration>\n>  </plugin>\n>\n> from <plugins>, because\n>\n> mvn clean package\n>\n> did not work from the command line. I got the following error:\n>\n> mvn clean package\n> [INFO] Scanning for projects...\n> [INFO]\n> ----------------------------------------------------------------------\n> --\n> [INFO] Building Mahout Core\n> [INFO]    task-segment: [clean, package]\n> [INFO]\n> ----------------------------------------------------------------------\n> --\n> Downloading:\n> http://repo1.maven.org/maven2/org/eclipse/m2e/lifecycle-mapping/1.0.0/\n> lifecycle-mapping-1.0.0.pom\n> [INFO] Unable to find resource\n> 'org.eclipse.m2e:lifecycle-mapping:pom:1.0.0' in repository central (\n> http://repo1.maven.org/maven2)\n> Downloading:\n> http://repo1.maven.org/maven2/org/eclipse/m2e/lifecycle-mapping/1.0.0/\n> lifecycle-mapping-1.0.0.pom\n> [INFO] Unable to find resource\n> 'org.eclipse.m2e:lifecycle-mapping:pom:1.0.0' in repository central (\n> http://repo1.maven.org/maven2)\n> [INFO]\n> ----------------------------------------------------------------------\n> --\n> [ERROR] BUILD ERROR\n> [INFO]\n> ----------------------------------------------------------------------\n> -- [INFO] Error building POM (may not be this project's POM).\n>\n>\n> Project ID: org.eclipse.m2e:lifecycle-mapping\n>\n> Reason: POM 'org.eclipse.m2e:lifecycle-mapping' not found in repository:\n> Unable to download the artifact from any repository\n>\n>  org.eclipse.m2e:lifecycle-mapping:pom:1.0.0\n>\n> from the specified remote repositories:\n>  apache.snapshots (http://repository.apache.org/snapshots),\n>  central (http://repo1.maven.org/maven2)\n>\n>  for project org.eclipse.m2e:lifecycle-mapping\n>\n>\n> The above change to my pom.xml fixed the problem and allowed me to \n> package. I don’t imagine that this is related. Do you? (Just trying to \n> rule it out)\n>\n>\n>\n> In answer to your questions, I’m on maven version 2.2.1, and Ubuntu \n> 11.10\n>\n> mvn --version\n> Apache Maven 2.2.1 (rdebian-6)\n> Java version: 1.6.0_23\n> Java home: /usr/lib/jvm/java-6-openjdk/jre Default locale: en_GB, \n> platform encoding: UTF-8 OS name: "linux" version: \n> "3.0.0-14-generic-pae" arch: "i386" Family:\n> "unix"\n>\n>\n> And in terms of my mahout structure, I just downloaded trunk:\n>\n> martin@ubuntu:~/mahout/trunk $ ll\n> total 168\n> drwxrwxr-x 14 martin martin  4096 2012-01-09 16:53 ./ drwxrwxr-x  3 \n> martin martin  4096 2012-01-06 16:10 ../ drwxrwxr-x  3 martin martin  \n> 4096 2012-01-06 16:11 bin/ drwxrwxr-x  6 martin martin  4096 \n> 2012-01-09 15:54 buildtools/\n> -rw-rw-r--  1 martin martin 14653 2012-01-09 17:48 .classpath \n> drwxrwxr-x  8 martin martin  4096 2012-01-18 12:24 core/ drwxrwxr-x  5 \n> martin martin  4096 2012-01-09 15:54 distribution/\n> -rw-rw-r--  1 martin martin  2320 2012-01-06 16:11 doap_Mahout.rdf \n> drwxrwxr-x  7 martin martin  4096 2012-01-09 15:54 examples/\n> -rw-rw-r--  1 martin martin   345 2012-01-06 16:11 .gitignore\n> drwxrwxr-x  8 martin martin  4096 2012-01-09 15:54 integration/\n> -rw-rw-r--  1 martin martin 39588 2012-01-06 16:10 LICENSE.txt \n> drwxrwxr-x  7 martin martin  4096 2012-01-09 16:01 math/ drwxrwxr-x  5 \n> martin martin  4096 2012-01-10 11:22 mw-play/\n> -rw-rw-r--  1 martin martin  1888 2012-01-06 16:10 NOTICE.txt\n> -rw-rw-r--  1 martin martin 32289 2012-01-06 16:11 pom.xml\n> -rw-rw-r--  1 martin martin   535 2012-01-09 16:41 .project\n> -rw-rw-r--  1 martin martin  1655 2012-01-09 16:00 .project~\n> -rw-rw-r--  1 martin martin  1200 2012-01-06 16:11 README.txt \n> drwxrwxr-x  2 martin martin  4096 2012-01-09 15:58 .settings/ \n> drwxrwxr-x  5 martin martin  4096 2012-01-06 16:10 src/ drwxrwxr-x  6 \n> martin martin  4096 2012-01-06 16:10 .svn/ drwxrwxr-x  3 martin martin  \n> 4096 2012-01-06 16:10 target/\n>\n>\n>\n> Only other thing I can think of is I have hadoop version \n> hadoop-0.20.205.0. (I say that because the error I see is when running \n> hadoop after all…\n>\n> hadoop jar target/mahout-core-0.6-SNAPSHOT.jar\n> org.apache.mahout.cf.taste.hadoop.item.RecommenderJob\n> -Dmapred.input.dir=input/input.txt\n> -Dmapred.output.dir=output --usersFile input/users.txt --booleanData\n>\n> gives\n>\n> Exception in thread "main" java.lang.NoClassDefFoundError:\n> org/apache/commons/cli2/Option\n>       at java.lang.Class.forName0(Native Method)\n>       at java.lang.Class.forName(Class.java:264)\n>       at org.apache.hadoop.util.RunJar.main(RunJar.java:149)\n> Caused by: java.lang.ClassNotFoundException: org.apache.commons.cli2.Option\n>       at java.net.URLClassLoader$1.run(URLClassLoader.java:217)\n>       at java.security.AccessController.doPrivileged(Native Method)\n>       at java.net.URLClassLoader.findClass(URLClassLoader.java:205)\n>       at java.lang.ClassLoader.loadClass(ClassLoader.java:321)\n>       at java.lang.ClassLoader.loadClass(ClassLoader.java:266)\n>       ... 3 more\n> )\n>\n>\n> Anything in my above setup look incorrect?\n>\n> Thanks,\n>\n> Martin Wignall\n> Application Architect\n> Trader Media Digital\n> Redwood House\n> Woodlands Business Park\n> Newton-le-Willows\n> WA12 0HE\n> 01925 296 245\n> 07525 405 281\n> martin.wignall@autotrader.co.uk\n>\n>\n> -----Original Message-----\n> From: Sean Owen [mailto:srowen@gmail.com]\n> Sent: 18 January 2012 13:04\n> To: user@mahout.apache.org\n> Subject: Re: error running a RecommenderJob\n>\n> The exact same comment generates the job file for me... I see a lot \n> more output though, as it does the work of packaging the job file.\n>\n> I wonder if it's a Maven version thing (I use 3.0.3 on OS X 10.7) or, \n> do you have all the other files in the project, above core/? I'd \n> expect it to just outright fail if either of those were at issue though.\n>\n> Run package from the project root? wild guess. That shouldn't be needed.\n>\n> On Wed, Jan 18, 2012 at 12:51 PM, Martin Wignall < \n> Martin.Wignall@autotrader.co.uk<mailto:Martin.Wignall@autotrader.co.uk\n> >>\n> wrote:\n>\n> > Thanks Sean. Any idea why:\n> >\n> > mvn -DskipTests clean package\n> >\n> > (or\n> > mvn -DskipTests package\n> > )\n> >\n> > might not have created the .job file? Is there something else I need \n> > to\n> do?\n> >\n> >\n>\n>\n> ________________________________________\n>\n>\n> This e-mail is sent on behalf of Trader Media Group Limited, \n> Registered\n> Office: Auto Trader House, Cutbush Park Industrial Estate, Danehill, \n \n> Lower Earley, Reading, Berkshire, RG6 4UT(Registered in England No. 4768833).\n> This email and any files transmitted with it are confidential and may \n> be legally privileged, and intended solely for the use of the \n> individual or entity to whom they are addressed. If you have received \n> this email in error please notify the sender. This email message has \n> been swept for the presence of computer viruses.\n>\n>\n\n________________________________________\n\n\nThis e-mail is sent on behalf of Trader Media Group Limited, Registered Office: Auto Trader House, Cutbush Park Industrial Estate, Danehill, Lower Earley, Reading, Berkshire, RG6 4UT(Registered in England No. 4768833). This email and any files transmitted with it are confidential and may be legally privileged, and intended solely for the use of the individual or entity to whom they are addressed. If you have received this email in error please notify the sender. This email message has been swept for the presence of computer viruses. \n\n \n
<E3B4FFD6E695EE408BB1945B129CAF58A8ADCC1C40@TMG-E2K7CCR2.group.tradermedia.co.uk>	Martin Wignall 	Martin.Wignall@autotrader.co.uk	RE: error running a RecommenderJob	2012-01-18T17:07:47Z		Sean,\nWhen I execute my recommenderJob, with\n\nhadoop jar target/mahout-core-0.6-SNAPSHOT-job.jar org.apache.mahout.cf.taste.hadoop.item.RecommenderJob -Dmapred.input.dir=input/input.txt -Dmapred.output.dir=output --usersFile input/users.txt --booleanData --similarityClassname SIMILARITY_COOCCURRENCE\n\n…then I see that 7 jobs are successful, but 3 jobs fail (http://localhost:50030/jobtracker.jsp) with the following output:\n\nJOB NAME: PreparePreferenceMatrixJob-ItemIDIndexMapper-Reducer\nDIAGNOSTIC INFO:  # of failed Map Tasks exceeded allowed limit. FailedCount: 1. LastFailedTask: task_201201171740_0001_m_000000\n\nJOB NAME: PreparePreferenceMatrixJob-ToItemPrefsMapper-Reducer\nDIAGNOSTIC INFO: # of failed Map Tasks exceeded allowed limit. FailedCount: 1. LastFailedTask: task_201201171740_0002_m_000000\n\nJOB NAME: RowSimilarityJob-CooccurrencesMapper-Reducer\nDIAGNOSTIC INFO:  # of failed Reduce Tasks exceeded allowed limit. FailedCount: 1. LastFailedTask: task_201201171740_0005_r_000000\n\n\nAny idea what I can do about this?\n\nMany thanks once again!\n\nMartin\n\n\n\n\n-----Original Message-----\nFrom: Martin Wignall\nSent: 18 January 2012 16:39\nTo: 'user@mahout.apache.org'\nSubject: RE: error running a RecommenderJob\n\nHi Sean,\nI've downloaded Mahout from scratch and built again. This time all is fine.\n\nOriginally I had changed a pom.xml file in order to get Mahout to build, but for some reason this was not necessary on my second attempt. So all fine, thanks very much for your help.\n\nCheers!\n\nMartin\n\n\n-----Original Message-----\nFrom: Sean Owen [mailto:srowen@gmail.com]\nSent: 18 January 2012 14:15\nTo: user@mahout.apache.org\nSubject: Re: error running a RecommenderJob\n\nThe Hadoop version isn't the issue as it's a ClassNotFoundException for a Mahout dependency for sure.\n\nHmm I do think maybe the pom change is relevant. The original error concerns some plugin, org.eclipse.m2e, but that is not present in the project poms. are you sure you have the latest files from SVN, no possiblity of other files, no other modifications? This plugin does indeed not exist, but I don't see any use of it when I build. That seems to be the source of the issue.\n\nOn Wed, Jan 18, 2012 at 1:55 PM, Martin Wignall < Martin.Wignall@autotrader.co.uk<mailto:Martin.Wignall@autotrader.co.uk>> wrote:\n\n> Thanks for your response Sean. Only other thing I can think of is I\n> had to amend my pom.xml - I removed\n>\n> <plugin>\n>  <groupId>org.eclipse.m2e</groupId>\n>  <artifactId>lifecycle-mapping</artifactId>\n>  <version>1.0.0</version>\n>  <configuration>\n>  <lifecycleMappingMetadata>\n>  <pluginExecutions>\n>  <pluginExecution>\n>  <pluginExecutionFilter>\n>  <groupId>org.apache.maven.plugins</groupId>\n>  <artifactId>maven-dependency-plugin</artifactId>\n>  <versionRange>[2.0,)</versionRange>\n>  <goals>\n>  <goal>copy-dependencies</goal>\n>  </goals>\n>  </pluginExecutionFilter>\n>  <action>\n>  <ignore />\n>  </action>\n>  </pluginExecution>\n>  </pluginExecutions>\n>  </lifecycleMappingMetadata>\n>  </configuration>\n>  </plugin>\n>\n> from <plugins>, because\n>\n> mvn clean package\n>\n> did not work from the command line. I got the following error:\n>\n> mvn clean package\n> [INFO] Scanning for projects...\n> [INFO]\n> ----------------------------------------------------------------------\n> --\n> [INFO] Building Mahout Core\n> [INFO]    task-segment: [clean, package]\n> [INFO]\n> ----------------------------------------------------------------------\n> --\n> Downloading:\n> http://repo1.maven.org/maven2/org/eclipse/m2e/lifecycle-mapping/1.0.0/\n> lifecycle-mapping-1.0.0.pom\n> [INFO] Unable to find resource\n> 'org.eclipse.m2e:lifecycle-mapping:pom:1.0.0' in repository central (\n> http://repo1.maven.org/maven2)\n> Downloading:\n> http://repo1.maven.org/maven2/org/eclipse/m2e/lifecycle-mapping/1.0.0/\n> lifecycle-mapping-1.0.0.pom\n> [INFO] Unable to find resource\n> 'org.eclipse.m2e:lifecycle-mapping:pom:1.0.0' in repository central (\n> http://repo1.maven.org/maven2)\n> [INFO]\n> -------------------------------- --------------------------------------\n> --\n> [ERROR] BUILD ERROR\n> [INFO]\n> ----------------------------------------------------------------------\n> -- [INFO] Error building POM (may not be this project's POM).\n>\n>\n> Project ID: org.eclipse.m2e:lifecycle-mapping\n>\n> Reason: POM 'org.eclipse.m2e:lifecycle-mapping' not found in repository:\n> Unable to download the artifact from any repository\n>\n>  org.eclipse.m2e:lifecycle-mapping:pom:1.0.0\n>\n> from the specified remote repositories:\n>  apache.snapshots (http://repository.apache.org/snapshots),\n>  central (http://repo1.maven.org/maven2)\n>\n>  for project org.eclipse.m2e:lifecycle-mapping\n>\n>\n> The above change to my pom.xml fixed the problem and allowed me to\n> package. I don’t imagine that this is related. Do you? (Just trying to\n> rule it out)\n>\n>\n>\n> In answer to your questions, I’m on maven version 2.2.1, and Ubuntu\n> 11.10\n>\n> mvn --version\n> Apache Maven 2.2.1 (rdebian-6)\n> Java version: 1.6.0_23\n> Java home: /usr/lib/jvm/java-6-openjdk/jre Default locale: en_GB,\n> platform encoding: UTF-8 OS name: "linux" version:\n> "3.0.0-14-generic-pae" arch: "i386" Family:\n> "unix"\n>\n>\n> And in terms of my mahout structure, I just downloaded trunk:\n>\n> martin@ubuntu:~/mahout/trunk$ ll\n> total 168\n> drwxrwxr-x 14 martin martin  4096 2012-01-09 16:53 ./ drwxrwxr-x  3\n> martin martin  4096 2012-01-06 16:10 ../ drwxrwxr-x  3 martin martin\n> 4096 2012-01-06 16:11 bin/ drwxrwxr-x  6 martin martin  4096\n> 2012-01-09 15:54 buildtools/\n> -rw-rw-r--  1 martin martin 14653 2012-01-09 17:48 .classpath\n> drwxrwxr-x  8 martin martin  4096 2012-01-18 12:24 core/ drwxrwxr-x  5\n> martin martin  4096 2012-01-09 15:54 distribution/\n> -rw-rw-r--  1 martin martin  2320 2012-01-06 16:11 doap_Mahout.rdf\n> drwxrwxr-x  7 martin martin  4096 2012-01-09 15:54 examples/\n> -rw-rw-r--  1 martin martin   345 2012-01-06 16:11 .gitignore\n> drwxrwxr-x  8 martin martin  4096 2012-01-09 15:54 integration/\n> -rw-rw-r--  1 martin martin 39588 2012-01-06 16:10 LICENSE.txt\n> drwxrwxr-x  7 martin martin  4096 2012-01-09 16:01 math/ drwxrwxr-x  5\n> martin martin  4096 2012-01-10 11:22 mw-play/\n> -rw-rw-r--  1 martin martin  1888 2012-01-06 16:10 NOTICE.txt\n> -rw-rw-r--  1 martin martin 32289 2012-01-06 16:11 pom.xml\n> -rw-rw-r--  1 martin martin   535 2012-01-09 16:41 .project\n> -rw-rw-r--  1 martin martin  1655 2012-01-09 16:00 .project~\n> -rw-rw-r--  1 martin martin  1200 2012-01-06 16:11 README.txt\n> drwxrwxr-x  2 martin martin  4096 2012-01-09 15:58 .settings/\n> drwxrwxr-x  5 martin martin  4096 2012-01-06 16:10 src/ drwxrwxr-x  6\n> martin martin  4096 2012-01-06 16:10 .svn/ drwxrwxr-x  3 martin martin\n> 4096 2012-01-06 16:10 target/\n>\n>\n>\n> Only other thing I can think of is I have hadoop version\n> hadoop-0.20.205.0. (I say that because the error I see is when running\n> hadoop after all…\n>\n> hadoop jar target/mahout-core-0.6-SNAPSHOT.jar\n> org.apache.mahout.cf.taste.hadoop.item.RecommenderJob\n> -Dmapred.input.dir=input/input.txt\n> -Dmapred.output.dir=output --usersFile input/users.txt --booleanData\n>\n> gives\n>\n> Exception in thread "main" java.lang.NoClassDefFoundError:\n> org/apache/commons/cli2/Option\n>       at java.lang.Class.forName0(Native Method)\n>       at java.lang.Class.forName(Class.java:264)\n>       at org.apache.hadoop.util.RunJar.main(RunJar.java:149)\n> Caused by: java.lang.ClassNotFoundException: org.apache.commons.cli2.Option\n>       at java.net.URLClassLoader$1.run(URLClassLoader.java:217)\n>       at java.security.AccessController.doPrivileged(Native Method)\n>       at java.net.URLClassLoader.findClass(URLClassLoader.java:205)\n>       at java.lang.ClassLoader.loadClass(ClassLoader.java:321)\n>       at java.lang.ClassLoader.loadClass(ClassLoader.java:266)\n>       ... 3 more\n> )\n>\n>\n> Anything in my above setup look incorrect?\n>\n> Thanks,\n>\n> Martin Wignall\n> Application Architect\n> Trader Media Digital\n> Redwood House\n> Woodlands Business Park\n> Newton-le-Willows\n> WA12 0HE\n> 01925 296 245\n> 075 25 405 281\n> martin.wignall@autotrader.co.uk<mailto:martin.wignall@autotrader.co.uk>\n>\n>\n> -----Original Message-----\n> From: Sean Owen [mailto:srowen@gmail.com]\n> Sent: 18 January 2012 13:04\n> To: user@mahout.apache.org\n> Subject: Re: error running a RecommenderJob\n>\n> The exact same comment generates the job file for me... I see a lot\n> more output though, as it does the work of packaging the job file.\n>\n> I wonder if it's a Maven version thing (I use 3.0.3 on OS X 10.7) or,\n> do you have all the other files in the project, above core/? I'd\n> expect it to just outright fail if either of those were at issue though.\n>\n> Run package from the project root? wild guess. That shouldn't be needed.\n>\n> On Wed, Jan 18, 2012 at 12:51 PM, Martin Wignall <\n> Martin.Wignall@autotrader.co.uk<mailto:Martin.Wignall@autotrader.co.uk<mailto:Martin.Wignall@autotrader.co.uk<mailto:Martin.Wignall@autotrader.co.uk>\n> >>\n> wrote:\n>\n> > Thanks Sean. Any idea why:\n> >\n> > mvn -DskipTests clean package\n> >\n> > (or\n> > mvn -DskipTests package\n> > )\n> >\n> > might not have created the .job file? Is there something else I need\n> > to\n> do?\n> >\n> >\n>\n>\n> ________________________________________\n>\n>\n> This e-mail is sent on behalf of Trader Media Group Limited,\n> Registered\n> Office: Auto Trader House, Cutbush Park Industrial Estate, Danehill,\n> Lower Earley, Reading, Berkshire, RG6 4UT(Registered in England No. 4768833).\n> This email and any files transmitted with it are confidential and may\n> be legally privileged, and intended solely for the use of the\n> individual or entity to whom they are addressed. If you have received\n> this email in error please notify the sender. This email message has\n> been swept for the presence of computer viruses.\n>\n>\n\n\n________________________________________\n\n\nThis e-mail is sent on behalf of Trader Media Group Limited, Registered Office: Auto Trader House, Cutbush Park Industrial Estate, Danehill, Lower Earley, Reading, Berkshire, RG6 4UT(Registered in England No. 4768833). This email and any files transmitted with it are confidential and may be legally privileged, and intended solely for the use of the individual or entity to whom they are addressed. If you have received this email in error please notify the sender. This email message has been swept for the presence of computer viruses. \n\n \n\n
<CAEccTyzg3xvroDBZhXTKkBOBpg+J4vv5tu-+AeBBR3FbnhZA_w@mail.gmail.com>	Sean Owen 	srowen@gmail.com	Re: error running a RecommenderJob	2012-01-18T17:09:55Z	<E3B4FFD6E695EE408BB1945B129CAF58A8ADCC1C40@TMG-E2K7CCR2.group.tradermedia.co.uk>	Something is definitely failing and you will find detail in the actual\ntask's log. A mapper failed here early on. The command line here isn't\nshowing you anything.\n\nIf I had to guess -- your input is not in the right format.\n\nOn Wed, Jan 18, 2012 at 5:07 PM, Martin Wignall\n<Martin.Wignall@autotrader.co.uk> wrote:\n> Sean,\n> When I execute my recommenderJob, with\n>\n> hadoop jar target/mahout-core-0.6-SNAPSHOT-job.jar org.apache.mahout.cf.taste.hadoop.item.RecommenderJob -Dmapred.input.dir=input/input.txt -Dmapred.output.dir=output --usersFile input/users.txt --booleanData --similarityClassname SIMILARITY_COOCCURRENCE\n>\n> …then I see that 7 jobs are successful, but 3 jobs fail (http://localhost:50030/jobtracker.jsp) with the following output:\n>\n> JOB NAME: PreparePreferenceMatrixJob-ItemIDIndexMapper-Reducer\n> DIAGNOSTIC INFO:  # of failed Map Tasks exceeded allowed limit. FailedCount: 1. LastFailedTask: task_201201171740_0001_m_000000\n>\n> JOB NAME: PreparePreferenceMatrixJob-ToItemPrefsMapper-Reducer\n> DIAGNOSTIC INFO: # of failed Map Tasks exceeded allowed limit. FailedCount: 1. LastFailedTask: task_201201171740_0002_m_000000\n>\n> JOB NAME: RowSimilarityJob-CooccurrencesMapper-Reducer\n> DIAGNOSTIC INFO:  # of failed Reduce Tasks exceeded allowed limit. FailedCount: 1. LastFailedTask: task_201201171740_0005_r_000000\n>\n>\n> Any idea what I can do about this?\n>\n> Many thanks once again!\n>\n> Martin\n>\n>\n>\n>\n> -----Original Message-----\n> From: Martin Wignall\n> Sent: 18 January 2012 16:39\n> To: 'user@mahout.apache.org'\n> Subject: RE: error running a RecommenderJob\n>\n> Hi Sean,\n> I've downloaded Mahout from scratch and built again. This time all is fine.\n>\n> Originally I had changed a pom.xml file in order to get Mahout to build, but for some reason this was not necessary on my second attempt. So all fine, thanks very much for your help.\n>\n> Cheers!\n>\n> Martin\n>\n>\n> -----Original Message-----\n> From: Sean Owen [mailto:srowen@gmail.com]\n> Sent: 18 January 2012 14:15\n> To: user@mahout.apache.org\n> Subject: Re: error running a RecommenderJob\n>\n> The Hadoop version isn't the issue as it's a ClassNotFoundException for a Mahout dependency for sure.\n>\n> Hmm I do think maybe the pom change is relevant. The original error concerns some plugin, org.eclipse.m2e, but that is not present in the project poms. are you sure you have the latest files from SVN, no possiblity of other files, no other modifications? This plugin does indeed not exist, but I don't see any use of it when I build. That seems to be the source of the issue.\n>\n> On Wed, Jan 18, 2012 at 1:55 PM, Martin Wignall < Martin.Wignall@autotrader.co.uk<mailto:Martin.Wignall@autotrader.co.uk>> wrote:\n>\n>> Thanks for your response Sean. Only other thing I can think of is I\n>> had to amend my pom.xml - I removed\n>>\n>> <plugin>\n>>  <groupId>org.eclipse.m2e</groupId>\n>>  <artifactId>lifecycle-mapping</artifactId>\n>>  <version>1.0.0</version>\n>>  <configuration>\n>>  <lifecycleMappingMetadata>\n>>  <pluginExecutions>\n>>  <pluginExecution>\n>>  <pluginExecutionFilter>\n>>  <groupId>org.apache.maven.plugins</groupId>\n>>  <artifactId>maven-dependency-plugin</artifactId>\n>>  <versionRange>[2.0,)</versionRange>\n>>  <goals>\n>>  <goal>copy-dependencies</goal>\n>>  </goals>\n>>  </pluginExecutionFilter>\n>>  <action>\n>>  <ignore />\n>>  </action>\n>>  </pluginExecution>\n>>  </pluginExecutions>\n>>  </lifecycleMappingMetadata>\n>>  </configuration>\n>>  </plugin>\n>>\n>> from <plugins>, because\n>>\n>> mvn clean package\n>>\n>> did not work from the command line. I got the following error:\n>>\n>> mvn clean package\n>> [INFO] Scanning for projects...\n>> [INFO]\n>> ----------------------------------------------------------------------\n>> --\n>> [INFO] Building Mahout Core\n>> [INFO]    task-segment: [clean, package]\n>> [INFO]\n>> ----------------------------------------------------------------------\n>> --\n>> Downloading:\n>> http://repo1.maven.org/maven2/org/eclipse/m2e/lifecycle-mapping/1.0.0/\n>> lifecycle-mapping-1.0 .0.pom\n>> [INFO] Unable to find resource\n>> 'org.eclipse.m2e:lifecycle-mapping:pom:1.0.0' in repository central (\n>> http://repo1.maven.org/maven2)\n>> Downloading:\n>> http://repo1.maven.org/maven2/org/eclipse/m2e/lifecycle-mapping/1.0.0/\n>> lifecycle-mapping-1.0.0.pom\n>> [INFO] Unable to find resource\n>> 'org.eclipse.m2e:lifecycle-mapping:pom:1.0.0' in repository central (\n>> http://repo1.maven.org/maven2)\n>> [INFO]\n>> ----------------------------------------------------------------------\n>> --\n>> [ERROR] BUILD ERROR\n>> [INFO]\n>> ----------------------------------------------------------------------\n>> -- [INFO] Error building POM (may not be this project's POM).\n>>\n>>\n>> Project ID: org.eclipse.m2e:lifecycle-mapping\n>>\n>> Reason: POM 'org.eclipse.m2e:lifecycle-mapping' not found in repository:\n>> Unable to download the artifact from any repository\n>>\n>>  org.eclipse.m2e:lifecycle-mapping:pom:1.0.0\n>>\n>> from the specified remote repositories:\n>>  apache.snapshots (http://repository.apache.org/snapshots),\n>>  central (http://repo1.maven.org/maven2)\n>>\n>>  for project org.eclipse.m2e:lifecycle-mapping\n>>\n>>\n>> The above change to my pom.xml fixed the problem and allowed me to\n>> package. I don’t imagine that this is related. Do you? (Just trying to\n>> rule it out)\n>>\n>>\n>>\n>> In answer to your questions, I’m on maven version 2.2.1, and Ubuntu\n>> 11.10\n>>\n>> mvn --version\n>> Apache Maven 2.2.1 (rdebian-6)\n>> Java version: 1.6.0_23\n>> Java home: /usr/lib/jvm/java-6-openjdk/jre Default locale: en_GB,\n>> platform encoding: UTF-8 OS name: "linux" version:\n>> "3.0.0-14-generic-pae" arch: "i386" Family:\n>> "unix"\n>>\n>>\n>> And in terms of my mahout structure, I just downloaded trunk:\n>>\n>> martin@ubuntu:~/mahout/trunk$ ll\n>> total 168\n>> drwxrwxr-x 14 martin martin  4096 2012-01-09 16:53 ./ drwxrwxr-x  3\n>> martin martin  4096 2012-01-06 16:10 ../ drwxrwxr-x  3 martin martin\n>> 4096 2012-01-06 16:11 bin/ drwxrwxr-x  6 martin martin  4096\n>> 2012-01-09 15:54 buildtools/\n>> -rw-rw-r--  1 martin martin 14653 2012-01-09 17:48 .classpath\n>> drwxrwxr-x  8 martin martin  4096 2012-01-18 12:24 core/ drwxrwxr-x  5\n>> martin martin  4096 2012-01-09 15:54 distribution/\n>> -rw-rw-r--  1 martin martin  2320 2012-01-06 16:11 doap_Mahout.rdf\n>> drwxrwxr-x  7 martin martin  4096 2012-01-09 15:54 examples/\n>> -rw-rw-r--  1 martin martin   345 2012-01-06 16:11 .gitignore\n>> drwxrwxr-x  8 martin martin  4096 2012-01-09 15:54 integration/\n>> -rw-rw-r--  1 martin martin 39588 2012-01-06 16:10 LICENSE.txt\n>> drwxrwxr-x  7 martin martin  4096 2012-01-09 16:01 math/ drwxrwxr-x  5\n>> martin martin  4096 2012-01-10 11:22 mw-play/\n>> -rw-rw-r--  1 martin martin  1888 2012-01-06 16:10 NOTICE.txt\n>> -rw-rw-r--  1 martin martin 32289 2012-01-06 16:11 pom.xml\n>> -rw-rw-r--  1 martin martin   535 2012-01-09 16:41 .project\n>> -rw-rw-r--  1 martin martin  1655 2012-01-09 16:00 .project~\n>> -rw-rw-r--  1 martin martin  1200 2012-01-06 16:11 README.txt\n>> drwxrwxr-x  2 martin martin  4096 2012-01-09 15:58 .settings/\n>> drwxrwxr-x  5 martin martin  4096 2012-01-06 16:10 src/ drwxrwxr-x  6\n>> martin martin  4096 2012-01-06 16:10 .svn/ drwxrwxr-x  3 martin martin\n>> 4096 2012-01-06 16:10 target/\n>>\n>>\n>>\n>> Only other thing I can think of is I have hadoop version\n>> hadoop-0.20.205.0. (I say that because the error I see is when running\n>> hadoop after all…\n>>\n>> hadoop jar target/mahout-core-0.6-SNAPSHOT.jar\n>> org.apache.mahout.cf.taste.hadoop.item.RecommenderJob\n>> -Dmapred.input.dir=input/input.txt\n>> -Dmapred.output.dir=output --usersFile input/users.txt --booleanData\n>>\n>> gives\n>>\n>> Exception in thread "main" java.lang.NoClassDefFoundError:\n>> org/apache/commons/cli2/Option\n>>       at java.lang.Class.forName0(Native Method)\n>>       at java.lang.Class.forName(Class.java:264)\n>>       at org.apache.hadoop.util.RunJar.main(RunJar.java:149)\n>> Caused by: java.lang.ClassNotFoundExce ption: org.apache.commons.cli2.Option\n>>       at java.net.URLClassLoader$1.run(URLClassLoader.java:217)\n>>       at java.security.AccessController.doPrivileged(Native Method)\n>>       at java.net.URLClassLoader.findClass(URLClassLoader.java:205)\n>>       at java.lang.ClassLoader.loadClass(ClassLoader.java:321)\n>>       at java.lang.ClassLoader.loadClass(ClassLoader.java:266)\n>>       ... 3 more\n>> )\n>>\n>>\n>> Anything in my above setup look incorrect?\n>>\n>> Thanks,\n>>\n>> Martin Wignall\n>> Application Architect\n>> Trader Media Digital\n>> Redwood House\n>> Woodlands Business Park\n>> Newton-le-Willows\n>> WA12 0HE\n>> 01925 296 245\n>> 07525 405 281\n>> martin.wignall@autotrader.co.uk<mailto:martin.wignall@autotrader.co.uk>\n>>\n>>\n>> -----Original Message-----\n>> From: Sean Owen [mailto:srowen@gmail.com]\n>> Sent: 18 January 2012 13:04\n>> To: user@mahout.apache.org\n>> Subject: Re: error running a RecommenderJob\n>>\n>> The exact same comment generates the job file for me... I see a lot\n>> more output though, as it does the work of packaging the job file.\n>>\n>> I wonder if it's a Maven version thing (I use 3.0.3 on OS X 10.7) or,\n>> do you have all the other files in the project, above core/? I'd\n>> expect it to just outright fail if either of those were at issue though.\n>>\n>> Run package from the project root? wild guess. That shouldn't be needed.\n>>\n>> On Wed, Jan 18, 2012 at 12:51 PM, Martin Wignall <\n>> Martin.Wignall@autotrader.co.uk<mailto:Martin.Wignall@autotrader.co.uk<mailto:Martin.Wignall@autotrader.co.uk<mailto:Martin.Wignall@autotrader.co.uk>\n>> >>\n>> wrote:\n>>\n>> > Thanks Sean. Any idea why:\n>> >\n>> > mvn -DskipTests clean package\n>> >\n>> > (or\n>> > mvn -DskipTests package\n>> > )\n>> >\n>> > might not have created the .job file? Is there something else I need\n>> > to\n>> do?\n>> >\n>> >\n>>\n>>\n>> ________________________________________\n>>\n>>\n>> This e-mail is sent on behalf of Trader Media Group Limited,\n>> Registered\n>> Office: Auto Trader House, Cutbush Park Industrial Estate, Danehill,\n>> Lower Earley, Reading, Berkshire, RG6 4UT(Registered in England No. 4768833).\n>> This email and any files transmitted with it are confidential and may\n>> be legally privileged, and intended solely for the use of the\n>> individual or entity to whom they are addressed. If you have received\n>> this email in error please notify the sender. This email message has\n>> been swept for the presence of computer viruses.\n>>\n>>\n>\n>\n> ________________________________________\n>\n>\n> This e-mail is sent on behalf of Trader Media Group Limited, Registered Office: Auto Trader House, Cutbush Park Industrial Estate, Danehill, Lower Earley, Reading, Berkshire, RG6 4UT(Registered in England No. 4768833). This email and any files transmitted with it are confidential and may be legally privileged, and intended solely for the use of the individual or entity to whom they are addressed. If you have received this email in error please notify the sender. This email message has been swept for the presence of computer viruses.\n>\n\n \n
<CAKF+NEaKxZUQ7oYsxwPJo_-Mp4-H213O_+ZRqPdmCutG4MxQsg@mail.gmail.com>	Daniel Korzekwa 	daniel.korzekwa@gmail.com	Bayes classification - strange results	2012-01-18T19:22:06Z		Hello,\n\nI'm training bayes classifier against this data (6 records):\n\ntarget, words\nT A A A\nT A A A\nT A A A\nT A A B\nT A A B\nF A A B\n\nwith a command:\n ./mahout trainclassifier -i /mnt/hgfs/C/daniel/my_fav_data/test -o model\n-type bayes -ng 1 -source hdfs\n\nthen I test this classifier against the same data with:\n./mahout testclassifier -d /mnt/hgfs/C/daniel/my_fav_data/test -m model\n-type bayes -ng 1 -source hdfs -method sequential -v\n\n and I'm getting classification I cannot understand. All records are\nclassified as F, why is that?, shouldn't they be all classified as T?\n12/01/18 11:07:55 INFO bayes.TestClassifier: Line Number: 0 Line(30): T A A\nA Expected Label: T Classified Label: F Correct: false\n12/01/18 11:07:55 INFO bayes.TestClassifier: Line Number: 1 Line(30): T A A\nA Expected Label: T Classified Label: F Correct: false\n12/01/18 11:07:55 INFO bayes.TestClassifier: Line Number: 2 Line(30): T A A\nA Expected Label: T Classified Label: F Correct: false\n12/01/18 11:07:55 INFO bayes.TestClassifier: Line Number: 3 Line(30): T A A\nB Expected Label: T Classified Label: F Correct: false\n12/01/18 11:07:55 INFO bayes.TestClassifier: Line Number: 4 Line(30): T A A\nB Expected Label: T Classified Label: F Correct: false\n12/01/18 11:07:55 INFO bayes.TestClassifier: Line Number: 5 Line(30): F A A\nB Expected Label: F Classified Label: F Correct: true\n\nMy reasoning (no smoothing applied):\nPrior:\nP(T) = 5/6\nP(F) = 1/6\n\nP(A/T) = 13/15\nP(A/F) = 2/3\n\nP(B/T) = 2/15\nP(B/F) = 1/3\n\nThen I calculate posterior probability, e.g. P(T|A,A,B) = 0.7717 - record\nclassified as T.\n\nWhat is the reasoning behind classifying all records above as F?\n\nAny help much appreciated.\n\nPS. I was using mahout trunk from 16.01.2012.\n\nRegards.\nDaniel\n\n-- \nDaniel Korzekwa\nSoftware Engineer\npriv: http://danmachine.com\nblog: http://blog.danmachine.com\n \n\n
<CALH6cCM0RZcmNUC2bzcJv4u2_kzzrJNepe3L+NUP-v=82BmiJA@mail.gmail.com>	John Conwell 	john@iamjohn.me	term vectors not created in SparseVectorsFromSequenceFiles using tf weighting and maxDFSigma filtering	2012-01-18T21:16:48Z		I got latest from Trunk and built it, and when\nrunning SparseVectorsFromSequenceFiles I noticed what I think is a bug.\n The SparseVectorsFromSequenceFiles throws an exception when you want term\nfrequency vectors output, with the maxDFSigma filtering option.\n\nBasically the if / else if section shown below, will skip\ncalling DictionaryVectorizer.createTermFrequencyVectors when have that\ncombination.  The condition will create vectors when you want tf vectors\nwithout maxDFSigma filtering, or tfidf vectors with maxDFSigma filtering,\nbut if you want tf vectors with maxDFSigma filtering, it totally skips over\nthe call to createTermFrequencyVectors, and later on throws an exception\nbecause the vector input path doesn't exist.\n\nIs this a known issue?  I'm assuming thats not the way its suposed to work,\ncorrect?  If so, I think some sort of validation should break the user out\nbefore they start processing anything\n\n//at line ~267 in trunk\n\nif (!processIdf && !shouldPrune) {\n\n        DictionaryVectorizer.createTermFrequencyVectors(tokenizedPath,\noutputDir, tfDirName, conf, minSupport, maxNGramSize,\n\n          minLLRValue, norm, logNormalize, reduceTasks, chunkSize,\nsequentialAccessOutput, namedVectors);\n\n} else if (processIdf) {\n\n        DictionaryVectorizer.createTermFrequencyVectors(tokenizedPath,\noutputDir, tfDirName, conf, minSupport, maxNGramSize,\n\n          minLLRValue, -1.0f, false, reduceTasks, chunkSize,\nsequentialAccessOutput, namedVectors);\n\n}\n\n-- \n\nThanks,\nJohn C\n \n\n
<CALH6cCM331vP+9Km-o__mkPZMbiQAWVkVwOR5_ng2bzK05DtCw@mail.gmail.com>	John Conwell 	turbocodr@gmail.com	term vectors not created in SparseVectorsFromSequenceFiles using tf weighting and maxDFSigma filtering	2012-01-18T21:26:48Z	<CALH6cCM0RZcmNUC2bzcJv4u2_kzzrJNepe3L+NUP-v=82BmiJA@mail.gmail.com>	I got latest from Trunk and built it, and when\nrunning SparseVectorsFromSequenceFiles I noticed what I think is a bug.\n The SparseVectorsFromSequenceFiles throws an exception when you want term\nfrequency vectors output, with the maxDFSigma filtering option.\n\nBasically the if / else if section shown below, will skip\ncalling DictionaryVectorizer.createTermFrequencyVectors when have that\ncombination.  The condition will create vectors when you want tf vectors\nwithout maxDFSigma filtering, or tfidf vectors with maxDFSigma filtering,\nbut if you want tf vectors with maxDFSigma filtering, it totally skips over\nthe call to createTermFrequencyVectors, and later on throws an exception\nbecause the vector input path doesn't exist.\n\nIs this a known issue?  I'm assuming thats not the way its suposed to work,\ncorrect?  If so, I think some sort of validation should break the user out\nbefore they start processing anything\n\n//at line ~267 in trunk\n\nif (!processIdf && !shouldPrune) {\n\n        DictionaryVectorizer.createTermFrequencyVectors(tokenizedPath,\noutputDir, tfDirName, conf, minSupport, maxNGramSize,\n\n          minLLRValue, norm, logNormalize, reduceTasks, chunkSize,\nsequentialAccessOutput, namedVectors);\n\n} else if (processIdf) {\n\n        DictionaryVectorizer.createTermFrequencyVectors(tokenizedPath,\noutputDir, tfDirName, conf, minSupport, maxNGramSize,\n\n          minLLRValue, -1.0f, false, reduceTasks, chunkSize,\nsequentialAccessOutput, namedVectors);\n\n}\n\n-- \n\nThanks,\nJohn C\n\n\n\n\n-- \n\n-- John C\n \n\n
<CAJwFCa3kKbrcDpQFQ6+L9sTgXEKK2cNkT-tYEa-fVpY7_ScQ4Q@mail.gmail.com>	Ted Dunning 	ted.dunning@gmail.com	Re: About QRDecomposition	2012-01-19T00:01:41Z	<CAPbe6MPusbaZ7qsfLnmcYydFWm9jrM+59pLijRQvQcEd_GU5Cw@mail.gmail.com>	There are lots of QR decomposition algorithms and the results are not\nnecessarily unique, especially for rank deficient inputs.\n\nIf you post your exact results, I could comment more specifically.  Without\nmore details, I really can't answer your question in any specific way.\n\nOn Wed, Jan 18, 2012 at 9:39 AM, 刘鎏 <liuliu.tju@gmail.com> wrote:\n\n> Hi,\n> When I run QRDecomposition in mahout , I find the result of Q or R is\n> different from the example in wiki(\n> http://en.wikipedia.org/wiki/QR_decomposition#Using_Householder_reflections\n> ).\n> After reading the source code, I find the implement of QR decomposition is\n> exactly different from the tradition way such as the procedure in wiki.\n> Could any one show why mahout implement it in such a way? Thanks for your\n> replies!\n>\n> Liu Liu\n>\n \n\n
<1326943127.4098.YahooMailNeo@web113302.mail.gq1.yahoo.com>	Vikas Pandya 	vikasdp@yahoo.com	How to present mahout cluster in combination with Solr results	2012-01-19T03:18:47Z		Hello,\n\nI have successfully created vectors from reading my existing Solr Index. Then created sequenceFile and mahout clusters from it. As I understand that currently solr and mahout clustering aren't integrated, what's the best way to represent mahout clusters to the user? Mine is a search application which renders results by querying solr index. Now I need to incorporate Mahout created clusters in the result. While Solr-Mahout integration isn't there yet, what's the best alternative way to represent this info?\n\nThanks, \n\n
<CAPbe6MM9_VHqXVTGjoxpejLJYNmK7bp00djbmvbHHvcg7Vw70A@mail.gmail.com>	刘鎏 	liuliu.tju@gmail.com	Re: About QRDecomposition	2012-01-19T07:25:54Z	<CAJwFCa3kKbrcDpQFQ6+L9sTgXEKK2cNkT-tYEa-fVpY7_ScQ4Q@mail.gmail.com>	Hi, Ted\n\nThanks for your comment. The original matrix is:\n\nqRef - 3 x 3\n  12.00000 -51.00000   4.00000\n   6.00000 167.00000 -68.00000\n  -4.00000  24.00000 -41.00000\n\nThe result of mahout:\nq - 3 x 3\n  -0.85714   0.39429  -0.33143\n  -0.42857  -0.90286   0.03429\n   0.28571  -0.17143  -0.94286\n\nr - 3 x 3\n -14.00000 -21.00000  14.00000\n   0.00000-175.00000  70.00000\n   0.00000   0.00000  35.00000\n\nThe result of wiki(http://en.wikipedia.org/wiki/QR_decomposition):\n\nq - 3 x 3\n  -0.85714   0.3110  -0.4106\n  -0.42857  -0.8728   0.2335\n  -0.28571   0.3761   0.8814\n\nr - 3 x 3\n -14.00000 -8.4286  -2.0000\n   0.00000  -187.1736  -35.1241\n   0.00000   0.00000    -32.1761\n\nI notice that there are differences in computing householder vector, given\ninput vector x,\n\nIn mahout, householder vector is computed as v= x/||x||,\nhowever in wiki, it is computed as u = x+||x||*e1, v=u/||u||.\n\nSo, would you please give me  some clues about why did mahout(actually\nJAMA) do like that?\nThanks for your reply!\n\nLiu Liu\nOn Thu, Jan 19, 2012 at 8:01 AM, Ted Dunning <ted.dunning@gmail.com> wrote:\n\n> There are lots of QR decomposition algorithms and the results are not\n> necessarily unique, especially for rank deficient inputs.\n>\n> If you post your exact results, I could comment more specifically.  Without\n> more details, I really can't answer your question in any specific way.\n>\n> On Wed, Jan 18, 2012 at 9:39 AM, 刘鎏 <liuliu.tju@gmail.com> wrote:\n>\n> > Hi,\n> > When I run QRDecomposition in mahout , I find the result of Q or R is\n> > different from the example in wiki(\n> >\n> http://en.wikipedia.org/wiki/QR_decomposition#Using_Householder_reflections\n> > ).\n> > After reading the source code, I find the implement of QR decomposition\n> is\n> > exactly different from the tradition way such as the procedure in wiki.\n> > Could any one show why mahout implement it in such a way? Thanks for your\n> > replies!\n> >\n> > Liu Liu\n> >\n>\n\n\n\n-- \n刘鎏\n\n奇艺公司\n北京市海淀区海淀东三街2号欧美汇大厦8层 100080\n\nPhone: (010) 6267-7240\nE-mail: liuliu@qiyi.com\nMSN: alex_rose2001@hotmail.com\n \n\n
<871uqwxhq1.fsf@no-fixed-abode.cable.virginmedia.net>	Paul Rudin 	paul@rudin.co.uk	stackoverflow - recursion in LuceneIterator.computeNext	2012-01-19T07:40:54Z		\nI have a large lucene index from which I'm trying to extract term\nvectors. I get a stackoverflow error, which is believe is caused by the\nrecursion in LuceneIterator.computeNext().  I could increase the stack\nsize, but with big enough data there could always be a problem.\n\nI have a modified version that uses a loop instead of the recursion\nwhich seems to work OK. Should I put a patch somewhere?\n\nOn a related note I'd quite like to be able to write the vectors\nstraight to s3 without writing to a local file first - is there a\npractical way to do this?\n\n\n\n \n
<CAKF+NEb002Q=OxTF5L-fMH+dPNR4AyOx_xJLGLec0eqtwQQddA@mail.gmail.com>	Daniel Korzekwa 	daniel.korzekwa@gmail.com	Why Mahout bayes implementation is tightly coupled with Hadoop?	2012-01-19T08:07:21Z		Hello,\n\nI'd like to ask about the rationale behind the design of Mahout Bayes\nalgorithm. I found that Mahout Bayes implementation is tightly coupled with\nHadoop MapReduce classes, for instance\n------------------------------\n/**\n * Reads the input train set(preprocessed using the {@link\norg.apache.mahout.classifier.BayesFileFormatter}).\n */\npublic class BayesFeatureMapper extends MapReduceBase implements\nMapper<Text,Text,StringTuple,DoubleWritable>\n------------------------------\n\nI would expected that Bayes algorithm is implemented as a pojo class(es)\nand tested with simple unit test, which knows nothing about hadoop,\ndatastores, filessystems, etc. And then a Hadoop wrapper around pojo bayes\nimplementation should be provided.\n\nWith a current design, first it's quite difficult to understand how the\nalgorithm works, as you need to go through several hadoop related classes.\nSecond, you can't just simply run BayesClassifierSelfTest test, which takes\ninput text in a format:\n\n----------------------------\n public static final String[][] DATA = {\n    {\n      "mahout",\n      "Mahout's goal is to build scalable machine learning libraries. With\nscalable we mean: "\n         + "Scalable to reasonably large data sets. Our core algorithms for\nclustering,"\n         + " classfication and batch based collaborative filtering are\nimplemented on top "\n         + "of Apache Hadoop using the map/reduce paradigm. However we do\nnot restrict "\n         + "contributions to Hadoop based implementations: Contributions\nthat run on"},\n    {\n      "mahout",\n      " a single node or on a non-Hadoop cluster are welcome as well. The\ncore"\n         + " libraries are highly optimized to allow for good performance\nalso for"\n         + " non-distribu\n-------------------------------\n\nand classifies it, as this test depends on hadoop and fails on Windows when\nrunning from Eclipse. Third, I don't always want to run this algorithm in a\nhadoop world. I may want to use some other map reduce provider. Also when I\nrun bayes classifier with:\n./mahout trainclassifier -i /mnt/hgfs/C/daniel/my_fav_data/test -o model\n-type bayes -ng 1 -source hdfs, it takes 40 seconds to train a model for a\nfile with 6 lines, even though hadoop is not really used. Is it so long\nbecause of all those hadoop related abstractions?\n\nRegards.\nDaniel\n\n-- \nDaniel Korzekwa\nSoftware Engineer\npriv: http://danmachine.com\nblog: http://blog.danmachine.com\n \n\n
<CAJwFCa3UewDWxfFbHwYL4RgwY=hkFUfP4jRqcFB_N0B89fQg5g@mail.gmail.com>	Ted Dunning 	ted.dunning@gmail.com	Re: About QRDecomposition	2012-01-19T08:18:20Z	<CAPbe6MM9_VHqXVTGjoxpejLJYNmK7bp00djbmvbHHvcg7Vw70A@mail.gmail.com>	And on the subject of correctness, R gives exactly the Mahout results.\n\n> x\n  V1  V2  V3\n1 12 -51   4\n2  6 167 -68\n3 -4  24 -41\n\n> qr.Q(qr(x))\n           [,1]       [,2]        [,3]\n[1,] -0.8571429  0.3942857  0.33142857\n[2,] -0.4285714 -0.9028571 -0.03428571\n[3,]  0.2857143 -0.1714286  0.94285714\n> qr.R(qr(x))\n      V1   V2  V3\n[1,] -14  -21  14\n[2,]   0 -175  70\n[3,]   0    0 -35\n>\n\nAnd the results you quote from Wikipedia don't actually combine to get the\nright result:\n\n> as.matrix(q) %*% as.matrix(r)\n           V1        V2         V3\n[1,] 11.99996 -50.98650   4.002192\n[2,]  5.99998 166.97736  24.000335\n[3,]  3.99994 -67.98786 -40.998769\n> q\n        V1      V2      V3\n1 -0.85714  0.3110 -0.4106\n2 -0.42857 -0.8728  0.2335\n3 -0.28571  0.3761  0.8814\n> r\n   V1        V2       V3\n1 -14   -8.4286  -2.0000\n2   0 -187.1736 -35.1241\n3   0    0.0000 -32.1761\n>\n\n\n2012/1/19 刘鎏 <liuliu.tju@gmail.com>\n\n> Hi, Ted\n>\n> Thanks for your comment. The original matrix is:\n>\n> qRef - 3 x 3\n>  12.00000 -51.00000   4.00000\n>   6.00000 167.00000 -68.00000\n>  -4.00000  24.00000 -41.00000\n>\n> The result of mahout:\n> q - 3 x 3\n>  -0.85714   0.39429  -0.33143\n>  -0.42857  -0.90286   0.03429\n>   0.28571  -0.17143  -0.94286\n>\n> r - 3 x 3\n>  -14.00000 -21.00000  14.00000\n>   0.00000-175.00000  70.00000\n>   0.00000   0.00000  35.00000\n>\n> The result of wiki(http://en.wikipedia.org/wiki/QR_decomposition):\n>\n> q - 3 x 3\n>  -0.85714   0.3110  -0.4106\n>  -0.42857  -0.8728   0.2335\n>  -0.28571   0.3761   0.8814\n>\n> r - 3 x 3\n>  -14.00000 -8.4286  -2.0000\n>   0.00000  -187.1736  -35.1241\n>   0.00000   0.00000    -32.1761\n>\n> I notice that there are differences in computing householder vector, given\n> input vector x,\n>\n> In mahout, householder vector is computed as v= x/||x||,\n> however in wiki, it is computed as u = x+||x||*e1, v=u/||u||.\n>\n> So, would you please give me  some clues about why did mahout(actually\n> JAMA) do like that?\n> Thanks for your reply!\n>\n> Liu Liu\n> On Thu, Jan 19, 2012 at 8:01 AM, Ted Dunning <ted.dunning@gmail.com>\n> wrote:\n>\n> > There are lots of QR decomposition algorithms and the results are not\n> > necessarily unique, especially for rank deficient inputs.\n> >\n> > If you post your exact results, I could comment more specifically.\n>  Without\n> > more details, I really can't answer your question in any specific way.\n> >\n> > On Wed, Jan 18, 2012 at 9:39 AM, 刘鎏 <liuliu.tju@gmail.com> wrote:\n> >\n> > > Hi,\n> > > When I run QRDecomposition in mahout , I find the result of Q or R is\n> > > different from the example in wiki(\n> > >\n> >\n> http://en.wikipedia.org/wiki/QR_decomposition#Using_Householder_reflections\n> > > ).\n> > > After reading the source code, I find the implement of QR decomposition\n> > is\n> > > exactly different from the tradition way such as the procedure in wiki.\n> > > Could any one show why mahout implement it in such a way? Thanks for\n> your\n> > > replies!\n> > >\n> > > Liu Liu\n> > >\n> >\n>\n>\n>\n> --\n> 刘鎏\n>\n> 奇艺公司\n> 北京市海淀区海淀东三街2号欧美汇大厦8层 100080\n>\n> Phone: (010) 6267-7240\n> E-mail: liuliu@qiyi.com\n> MSN: alex_rose2001@hotmail.com\n>\n \n\n
<CAJwFCa3NT+2Qb4=eeOoj6bRFtvnZo0DG=ak3Af_Yg3N2ThHr8g@mail.gmail.com>	Ted Dunning 	ted.dunning@gmail.com	Re: About QRDecomposition	2012-01-19T08:24:29Z	<CAJwFCa3UewDWxfFbHwYL4RgwY=hkFUfP4jRqcFB_N0B89fQg5g@mail.gmail.com>	I put a comment on the wikipedia talk page.  The wikipedia example is just\nwrong.\n\nOn Thu, Jan 19, 2012 at 8:18 AM, Ted Dunning <ted.dunning@gmail.com> wrote:\n\n> And on the subject of correctness, R gives exactly the Mahout results.\n>\n> > x\n>   V1  V2  V3\n> 1 12 -51   4\n> 2  6 167 -68\n> 3 -4  24 -41\n>\n> > qr.Q(qr(x))\n>            [,1]       [,2]        [,3]\n> [1,] -0.8571429  0.3942857  0.33142857\n> [2,] -0.4285714 -0.9028571 -0.03428571\n> [3,]  0.2857143 -0.1714286  0.94285714\n> > qr.R(qr(x))\n>       V1   V2  V3\n> [1,] -14  -21  14\n> [2,]   0 -175  70\n> [3,]   0    0 -35\n> >\n>\n> And the results you quote from Wikipedia don't actually combine to get the\n> right result:\n>\n> > as.matrix(q) %*% as.matrix(r)\n>            V1        V2         V3\n> [1,] 11.99996 -50.98650   4.002192\n> [2,]  5.99998 166.97736  24.000335\n> [3,]  3.99994 -67.98786 -40.998769\n> > q\n>         V1      V2      V3\n> 1 -0.85714  0.3110 -0.4106\n> 2 -0.42857 -0.8728  0.2335\n> 3 -0.28571  0.3761  0.8814\n> > r\n>    V1        V2       V3\n> 1 -14   -8.4286  -2.0000\n> 2   0 -187.1736 -35.1241\n> 3   0    0.0000 -32.1761\n> >\n>\n>\n> 2012/1/19 刘鎏 <liuliu.tju@gmail.com>\n>\n>> Hi, Ted\n>>\n>> Thanks for your comment. The original matrix is:\n>>\n>> qRef - 3 x 3\n>>  12.00000 -51.00000   4.00000\n>>   6.00000 167.00000 -68.00000\n>>  -4.00000  24.00000 -41.00000\n>>\n>> The result of mahout:\n>> q - 3 x 3\n>>  -0.85714   0.39429  -0.33143\n>>  -0.42857  -0.90286   0.03429\n>>   0.28571  -0.17143  -0.94286\n>>\n>> r - 3 x 3\n>>  -14.00000 -21.00000  14.00000\n>>   0.00000-175.00000  70.00000\n>>   0.00000   0.00000  35.00000\n>>\n>> The result of wiki(http://en.wikipedia.org/wiki/QR_decomposition):\n>>\n>> q - 3 x 3\n>>  -0.85714   0.3110  -0.4106\n>>  -0.42857  -0.8728   0.2335\n>>  -0.28571   0.3761   0.8814\n>>\n>> r - 3 x 3\n>>  -14.00000 -8.4286  -2.0000\n>>   0.00000  -187.1736  -35.1241\n>>   0.00000   0.00000    -32.1761\n>>\n>> I notice that there are differences in computing householder vector, given\n>> input vector x,\n>>\n>> In mahout, householder vector is computed as v= x/||x||,\n>> however in wiki, it is computed as u = x+||x||*e1, v=u/||u||.\n>>\n>> So, would you please give me  some clues about why did mahout(actually\n>> JAMA) do like that?\n>> Thanks for your reply!\n>>\n>> Liu Liu\n>> On Thu, Jan 19, 2012 at 8:01 AM, Ted Dunning <ted.dunning@gmail.com>\n>> wrote:\n>>\n>> > There are lots of QR decomposition algorithms and the results are not\n>> > necessarily unique, especially for rank deficient inputs.\n>> >\n>> > If you post your exact results, I could comment more specifically.\n>>  Without\n>> > more details, I really can't answer your question in any specific way.\n>> >\n>> > On Wed, Jan 18, 2012 at 9:39 AM, 刘鎏 <liuliu.tju@gmail.com> wrote:\n>> >\n>> > > Hi,\n>> > > When I run QRDecomposition in mahout , I find the result of Q or R is\n>> > > different from the example in wiki(\n>> > >\n>> >\n>> http://en.wikipedia.org/wiki/QR_decomposition#Using_Householder_reflections\n>> > > ).\n>> > > After reading the source code, I find the implement of QR\n>> decomposition\n>> > is\n>> > > exactly different from the tradition way such as the procedure in\n>> wiki.\n>> > > Could any one show why mahout implement it in such a way? Thanks for\n>> your\n>> > > replies!\n>> > >\n>> > > Liu Liu\n>> > >\n>> >\n>>\n>>\n>>\n>> --\n>> 刘鎏\n>>\n>> 奇艺公司\n>> 北京市海淀区海淀东三街2号欧美汇大厦8层 100080\n>>\n>> Phone: (010) 6267-7240\n>> E-mail: liuliu@qiyi.com\n>> MSN: alex_rose2001@hotmail.com\n>>\n>\n>\n \n\n
<CAJwFCa3uFoayyKnrkQOo8nPPZm73wKmEV=A+XN6wDpyuAf+wjQ@mail.gmail.com>	Ted Dunning 	ted.dunning@gmail.com	Re: Why Mahout bayes implementation is tightly coupled with Hadoop?	2012-01-19T08:25:36Z	<CAKF+NEb002Q=OxTF5L-fMH+dPNR4AyOx_xJLGLec0eqtwQQddA@mail.gmail.com>	Yes.\n\nThe use of Hadoop here makes things silly slow.\n\nOn Thu, Jan 19, 2012 at 8:07 AM, Daniel Korzekwa\n<daniel.korzekwa@gmail.com>wrote:\n\n> ./mahout trainclassifier -i /mnt/hgfs/C/daniel/my_fav_data/test -o model\n> -type bayes -ng 1 -source hdfs, it takes 40 seconds to train a model for a\n> file with 6 lines, even though hadoop is not really used. Is it so long\n> because of all those hadoop related abstractions?\n>\n \n\n
<4F17D843.7090000@gmail.com>	Ioan Eugen Stan 	stan.ieugen@gmail.com	Re: How to present mahout cluster in combination with Solr results	2012-01-19T08:45:55Z	<1326943127.4098.YahooMailNeo@web113302.mail.gq1.yahoo.com>	Pe 19.01.2012 05:18, Vikas Pandya a scris:\n> Hello,\n>\n> I have successfully created vectors from reading my existing Solr Index. Then created sequenceFile and mahout clusters from it. As I understand that currently solr and mahout clustering aren't integrated, what's the best way to represent mahout clusters to the user? Mine is a search application which renders results by querying solr index. Now I need to incorporate Mahout created clusters in the result. While Solr-Mahout integration isn't there yet, what's the best alternative way to represent this info?\n>\n> Thanks,\n\nThe only thing I can think of is to render the results yourself by \nreading the clusters. It depends very much on what information are you \ntrying to extract and present to the user. I can think of about two \nthings that you can find  out:\n\n- similar documents to the ones provided by a Solr search (by getting \nthe cluster to which they belong and getting the documents).\n- documents that have top terms that match the search query\n\nYou can find out good examples on how to do this by looking at the \nClusterDumper utility that reads and dumps clusters.\n\nHope this helps,\n\n-- \nIoan Eugen Stan\nhttp://ieugen.blogspot.com\n\n \n
<CAEccTyzP+2oZ4G-RZ6y_JDHvT6nwVv94=3qPqsFcGiZ0+fSDQg@mail.gmail.com>	Sean Owen 	srowen@gmail.com	Re: Why Mahout bayes implementation is tightly coupled with Hadoop?	2012-01-19T08:59:06Z	<CAKF+NEb002Q=OxTF5L-fMH+dPNR4AyOx_xJLGLec0eqtwQQddA@mail.gmail.com>	The test is just a tiny test. What you are observing is 99.9% Hadoop\noverhead. It's not as if a million times more text takes 40 million\nseconds.\n\nIf you don't have a very large pile of data, there's no need to use\nHadoop and indeed it is just complexity and overhead for no gain. A\nsimple Java implementation would be better.\n\nTurning it around, if you can't fit your information onto one machine,\na POJO won't work.\n\nThe whole raison d'être of the project is "scale" so it's not\nsurprising that you'll find Hadoop implementations first before\nnon-Hadoop here.\n\nIf you browse the recommender bits, in contrast, you'll see both\ndistributed and non-distributed versions indeed, and both have their\nplace.\n\nOn Thu, Jan 19, 2012 at 8:07 AM, Daniel Korzekwa\n<daniel.korzekwa@gmail.com> wrote:\n> Hello,\n>\n> I'd like to ask about the rationale behind the design of Mahout Bayes\n> algorithm. I found that Mahout Bayes implementation is tightly coupled with\n> Hadoop MapReduce classes, for instance\n> ------------------------------\n> /**\n>  * Reads the input train set(preprocessed using the {@link\n> org.apache.mahout.classifier.BayesFileFormatter}).\n>  */\n> public class BayesFeatureMapper extends MapReduceBase implements\n> Mapper<Text,Text,StringTuple,DoubleWritable>\n> ------------------------------\n>\n> I would expected that Bayes algorithm is implemented as a pojo class(es)\n> and tested with simple unit test, which knows nothing about hadoop,\n> datastores, filessystems, etc. And then a Hadoop wrapper around pojo bayes\n> implementation should be provided.\n>\n> With a current design, first it's quite difficult to understand how the\n> algorithm works, as you need to go through several hadoop related classes.\n> Second, you can't just simply run BayesClassifierSelfTest test, which takes\n> input text in a format:\n>\n> ----------------------------\n>  public static final String[][] DATA = {\n>    {\n>      "mahout",\n>      "Mahout's goal is to build scalable machine learning libraries. With\n> scalable we mean: "\n>         + "Scalable to reasonably large data sets. Our core algorithms for\n> clustering,"\n>         + " classfication and batch based collaborative filtering are\n> implemented on top "\n>         + "of Apache Hadoop using the map/reduce paradigm. However we do\n> not restrict "\n>         + "contributions to Hadoop based implementations: Contributions\n> that run on"},\n>    {\n>      "mahout",\n>      " a single node or on a non-Hadoop cluster are welcome as well. The\n> core"\n>         + " libraries are highly optimized to allow for good performance\n> also for"\n>         + " non-distribu\n> -------------------------------\n>\n> and classifies it, as this test depends on hadoop and fails on Windows when\n> running from Eclipse. Third, I don't always want to run this algorithm in a\n> hadoop world. I may want to use some other map reduce provider. Also when I\n> run bayes classifier with:\n> ./mahout trainclassifier -i /mnt/hgfs/C/daniel/my_fav_data/test -o model\n> -type bayes -ng 1 -source hdfs, it takes 40 seconds to train a model for a\n> file with 6 lines, even though hadoop is not really used. Is it so long\n> because of all those hadoop related abstractions?\n>\n> Regards.\n> Daniel\n>\n> --\n> Daniel Korzekwa\n> Software Engineer\n> priv: http://danmachine.com\n> blog: http://blog.danmachine.com\n\n \n
<CAEccTyyARzWXXJhwKrH-GfRKEt8Zo8Np=3CyR5WrgD=H4xvrgg@mail.gmail.com>	Sean Owen 	srowen@gmail.com	Re: stackoverflow - recursion in LuceneIterator.computeNext	2012-01-19T09:06:56Z	<871uqwxhq1.fsf@no-fixed-abode.cable.virginmedia.net>	Yes open a JIRA here with a patch: https://issues.apache.org/jira/browse/MAHOUT\n\nIf you're writing SequenceFiles, sure you can just write them straight\nto S3 by writing to a Path on s3:// -- is that something you've tried\nand doesn't work? should be that easy.\n\nOn Thu, Jan 19, 2012 at 7:40 AM, Paul Rudin <paul@rudin.co.uk> wrote:\n>\n> I have a large lucene index from which I'm trying to extract term\n> vectors. I get a stackoverflow error, which is believe is caused by the\n> recursion in LuceneIterator.computeNext().  I could increase the stack\n> size, but with big enough data there could always be a problem.\n>\n> I have a modified version that uses a loop instead of the recursion\n> which seems to work OK. Should I put a patch somewhere?\n>\n> On a related note I'd quite like to be able to write the vectors\n> straight to s3 without writing to a local file first - is there a\n> practical way to do this?\n>\n>\n\n \n
<CAEMjZ247m4FJeE7Am_DpOVTj7rOD8ZLo8TsJJVBSJ_Ls2NYHwg@mail.gmail.com>	Adil 	adil.chabaq@gmail.com	sentiment detection	2012-01-19T09:10:47Z		Hi,\n\nI'm new to Mahout, i want to know if i can use Mahout in order to detect if\ni text is positive or negative? if yes, there is some docs or examples on\nhow to do it?\n\nThanks\n\nAdil\n \n\n
<CAKF+NEb_QeS+xg6G0K4mhnor4XweETV3KD64hcpPd3nNzdVzaA@mail.gmail.com>	Daniel Korzekwa 	daniel.korzekwa@gmail.com	Re: sentiment detection	2012-01-19T09:19:46Z	<CAEMjZ247m4FJeE7Am_DpOVTj7rOD8ZLo8TsJJVBSJ_Ls2NYHwg@mail.gmail.com>	Read here:  https://cwiki.apache.org/MAHOUT/bayesian.html or Part III\nClassification in a Mahout in action book.\n\n2012/1/19 Adil <adil.chabaq@gmail.com>\n\n> Hi,\n>\n> I'm new to Mahout, i want to know if i can use Mahout in order to detect if\n> i text is positive or negative? if yes, there is some docs or examples on\n> how to do it?\n>\n> Thanks\n>\n> Adil\n>\n\n\n\n-- \nDaniel Korzekwa\nSoftware Engineer\npriv: http://danmachine.com\nblog: http://blog.danmachine.com\n \n\n
<CAJBgT0mC+rDSKPUbpq6F-o3HY0FNXt7YcYzj2EjW-M2QTp_Byw@mail.gmail.com>	Frank Scholten 	frank@frankscholten.nl	Re: How to present mahout cluster in combination with Solr results	2012-01-19T09:24:12Z	<1326943127.4098.YahooMailNeo@web113302.mail.gq1.yahoo.com>	Hi Vikas,\n\nI suggest indexing the cluster label, cluster size and\ncluster-document mappings so you can use that information to build a\ntag cloud of your data. Checkout this presentation\nhttp://java.dzone.com/videos/configuring-mahout-clustering\n\nCheers,\n\nFrank\n\nOn Thu, Jan 19, 2012 at 4:18 AM, Vikas Pandya <vikasdp@yahoo.com> wrote:\n> Hello,\n>\n> I have successfully created vectors from reading my existing Solr Index. Then created sequenceFile and mahout clusters from it. As I understand that currently solr and mahout clustering aren't integrated, what's the best way to represent mahout clusters to the user? Mine is a search application which renders results by querying solr index. Now I need to incorporate Mahout created clusters in the result. While Solr-Mahout integration isn't there yet, what's the best alternative way to represent this info?\n>\n> Thanks,\n\n \n
<CAKF+NEZh5S8uF1Ni6zH6fQ0XDPrkJMm8xN_-96Sc1_0qFwMVHQ@mail.gmail.com>	Daniel Korzekwa 	daniel.korzekwa@gmail.com	Re: Why Mahout bayes implementation is tightly coupled with Hadoop?	2012-01-19T09:37:24Z	<CAEccTyzP+2oZ4G-RZ6y_JDHvT6nwVv94=3qPqsFcGiZ0+fSDQg@mail.gmail.com>	Thanks Sean for your response.\n\nI fully understand the rationale behind Mahout, and yes I really like its\nscale approach. I asked this question, because I was having some problems\nto understand how bayes works in Mahout (my question is here:\nhttp://www.manning-sandbox.com/thread.jspa?threadID=48160&tstart=0).\nFinally I found in Mahout code, that priors are not used and that this\nbayes is not the same simple approach as I described in my question, but I\nspent quite a lot of time to go through all mahout bayes classes.\n\nWhen I first jumped into mahout code base I was expecting to see,\nmahout-algorithms (implemented as pure functions) and then mahout-hadoop,\nwhich takes those pure functions and reuse them in a context of hadoop.\n\nor something like:\n- Bayes functions.\n- Bayes ref impl - the simplest one, depends on Bayes functions.\n- Bayes hadoop impl - depends on Bayes functions and hadoop.\n\nRegarding to long time of running bayes against small training data set, I\nknow that this problem disappears when you process bigger data file, e.g.\nmore than 100K text records. But when you want to play with mahout, it's\nnice to start with some small files, and then waiting 40sec for every run\nis a waste of time\n\nPS. I'm in the middle of Mahout in Action book. Good stuff to you Sean and\nother authors, it's really enjoyable read.\n\nRegards.\nDaniel Korzekwa\n2012/1/19 Sean Owen <srowen@gmail.com>\n\n> The test is just a tiny test. What you are observing is 99.9% Hadoop\n> overhead. It's not as if a million times more text takes 40 million\n> seconds.\n>\n> If you don't have a very large pile of data, there's no need to use\n> Hadoop and indeed it is just complexity and overhead for no gain. A\n> simple Java implementation would be better.\n>\n> Turning it around, if you can't fit your information onto one machine,\n> a POJO won't work.\n>\n> The whole raison d'être of the project is "scale" so it's not\n> surprising that you'll find Hadoop implementations first before\n> non-Hadoop here.\n>\n> If you browse the recommender bits, in contrast, you'll see both\n> distributed and non-distributed versions indeed, and both have their\n> place.\n>\n> On Thu, Jan 19, 2012 at 8:07 AM, Daniel Korzekwa\n> <daniel.korzekwa@gmail.com> wrote:\n> > Hello,\n> >\n> > I'd like to ask about the rationale behind the design of Mahout Bayes\n> > algorithm. I found that Mahout Bayes implementation is tightly coupled\n> with\n> > Hadoop MapReduce classes, for instance\n> > ------------------------------\n> > /**\n> >  * Reads the input train set(preprocessed using the {@link\n> > org.apache.mahout.classifier.BayesFileFormatter}).\n> >  */\n> > public class BayesFeatureMapper extends MapReduceBase implements\n> > Mapper<Text,Text,StringTuple,DoubleWritable>\n> > ------------------------------\n> >\n> > I would expected that Bayes algorithm is implemented as a pojo class(es)\n> > and tested with simple unit test, which knows nothing about hadoop,\n> > datastores, filessystems, etc. And then a Hadoop wrapper around pojo\n> bayes\n> > implementation should be provided.\n> >\n> > With a current design, first it's quite difficult to understand how the\n> > algorithm works, as you need to go through several hadoop related\n> classes.\n> > Second, you can't just simply run BayesClassifierSelfTest test, which\n> takes\n> > input text in a format:\n> >\n> > ----------------------------\n> >  public static final String[][] DATA = {\n> >    {\n> >      "mahout",\n> >      "Mahout's goal is to build scalable machine learning libraries. With\n> > scalable we mean: "\n> >         + "Scalable to reasonably large data sets. Our core algorithms\n> for\n> > clustering,"\n> >         + " classfication and batch based collaborative filtering are\n> > implemented on top "\n> >         + "of Apache Hadoop using the map/reduce paradigm. However we do\n> > not restrict "\n> >         + "contributions to Hadoop based implementations: Contributions\n> > that run on"},\n> >    {\n> >      "mahout",\n> >      " a single node or on a non-Hadoop cluster are welcom e as well. The\n> > core"\n> >         + " libraries are highly optimized to allow for good performance\n> > also for"\n> >         + " non-distribu\n> > -------------------------------\n> >\n> > and classifies it, as this test depends on hadoop and fails on Windows\n> when\n> > running from Eclipse. Third, I don't always want to run this algorithm\n> in a\n> > hadoop world. I may want to use some other map reduce provider. Also\n> when I\n> > run bayes classifier with:\n> > ./mahout trainclassifier -i /mnt/hgfs/C/daniel/my_fav_data/test -o model\n> > -type bayes -ng 1 -source hdfs, it takes 40 seconds to train a model for\n> a\n> > file with 6 lines, even though hadoop is not really used. Is it so long\n> > because of all those hadoop related abstractions?\n> >\n> > Regards.\n> > Daniel\n> >\n> > --\n> > Daniel Korzekwa\n> > Software Engineer\n> > priv: http://danmachine.com\n> > blog: http://blog.danmachine.com\n>\n\n\n\n-- \nDaniel Korzekwa\nSoftware Engineer\npriv: http://danmachine.com\nblog: http://blog.danmachine.com\n \n\n
<CAEccTywk79dR98_mx6VsfD5YpSk+Ntm0HG_AzqjUQ_nryKHW3g@mail.gmail.com>	Sean Owen 	srowen@gmail.com	Re: Why Mahout bayes implementation is tightly coupled with Hadoop?	2012-01-19T09:43:56Z	<CAKF+NEZh5S8uF1Ni6zH6fQ0XDPrkJMm8xN_-96Sc1_0qFwMVHQ@mail.gmail.com>	It's not possible to write a pure Java algorithm, and then wrap or\nseason it with Hadoop to parallelize. It's just totally different when\nparallelized, and more so when ported to Hadoop.\n\nThat's not to say some small bits like key formulas can't be factored\nout, or that there could not be a separate non-Hadoop implementation.\nAnd I am not sure Bayes is a great example, of everything in the\nproject... I do not know how much it's been loved in the last year or\nmore. (Is anyone actually looking at it anymore?)\n\nThere's an open invite to improve things.\n\nI do agree that writing and reading Hadoop code is very hard.\n\nOn Thu, Jan 19, 2012 at 9:37 AM, Daniel Korzekwa\n<daniel.korzekwa@gmail.com> wrote:\n> Thanks Sean for your response.\n>\n> I fully understand the rationale behind Mahout, and yes I really like its\n> scale approach. I asked this question, because I was having some problems\n> to understand how bayes works in Mahout (my question is here:\n> http://www.manning-sandbox.com/thread.jspa?threadID=48160&tstart=0).\n> Finally I found in Mahout code, that priors are not used and that this\n> bayes is not the same simple approach as I described in my question, but I\n> spent quite a lot of time to go through all mahout bayes classes.\n>\n> When I first jumped into mahout code base I was expecting to see,\n> mahout-algorithms (implemented as pure functions) and then mahout-hadoop,\n> which takes those pure functions and reuse them in a context of hadoop.\n>\n> or something like:\n> - Bayes functions.\n> - Bayes ref impl - the simplest one, depends on Bayes functions.\n> - Bayes hadoop impl - depends on Bayes functions and hadoop.\n>\n> Regarding to long time of running bayes against small training data set, I\n> know that this problem disappears when you process bigger data file, e.g.\n> more than 100K text records. But when you want to play with mahout, it's\n> nice to start with some small files, and then waiting 40sec for every run\n> is a waste of time\n>\n> PS. I'm in the middle of Mahout in Action book. Good stuff to you Sean and\n> other authors, it's really enjoyable read.\n>\n> Regards.\n> Daniel Korzekwa\n\n \n
<4F17E77F.3060007@gmail.com>	Ioan Eugen Stan 	stan.ieugen@gmail.com	Re: How to present mahout cluster in combination with Solr results	2012-01-19T09:50:55Z	<CAJBgT0mC+rDSKPUbpq6F-o3HY0FNXt7YcYzj2EjW-M2QTp_Byw@mail.gmail.com>	Pe 19.01.2012 11:24, Frank Scholten a scris:\n> Hi Vikas,\n>\n> I suggest indexing the cluster label, cluster size and\n> cluster-document mappings so you can use that information to build a\n> tag cloud of your data. Checkout this presentation\n> http://java.dzone.com/videos/configuring-mahout-clustering\n>\n> Cheers,\n>\n> Frank\n\nWow Frank, this is good. I didn't thought of it this way.\n\n>\n> On Thu, Jan 19, 2012 at 4:18 AM, Vikas Pandya<vikasdp@yahoo.com>  wrote:\n>> Hello,\n>>\n>> I have successfully created vectors from reading my existing Solr Index. Then created sequenceFile and mahout clusters from it. As I understand that currently solr and mahout clustering aren't integrated, what's the best way to represent mahout clusters to the user? Mine is a search application which renders results by querying solr index. Now I need to incorporate Mahout created clusters in the result. While Solr-Mahout integration isn't there yet, what's the best alternative way to represent this info?\n>>\n>> Thanks,\n\n-- \nIoan Eugen Stan\nhttp://ieugen.blogspot.com\n\n \n
<CAKF+NEY5NM63hWavEtJPmONejH-6N20p4u9W=q302=Eprp9phA@mail.gmail.com>	Daniel Korzekwa 	daniel.korzekwa@gmail.com	Recommending/predicting 'my favorite drink' - asking for opinion about different approaches?	2012-01-19T11:15:23Z		Hello,\n\nI'm trying to solve a problem of recommending/predicting 'my favorite\ndrink' and I'm hoping to get some support from this energetic community. Of\ncourse, I plan to use Mahout to process tremendous amount of data.\n\nProblem definition:\nThere are 20 different drinks, e.g. pepsi, coke, fanta, etc.\nThere are millions of customers of a supermarket who were buying those\ndrinks over a period of lets say last three months.\n\nData definition:\n\nDrinks (id,name): [100:Pepsi],[101:Coke],....\n\nTransactions\ncustomer_id, list of bought drink ids\n1   100,100,100,101,101,101\n2   100, 102,106,106,106...\n....\n\nDefinition of 'my favorite drink' is a bit foggy. We don't have any\ntraining data we can learn from, e.g. list of fans for a given drink, the\nonly thing we have are transactions, and customer data (id, age, postcode).\nCustomer may not have a favorite drink and this should be predicted as well.\n\nThose are 4 approaches I came up for predicting 'my favorite drink'.\n\n1) 50% ratio - The drink, I buy the most. If the percentage of a my drink\ntransactions is >50% then this is my favorite drink. Otherwise I don't have\na favorite drink.\n\n2) Gini index, more clever version of 50% ratio, If I bought pepsi 4 times,\nand other 6 drinks once only each, then Pepsi is my favorite drink. Gini\nindex = 1 minus sum of squares of drink probabilities. In this case Gini =\n1 - (4/10)^2 + 6*(1/10)^2. I have a favorite drink if gini is <0.7.\n\n3) Rationale - My favorite drink not necessarily has to be the one I drink\nthe most. For example if I bought 49 CopaCopa drinks and 51 Pepsi drinks,\nthen CopaCopa drink is more likely my favorite one. This is based on\nobservations that customers who buy CopaCopa are more likely to buy Pepsi\n(because this is generally popular drink), than the other way round. If I\nbuy the same number of unpopular CopaCopa and popular Pepsi drinks then it\nprobably means I'm more likely a fan of CopaCopa.\n\nMethod 3a: Naive Bayes Text classifier.  For this approach I calculate\npriors - probability of buying a given drink using Maximum Likelihood based\non all customers transactions data, e.g. P(Pepsi)=0.2, P(CopaCopa)=0.02.\nAnd then I calculate conditional probabilities of buying a drink given I\nalso bought something else, e.g. P(CopaCopa | Pepsi) = 0.03 and\nP(Pepsi|CopaCopa) = 0.07.\n\nCustomer has a favorite drink if a posterior, e.g P(CopaCopa | Pepsi,\nPepsi, CopaCopa, CopaCopa) (probability of being a fan of CopaCopa given I\nbought both Pepsi and CopaCopa twice) is >50%.\n\nData for bayes classification (one record for a single drink transaction).\nThose five records represent a customer who bought three drinks 101,101,102\nand a customer who bought two drinks 105:\ndrink_id(prior)\nall_drink_ids_bought_by_customer_of_this_drink_transaction(prediction\nrecord)\n101 101,101,102\n101 101,101,102\n102 101,101,102\n105 105,105\n105 105,105\n\nMethod 3b: Logistic regression, I represent transactions as\n\nTarget = transaction drink id, prediction variables = percentages of drinks\nfor a given customer, who placed this transaction, e.g. for a single\ncustomer, who bought pepsi, pepsi, and copacopa, we have three\nclassification records (one per transaction):\n\ntarget, %pepsi, %copacopa, %coke,.....\npepsi, 2/3,1/3,0,0,0,0...\npepsi, 2/3,1/3,0,0,0,0...\ncopacopa, 2/3,1/3,0,0,0,0...\n\nCustomer has a favorite drink if a logistic regression predicts drink with\n>50% confidence, e.g. I take a customer who is represented by\nclassification record: 0.1(CopaCopa), 0.7(Pepsi),0(Coke)..... I'm fan of\nPepsi with a confidence level of 0.64.\n\nI would appreciate any feedback on presented approaches. Maybe there is a\nbetter way to address this problem? I would be also glad to hear on some\npapers describing similar prediction problems in various domains.\n\nRegards.\n\n-- \nDaniel Korzekwa\nSoftware Engineer\npriv: http://danmachine.com\nblog: http://blog.danmachine.com\n \n\n
<CAEccTyzNU=JZYj15EU=uO2jLRsncKeUFOv4ORV5o9A+t5j5Ghw@mail.gmail.com>	Sean Owen 	srowen@gmail.com	Re: Recommending/predicting 'my favorite drink' - asking for opinion about different approaches?	2012-01-19T11:26:56Z	<CAKF+NEY5NM63hWavEtJPmONejH-6N20p4u9W=q302=Eprp9phA@mail.gmail.com>	If the purchase data is really all you have... then I don't know if\nyou can do better than assuming that the drink that is bought most is\nthe favorite (or perhaps one of your more subtle suggestions, yes).\n\nThis isn't a recommender problem, it seems, since you are not trying\nto suggest other liked drinks.\n\nIt doesn't seem to be a supervised learning problem either since you\ndon't have outside information about what each customer really does\nlike best, which is needed to train.\n\nDo we need to define "favorite drink" more specifically here? are you\nreally trying to predict future purchases?\n\nOn Thu, Jan 19, 2012 at 11:15 AM, Daniel Korzekwa\n<daniel.korzekwa@gmail.com> wrote:\n> Hello,\n>\n> I'm trying to solve a problem of recommending/predicting 'my favorite\n> drink' and I'm hoping to get some support from this energetic community. Of\n> course, I plan to use Mahout to process tremendous amount of data.\n>\n> Problem definition:\n> There are 20 different drinks, e.g. pepsi, coke, fanta, etc.\n> There are millions of customers of a supermarket who were buying those\n> drinks over a period of lets say last three months.\n>\n> Data definition:\n>\n> Drinks (id,name): [100:Pepsi],[101:Coke],....\n>\n> Transactions\n> customer_id, list of bought drink ids\n> 1   100,100,100,101,101,101\n> 2   100, 102,106,106,106...\n> ....\n>\n> Definition of 'my favorite drink' is a bit foggy. We don't have any\n> training data we can learn from, e.g. list of fans for a given drink, the\n> only thing we have are transactions, and customer data (id, age, postcode).\n> Customer may not have a favorite drink and this should be predicted as well.\n>\n> Those are 4 approaches I came up for predicting 'my favorite drink'.\n>\n> 1) 50% ratio - The drink, I buy the most. If the percentage of a my drink\n> transactions is >50% then this is my favorite drink. Otherwise I don't have\n> a favorite drink.\n>\n> 2) Gini index, more clever version of 50% ratio, If I bought pepsi 4 times,\n> and other 6 drinks once only each, then Pepsi is my favorite drink. Gini\n> index = 1 minus sum of squares of drink probabilities. In this case Gini =\n> 1 - (4/10)^2 + 6*(1/10)^2. I have a favorite drink if gini is <0.7.\n>\n> 3) Rationale - My favorite drink not necessarily has to be the one I drink\n> the most. For example if I bought 49 CopaCopa drinks and 51 Pepsi drinks,\n> then CopaCopa drink is more likely my favorite one. This is based on\n> observations that customers who buy CopaCopa are more likely to buy Pepsi\n> (because this is generally popular drink), than the other way round. If I\n> buy the same number of unpopular CopaCopa and popular Pepsi drinks then it\n> probably means I'm more likely a fan of CopaCopa.\n>\n> Method 3a: Naive Bayes Text classifier.  For this approach I calculate\n> priors - probability of buying a given drink using Maximum Likelihood based\n> on all customers transactions data, e.g. P(Pepsi)=0.2, P(CopaCopa)=0.02.\n> And then I calculate conditional probabilities of buying a drink given I\n> also bought something else, e.g. P(CopaCopa | Pepsi) = 0.03 and\n> P(Pepsi|CopaCopa) = 0.07.\n>\n> Customer has a favorite drink if a posterior, e.g P(CopaCopa | Pepsi,\n> Pepsi, CopaCopa, CopaCopa) (probability of being a fan of CopaCopa given I\n> bought both Pepsi and CopaCopa twice) is >50%.\n>\n> Data for bayes classification (one record for a single drink transaction).\n> Those five records represent a customer who bought three drinks 101,101,102\n> and a customer who bought two drinks 105:\n> drink_id(prior)\n> all_drink_ids_bought_by_customer_of_this_drink_transaction(prediction\n> record)\n> 101 101,101,102\n> 101 101,101,102\n> 102 101,101,102\n> 105 105,105\n> 105 105,105\n>\n> Method 3b: Logistic regression, I represent transactions as\n>\n> Target = transaction drink id, prediction variables = percentages of drinks\n> for a given customer, who placed this transaction, e.g. for a single\n> customer, who bought pepsi, pepsi, and copacopa, we have three\n> classification records (one per transaction):\n>\n> target, % pepsi, %copacopa, %coke,.....\n> pepsi, 2/3,1/3,0,0,0,0...\n> pepsi, 2/3,1/3,0,0,0,0...\n> copacopa, 2/3,1/3,0,0,0,0...\n>\n> Customer has a favorite drink if a logistic regression predicts drink with\n>>50% confidence, e.g. I take a customer who is represented by\n> classification record: 0.1(CopaCopa), 0.7(Pepsi),0(Coke)..... I'm fan of\n> Pepsi with a confidence level of 0.64.\n>\n> I would appreciate any feedback on presented approaches. Maybe there is a\n> better way to address this problem? I would be also glad to hear on some\n> papers describing similar prediction problems in various domains.\n>\n> Regards.\n>\n> --\n> Daniel Korzekwa\n> Software Engineer\n> priv: http://danmachine.com\n> blog: http://blog.danmachine.com\n\n \n
<CAKF+NEbNpGVx0LkRt+6sH0WM9Oscs=kGyY263s62CmytAL9qSQ@mail.gmail.com>	Daniel Korzekwa 	daniel.korzekwa@gmail.com	Re: Recommending/predicting 'my favorite drink' - asking for opinion about different approaches?	2012-01-19T12:26:57Z	<CAEccTyzNU=JZYj15EU=uO2jLRsncKeUFOv4ORV5o9A+t5j5Ghw@mail.gmail.com>	I came up with the same conclusions you did below, it's neither pure\nrecommendation nor classification. It sounds similar to clustering with a\nfixed number of classes, e.g. one class per drink and one additional class\nfor non_fan. However I haven't achieved any satisfying results with this\nmethod yet.\n\nThe reasoning behind this 'my favorite drink' question is: If significant\nnumber of customers are fans of particular drink, then maybe it's better to\norganize shelves (or web shelves) with drinks in a more fans driven way to\nachieve better selling results?.\n\nOr maybe if we know that there are many fans of unpopular drink CopaCopa\nand then if we advert it more strongly we may get a big jump in selling.\nThose fans may be customers, who buy more Pepsi than CopaCopa, because\nPepsi is really well advertised and easily available. Looking only at\nindividual customer transactions, it may seem he is a fan of Pepsi, whereas\nif we look at all transactions for all customers with .e.g. Bayesian model.\nwe can classify more customers as fans of PopoPopa, which may push us to\nadvert PopaPopa more aggressively?\n\nIt's quite difficult to find research papers on such example of machine\nlearning, therefore I was hoping to find someone in this forum, who I could\ndiscuss this problem with.\n\nRegards.\nDaniel\n\n2012/1/19 Sean Owen <srowen@gmail.com>\n\n> If the purchase data is really all you have... then I don't know if\n> you can do better than assuming that the drink that is bought most is\n> the favorite (or perhaps one of your more subtle suggestions, yes).\n>\n> This isn't a recommender problem, it seems, since you are not trying\n> to suggest other liked drinks.\n>\n> It doesn't seem to be a supervised learning problem either since you\n> don't have outside information about what each customer really does\n> like best, which is needed to train.\n>\n> Do we need to define "favorite drink" more specifically here? are you\n> really trying to predict future purchases?\n>\n> On Thu, Jan 19, 2012 at 11:15 AM, Daniel Korzekwa\n> <daniel.korzekwa@gmail.com> wrote:\n> > Hello,\n> >\n> > I'm trying to solve a problem of recommending/predicting 'my favorite\n> > drink' and I'm hoping to get some support from this energetic community.\n> Of\n> > course, I plan to use Mahout to process tremendous amount of data.\n> >\n> > Problem definition:\n> > There are 20 different drinks, e.g. pepsi, coke, fanta, etc.\n> > There are millions of customers of a supermarket who were buying those\n> > drinks over a period of lets say last three months.\n> >\n> > Data definition:\n> >\n> > Drinks (id,name): [100:Pepsi],[101:Coke],....\n> >\n> > Transactions\n> > customer_id, list of bought drink ids\n> > 1   100,100,100,101,101,101\n> > 2   100, 102,106,106,106...\n> > ....\n> >\n> > Definition of 'my favorite drink' is a bit foggy. We don't have any\n> > training data we can learn from, e.g. list of fans for a given drink, the\n> > only thing we have are transactions, and customer data (id, age,\n> postcode).\n> > Customer may not have a favorite drink and this should be predicted as\n> well.\n> >\n> > Those are 4 approaches I came up for predicting 'my favorite drink'.\n> >\n> > 1) 50% ratio - The drink, I buy the most. If the percentage of a my drink\n> > transactions is >50% then this is my favorite drink. Otherwise I don't\n> have\n> > a favorite drink.\n> >\n> > 2) Gini index, more clever version of 50% ratio, If I bought pepsi 4\n> times,\n> > and other 6 drinks once only each, then Pepsi is my favorite drink. Gini\n> > index = 1 minus sum of squares of drink probabilities. In this case Gini\n> =\n> > 1 - (4/10)^2 + 6*(1/10)^2. I have a favorite drink if gini is <0.7.\n> >\n> > 3) Rationale - My favorite drink not necessarily has to be the one I\n> drink\n> > the most. For example if I bought 49 CopaCopa drinks and 51 Pepsi drinks,\n> > then CopaCopa drink is more likely my favorite one. This is based on\n> > observations that customers who buy CopaCopa are more likely to buy Pepsi\n> > (because this is generally popular drink), than the other way round. If I\n> > buy the same number of unpopular CopaCopa and popular Pepsi drinks  then\n> it\n> > probably means I'm more likely a fan of CopaCopa.\n> >\n> > Method 3a: Naive Bayes Text classifier.  For this approach I calculate\n> > priors - probability of buying a given drink using Maximum Likelihood\n> based\n> > on all customers transactions data, e.g. P(Pepsi)=0.2, P(CopaCopa)=0.02.\n> > And then I calculate conditional probabilities of buying a drink given I\n> > also bought something else, e.g. P(CopaCopa | Pepsi) = 0.03 and\n> > P(Pepsi|CopaCopa) = 0.07.\n> >\n> > Customer has a favorite drink if a posterior, e.g P(CopaCopa | Pepsi,\n> > Pepsi, CopaCopa, CopaCopa) (probability of being a fan of CopaCopa given\n> I\n> > bought both Pepsi and CopaCopa twice) is >50%.\n> >\n> > Data for bayes classification (one record for a single drink\n> transaction).\n> > Those five records represent a customer who bought three drinks\n> 101,101,102\n> > and a customer who bought two drinks 105:\n> > drink_id(prior)\n> > all_drink_ids_bought_by_customer_of_this_drink_transaction(prediction\n> > record)\n> > 101 101,101,102\n> > 101 101,101,102\n> > 102 101,101,102\n> > 105 105,105\n> > 105 105,105\n> >\n> > Method 3b: Logistic regression, I represent transactions as\n> >\n> > Target = transaction drink id, prediction variables = percentages of\n> drinks\n> > for a given customer, who placed this transaction, e.g. for a single\n> > customer, who bought pepsi, pepsi, and copacopa, we have three\n> > classification records (one per transaction):\n> >\n> > target, %pepsi, %copacopa, %coke,.....\n> > pepsi, 2/3,1/3,0,0,0,0...\n> > pepsi, 2/3,1/3,0,0,0,0...\n> > copacopa, 2/3,1/3,0,0,0,0...\n> >\n> > Customer has a favorite drink if a logistic regression predicts drink\n> with\n> >>50% confidence, e.g. I take a customer who is represented by\n> > classification record: 0.1(CopaCopa), 0.7(Pepsi),0(Coke)..... I'm fan of\n> > Pepsi with a confidence level of 0.64.\n> >\n> > I would appreciate any feedback on presented approaches. Maybe there is a\n> > better way to address this problem? I would be also glad to hear on some\n> > papers describing similar prediction problems in various domains.\n> >\n> > Regards.\n> >\n> > --\n> > Daniel Korzekwa\n> > Software Engineer\n> > priv: http://danmachine.com\n> > blog: http://blog.danmachine.com\n>\n\n\n\n-- \nDaniel Korzekwa\nSoftware Engineer\npriv: http://danmachine.com\nblog: http://blog.danmachine.com\n \n\n
<CAEccTyyLo46qWDDDd1iNOJSv5RAXOx6A9iTN=rGHN=T-4+TFoQ@mail.gmail.com>	Sean Owen 	srowen@gmail.com	Re: Recommending/predicting 'my favorite drink' - asking for opinion about different approaches?	2012-01-19T12:34:19Z	<CAKF+NEbNpGVx0LkRt+6sH0WM9Oscs=kGyY263s62CmytAL9qSQ@mail.gmail.com>	On Thu, Jan 19, 2012 at 12:26 PM, Daniel Korzekwa\n<daniel.korzekwa@gmail.com> wrote:\n> I came up with the same conclusions you did below, it's neither pure\n> recommendation nor classification. It sounds similar to clustering with a\n> fixed number of classes, e.g. one class per drink and one additional class\n> for non_fan. However I haven't achieved any satisfying results with this\n> method yet.\n\nMaybe I'm oversimplifying what you suggest, but it sounds like you're\ndefining a "my favorite drink is X" class, where favorite means lots\nof purchases. And then classifying people as to whether their favorite\ndrink is X. But if that means lots of purchases, then this is just a\nroundabout way of asking, which drink they purchased a lot, which can\nbe computed straight from the data.\n\n>\n> The reasoning behind this 'my favorite drink' question is: If significant\n> number of customers are fans of particular drink, then maybe it's better to\n> organize shelves (or web shelves) with drinks in a more fans driven way to\n> achieve better selling results?.\n\nMaybe but this sounds like you're asking one of two much simpler\nquestions, which don't need machine learning:\n\n- What is the best-selling drink? so I can emphasize it, etc\n- What drink is the 'favorite' (most purchased) for the largest number\nof customers? so I can emphasize it, etc\n\n\n> Or maybe if we know that there are many fans of unpopular drink CopaCopa\n> and then if we advert it more strongly we may get a big jump in selling.\n> Those fans may be customers, who buy more Pepsi than CopaCopa, because\n> Pepsi is really well advertised and easily available. Looking only at\n> individual customer transactions, it may seem he is a fan of Pepsi, whereas\n> if we look at all transactions for all customers with .e.g. Bayesian model.\n> we can classify more customers as fans of PopoPopa, which may push us to\n> advert PopaPopa more aggressively?\n\nSame, given the way that popular is defined as just a function of\npurchases, isn't this just something like looking at purchases per\nmarketing dollar? Finding the one with the most return on marketing,\nunder the assumption that the marginal additional marketing spend will\nbe most effective?\n\n \n
<1326989105.19094.YahooMailNeo@web113320.mail.gq1.yahoo.com>	Vikas Pandya 	vikasdp@yahoo.com	Re: How to present mahout cluster in combination with Solr results	2012-01-19T16:05:05Z	<CAJBgT0mC+rDSKPUbpq6F-o3HY0FNXt7YcYzj2EjW-M2QTp_Byw@mail.gmail.com>	Hi Frank,\n\nThanks for the link. That was useful. It's still bit unclear on how he built his index. are we saying, we index  clusterId,clusterSize and clusterLable in the same index (where other data is indexed)? So one index will have two sets of Solr documents in it?  one containing cluster info? \n\nMy requirement again; I have bunch of db columns which are being indexed. e.g. \nTitle,             RiskLevel1, RiskLevel2,RiskLevel3 etc\nTitle1        High             Medium      Low\n\nCurrent requirement is to cluster documents based on their riskLevels and NOT the title. \n\nThanks,\n\n\n________________________________\n From: Frank Scholten <frank@frankscholten.nl>\nTo: user@mahout.apache.org; Vikas Pandya <vikasdp@yahoo.com> \nSent: Thursday, January 19, 2012 4:24 AM\nSubject: Re: How to present mahout cluster in combination with Solr results\n \nHi Vikas,\n\nI suggest indexing the cluster label, cluster size and\ncluster-document mappings so you can use that information to build a\ntag cloud of your data. Checkout this presentation\nhttp://java.dzone.com/videos/configuring-mahout-clustering\n\nCheers,\n\nFrank\n\nOn Thu, Jan 19, 2012 at 4:18 AM, Vikas Pandya <vikasdp@yahoo.com> wrote:\n> Hello,\n>\n> I have successfully created vectors from reading my existing Solr Index. Then created sequenceFile and mahout clusters from it. As I understand that currently solr and mahout clustering aren't integrated, what's the best way to represent mahout clusters to the user? Mine is a search application which renders results by querying solr index. Now I need to incorporate Mahout created clusters in the result. While Solr-Mahout integration isn't there yet, what's the best alternative way to represent this info?\n>\n> Thanks, \n\n
<361FC4FA-BCCC-453C-A843-9907F9DA13E2@yahoo.com>	Jonathan 	fontajo@yahoo.com	Event processing	2012-01-19T16:15:29Z		Hi everyone,\n   I've been researching and relatively new to machine learning however familiar with distributed processing. I was wondering if there was a potential use case for mahout within the pub / sub model. \n\n  I've been considering intercepting all messages and if possible have the machine define features of the data, if data then begins arriving that is significantly outside the bounds of the current feature set then forcing a new event queue to be spawned in the parent system and publish the message to the new queue. Any thoughts on whether this would be a valid use case for mahout or suggestions?\n\nThanks!\n\n\n\n \n
<4F184BD0.1030409@gmail.com>	Eli Finkelshteyn 	iefinkel@gmail.com	Switching Mahout Version Project is Built On?	2012-01-19T16:58:56Z		Hi Folks,\nThis might be more of a Java/Maven question than a Mahout question, and \nif it is I apologize as I'm a bit new to those as well as Mahout. I'm \ncurrently working with a project that's built on top of Mahout 0.6, and \nI'm hoping to see if it will still run on the Cloudera supported \n0.5-cdh3u2. This in turn led me to the broader question: what's the \neasiest way to switch in a new version of Mahout to a project built on \ntop of it, or what's the best way to build a project that implements \nMahout classes so that I can easily try it on top of a different version \nof Mahout?\n\nCheers,\nEli\n \n\n
<CAJwFCa0XZC_L09cGsRMPcGGiBoEE0vY1k5BCgg+E23ajsO-jvg@mail.gmail.com>	Ted Dunning 	ted.dunning@gmail.com	Re: Why Mahout bayes implementation is tightly coupled with Hadoop?	2012-01-19T22:27:27Z	<CAEccTyz9FmE_jeQ9ieSvwCBmziOsTRa-B_feFcVqdwhh4zP25w@mail.gmail.com>	Mike,\n\nI think that where you are going is that Mahout might be well served by\nnon-Hadoop implementations or map-reduce or by non-map-reduce frameworks,\nespecially where smaller data and experimental use is concerned.\n\nYou are right.  Or, at least I agree with what I think you are saying.\n\nSean is also right.  The recommendations side of the house is pretty much\nin order with Hadoop and non-Hadoop versions of all key programs.\n\nThe classification and clustering side of the house is in a different\nstate.  Naive Bayes has only Hadoop based code and not easy-to-read code at\nthat.  The generalized linear regression stuff has sequential versions\nonly.  The clustering is a mix, but generally lacks good sequential\nimplementations.\n\nOver the horizon are things like Giraph and Storm which could make the\nworld a much better place for the clustering and classification codes by\ngiving implementations that would scale down as well as up.  But that is\nstill over the horizon.\n\n\nOn Thu, Jan 19, 2012 at 8:56 PM, Sean Owen <srowen@gmail.com> wrote:\n\n> Hadoop does abstract storage as far as we're concerned -- it hides\n> behind InputFormat/OutputFormat implementations. There's no in-memory\n> InputFormat that I know of but you could surely write one. I don't\n> think it's somehow the storage abstraction that is the difference\n> between a plain Java and a Hadoop-based implementation.\n>\n> It's either written to not have access to all data, in which case I\n> think you're going to always have something about shaped like the\n> Hadoop-based implementation of today, or it's written to be able to\n> access all data more or less at will, and then it ought be no more\n> complicated than a simple plain Java class indeed.\n>\n> On Thu, Jan 19, 2012 at 8:32 PM, Mike Spreitzer <mspreitz@us.ibm.com>\n> wrote:\n> > On the other hand, if Hadoop (or something like it) were based on a\n> > storage abstraction that had multiple implementations, say one in the\n> > client's memory and one in a cluster's disks (and maybe also others at\n> > other interesting points in between), and placement of computation were\n> > deferred to that store, then we could make Daniel happy both when\n> > developing and when doing real work on really large datasets.\n> >\n> > Regards,\n> > Mike\n>\n \n\n
<CAJwFCa0U6dL15A40msUyH4-4xTjpJx0Nq4HxQoauz_qG=tWgDQ@mail.gmail.com>	Ted Dunning 	ted.dunning@gmail.com	Mahout book on sale	2012-01-20T00:30:30Z		Manning is having a sale on big data books, notably including the Mahout in\nAction book.\n\nhttp://archive.constantcontact.com/fs043/1101335703814/archive/1109108833163.html\n \n\n
<87sjjawvou.fsf@no-fixed-abode.cable.virginmedia.net>	Paul Rudin 	paul@rudin.co.uk	Re: stackoverflow - recursion in LuceneIterator.computeNext	2012-01-20T09:49:05Z		\n> Yes open a JIRA here with a patch: https://issues.apache.org/jira/browse/MAHOUT\n\nOK - I put it here: \n<https://issues.apache.org/jira/browse/MAHOUT-951>\n\n> If you're writing SequenceFiles, sure you can just write them straight\n> to S3 by writing to a Path on s3:// -- is that something you've tried\n> and doesn't work? should be that easy.\n\nNo I haven't tried. I'm not sure how the authentication works if I use\ns3://... as a path. I can't immediately find anything by googling, but\nI'll investigate further.\n\n\n \n
<CAEccTyy3Sbsg=F0cxdC1e3-z75vfFsinTvA1g_0UFd4AZoQKig@mail.gmail.com>	Sean Owen 	srowen@gmail.com	Re: stackoverflow - recursion in LuceneIterator.computeNext	2012-01-20T09:58:11Z	<87sjjawvou.fsf@no-fixed-abode.cable.virginmedia.net>	That part is easy. On the job's Configuration, call:\n\n    set("fs.s3.awsAccessKeyId", "YourAccessKey");\n    set("fs.s3.awsSecretAccessKey", "YourSecretKey");\n\nIf you use s3n:// URLs, do the same with ".s3n.".\n\nI also set fs.defaultFS and fs.default.name to "s3://mybucket".\n\nOn Fri, Jan 20, 2012 at 9:49 AM, Paul Rudin <paul@rudin.co.uk> wrote:\n> No I haven't tried. I'm not sure how the authentication works if I use\n> s3://... as a path. I can't immediately find anything by googling, but\n> I'll investigate further.\n\n \n
<CAKF+NEaboMUmZh8LdayoDE2GKDpxfN8V6vLRrqoM5VFrFTMR7A@mail.gmail.com>	Daniel Korzekwa 	daniel.korzekwa@gmail.com	Re: Bayes classification - strange results	2012-01-20T13:01:49Z	<CAKF+NEaKxZUQ7oYsxwPJo_-Mp4-H213O_+ZRqPdmCutG4MxQsg@mail.gmail.com>	After analyzing Mahout bayes code I found that priors are not taken into\naccount. Mahout just provides some different version of Naive Bayes. Today\nI evaluated machine learning java library from  http://mallet.cs.umass.edu .\nFor the trivial test data presented below, it gives the results I was\nexpecting to see. All records are classified as T.\n\ncsvline:1 T 0.8709677419354839 F 0.12903225806451615\ncsvline:2 T 0.8709677419354839 F 0.12903225806451615\ncsvline:3 T 0.8709677419354839 F 0.12903225806451615\ncsvline:4 T 0.6923076923076923 F 0.30769230769230765\ncsvline:5 T 0.6923076923076923 F 0.30769230769230765\ncsvline:6 T 0.6923076923076923 F 0.30769230769230765\n\n\n2012/1/18 Daniel Korzekwa <daniel.korzekwa@gmail.com>\n\n> Hello,\n>\n> I'm training bayes classifier against this data (6 records):\n>\n> target, words\n> T A A A\n> T A A A\n> T A A A\n> T A A B\n> T A A B\n> F A A B\n>\n> with a command:\n>  ./mahout trainclassifier -i /mnt/hgfs/C/daniel/my_fav_data/test -o model\n> -type bayes -ng 1 -source hdfs\n>\n> then I test this classifier against the same data with:\n> ./mahout testclassifier -d /mnt/hgfs/C/daniel/my_fav_data/test -m model\n> -type bayes -ng 1 -source hdfs -method sequential -v\n>\n>  and I'm getting classification I cannot understand. All records are\n> classified as F, why is that?, shouldn't they be all classified as T?\n> 12/01/18 11:07:55 INFO bayes.TestClassifier: Line Number: 0 Line(30): T A\n> A A Expected Label: T Classified Label: F Correct: false\n> 12/01/18 11:07:55 INFO bayes.TestClassifier: Line Number: 1 Line(30): T A\n> A A Expected Label: T Classified Label: F Correct: false\n> 12/01/18 11:07:55 INFO bayes.TestClassifier: Line Number: 2 Line(30): T A\n> A A Expected Label: T Classified Label: F Correct: false\n> 12/01/18 11:07:55 INFO bayes.TestClassifier: Line Number: 3 Line(30): T A\n> A B Expected Label: T Classified Label: F Correct: false\n> 12/01/18 11:07:55 INFO bayes.TestClassifier: Line Number: 4 Line(30): T A\n> A B Expected Label: T Classified Label: F Correct: false\n> 12/01/18 11:07:55 INFO bayes.TestClassifier: Line Number: 5 Line(30): F A\n> A B Expected Label: F Classified Label: F Correct: true\n>\n> My reasoning (no smoothing applied):\n> Prior:\n> P(T) = 5/6\n> P(F) = 1/6\n>\n> P(A/T) = 13/15\n> P(A/F) = 2/3\n>\n> P(B/T) = 2/15\n> P(B/F) = 1/3\n>\n> Then I calculate posterior probability, e.g. P(T|A,A,B) = 0.7717 - record\n> classified as T.\n>\n> What is the reasoning behind classifying all records above as F?\n>\n> Any help much appreciated.\n>\n> PS. I was using mahout trunk from 16.01.2012.\n>\n> Regards.\n> Daniel\n>\n> --\n> Daniel Korzekwa\n> Software Engineer\n> priv: http://danmachine.com\n> blog: http://blog.danmachine.com\n>\n\n\n\n-- \nDaniel Korzekwa\nSoftware Engineer\npriv: http://danmachine.com\nblog: http://blog.danmachine.com\n \n\n
<CAJBgT0m89eamG0OTXcSEuExMP3u+bhz+orBuFyN4siGFPBiDcA@mail.gmail.com>	Frank Scholten 	frank@frankscholten.nl	Re: How to present mahout cluster in combination with Solr results	2012-01-20T17:48:04Z	<1327071675.74607.YahooMailNeo@web113305.mail.gq1.yahoo.com>	On Fri, Jan 20, 2012 at 4:01 PM, Vikas Pandya <vikasdp@yahoo.com> wrote:\n> From the example below, solr search results should be clustered in some\n> following way\n> list all the items which have matching RiskLevels e.g.\n>\n>\n> Cluster 1:\n> Title          RiskLevel1          RiskLevel2         RiskLevel3\n> abc            High                     Medium             Low\n> xyz            High                      Medium            High\n> def            Low                        Medium           High\n>\n> Cluster 2:\n> Title          RiskLevel1          RiskLevel2         RiskLevel3\n> omn            Low                     Medium             Low\n> yui            Low                      Medium            High\n> bnm            Medium             Medium           High\n>\n> Though I have a feeling I don't need to use Mahout clustering for this, I am\n> still trying to hook in mahout for this since we have more clustering\n> requirements in the pipeline to cluster based on other features (attributes\n> of objects).\n>\n\nYou only have 27 unique risklevel combinations. You could just sort by\nor more risklevels to get a sense of the data.\n\nIf you have more attributes then you could indeed look into clustering,\n\nCheers,\n\nFrank\n\n> Any thoughts?\n>\n> ________________________________\n> From: Vikas Pandya <vikasdp@yahoo.com>\n> To: Frank Scholten <frank@frankscholten.nl>; "user@mahout.apache.org"\n> <user@mahout.apache.org>\n> Sent: Thursday, January 19, 2012 11:05 AM\n>\n> Subject: Re: How to present mahout cluster in combination with Solr results\n>\n> Hi Frank,\n>\n> Thanks for the link. That was useful. It's still bit unclear on how he built\n> his index. are we saying, we index  clusterId,clusterSize and clusterLable\n> in the same index (where other data is indexed)? So one index will have two\n> sets of Solr documents in it?  one containing cluster info?\n>\n> My requirement again; I have bunch of db columns which are being indexed.\n> e.g.\n> Title,             RiskLevel1, RiskLevel2,RiskLevel3 etc\n> Title1        High             Medium      Low\n>\n> Current requirement is to cluster documents based on their riskLevels and\n> NOT the title.\n>\n> Thanks,\n>\n>\n> ________________________________\n> From: Frank Scholten <frank@frankscholten.nl>\n> To: user@mahout.apache.org; Vikas Pandya <vikasdp@yahoo.com>\n> Sent: Thursday, January 19, 2012 4:24 AM\n> Subject: Re: How to present mahout cluster in combination with Solr results\n>\n> Hi Vikas,\n>\n> I suggest indexing the cluster label, cluster size and\n> cluster-document mappings so you can use that information to build a\n> tag cloud of your data. Checkout this presentation\n> http://java.dzone.com/videos/configuring-mahout-clustering\n>\n> Cheers,\n>\n> Frank\n>\n> On Thu, Jan 19, 2012 at 4:18 AM, Vikas Pandya <vikasdp@yahoo.com> wrote:\n>> Hello,\n>>\n>> I have successfully created vectors from reading my existing Solr Index.\n>> Then created sequenceFile and mahout clusters from it. As I understand that\n>> currently solr and mahout clustering aren't integrated, what's the best way\n>> to represent mahout clusters to the user? Mine is a search application which\n>> renders results by querying solr index. Now I need to incorporate Mahout\n>> created clusters in the result. While Solr-Mahout integration isn't there\n>> yet, what's the best alternative way to represent this info?\n>>\n>> Thanks,\n>\n\n \n
<4F19AB49.2060000@apache.org>	Sebastian Schelter 	ssc@apache.org	Re: Question on RowSimilarityJob	2012-01-20T17:58:33Z	<1327077489.12746.YahooMailNeo@web39403.mail.mud.yahoo.com>	Hi,\n\n'maxSimilaritiesPerRow' denotes the maximum number of similar rows\n(documents in your use case) to keep per document.\n'excludeSelfSimilarity' means that rows (documents) should not be\ncompared to themselves.\n\nSry for the lack of documentation, RowSimilarityJob was originally only\nan internal job for the recommendation code. I'll try to add something\non the wiki in the next days.\n\n--sebastian\n\n\nOn 20.01.2012 17:38, Suneel Marthi wrote:\n> I am working on determining document similarity of a corpus I am working with using RowSimilarity.\n> \n> Questions:-\n> \n> a) What do the parameters - 'maxSimilaritiesPerRow' and 'excludeSelfSimilarity' mean?\n> b) Are there any docs available on RowSimilarityJob available, this is the best I could find on Sebastian's blog - http://ssc.io/rowsimilarityjob-on-steroids/ .\n> \n> c) Also do we have any docs on RowIdJob ?\n> \n> Thanks and Regards,\n> Suneel\n> \n\n\n \n
<CA+fkeAFHfMLeSxYy5toVouakERWTYnss4PJRWkNDuyD+BBzfQw@mail.gmail.com>	jams gost 	jamsgost@gmail.com	Mahout Taste Deployment On Hadoop	2012-01-20T20:45:25Z		Hello  Mahout Users,\n\n      I have Developed Mahout Taste Recommender with the referring Mahout\nIn Actipon on my system.So, after getting mahoutapp-1.0-SNAPSHOT.jar.How\ncan I execute on Hadoop 0.19. Can any one have tried this?\n\nThanks\n\nJams\n \n\n
<CAJwFCa0w7E5g4huU5958eYyC0nJkcBK1Gr+AdZmRNYm2XCDxFw@mail.gmail.com>	Ted Dunning 	ted.dunning@gmail.com	Re: Mahout Taste Deployment On Hadoop	2012-01-20T21:21:59Z	<CA+fkeAFHfMLeSxYy5toVouakERWTYnss4PJRWkNDuyD+BBzfQw@mail.gmail.com>	I doubt if it will work on Hadoop 0.19.  Mahout requires 0.20 and pretty\nmuch always has.  Changing that will be difficult to check even if it isn't\ndifficult to do.\n\nIn any case, you should probably get off of 0.19 as soon as possible as\nwell since there are known stability problems with that version.  0.20 has\nbeen the stable release for a very long time.\n\nOn Fri, Jan 20, 2012 at 12:45 PM, jams gost <jamsgost@gmail.com> wrote:\n\n> Hello  Mahout Users,\n>\n>      I have Developed Mahout Taste Recommender with the referring Mahout\n> In Actipon on my system.So, after getting mahoutapp-1.0-SNAPSHOT.jar.How\n> can I execute on Hadoop 0.19. Can any one have tried this?\n>\n> Thanks\n>\n> Jams\n>\n \n\n
<1327096447.12958.YahooMailNeo@web39402.mail.mud.yahoo.com>	Suneel Marthi 	suneel_marthi@yahoo.com	Help with MatrixDumper	2012-01-20T21:54:07Z		Mahout Version: 0.6-SNAPSHOT from trunk\n\n\nI convert my vectors (generated by seq2sparse) to matrix using the RowIdJob.\n\nRowIdJob creates a matrix (IntWritable, VectorWritable) and a docIndex (IntWritable, Text).\n\n\nI am now trying to dump the matrix generated using MatrixDumper and I see this error:-\n\nException in thread "main" java.lang.ClassCastException: org.apache.mahout.math.VectorWritable cannot be cast to org.apache.mahout.math.MatrixWritable\n    at org.apache.mahout.utils.MatrixDumper.exportCSV(MatrixDumper.java:70)\n    at org.apache.mahout.utils.MatrixDumper.run(MatrixDumper.java:63)\n    at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\n    at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:79)\n    at org.apache.mahout.utils.MatrixDumper.main(MatrixDumper.java:50)\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n    at java.lang.reflect.Method.invoke(Method.java:597)\n    at org.apache.hadoop.util.ProgramDriver$ProgramDescription.invoke(ProgramDriver.java:68)\n    at org.apache.hadoop.util.ProgramDriver.driver(ProgramDriver.java:139)\n    at org.apache.mahout.driver.MahoutDriver.main(MahoutDriver.java:188)\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n    at java.lang.reflect.Method.invoke(Method.java:597)\n    at org.apache.hadoop.util.RunJar.main(RunJar.java:156)\n\n\nObviously the MatrixDumper is expecting to retrieve a MatrixWritable, but what's being written into the matrix is a VectorWritable.\n\nAm I using MatrixDumper the right way or do we have an issue here?\n \n\n
<1327096923.98475.YahooMailNeo@web39405.mail.mud.yahoo.com>	Suneel Marthi 	suneel_marthi@yahoo.com	Re: Question on RowSimilarityJob	2012-01-20T22:02:03Z		Thanks for the reply Sebastian.\n\n\nFor the task I am working on - 'determine similar documents in a collection' - would RowSimilarity be the right approach?\n\n\n \n\n\n\n________________________________\n From: Sebastian Schelter <ssc@apache.org>\nTo: user@mahout.apache.org \nSent: Friday, January 20, 2012 12:58 PM\nSubject: Re: Question on RowSimilarityJob\n \nHi,\n\n'maxSimilaritiesPerRow' denotes the maximum number of similar rows\n(documents in your use case) to keep per document.\n'excludeSelfSimilarity' means that rows (documents) should not be\ncompared to themselves.\n\nSry for the lack of documentation, RowSimilarityJob was originally only\nan internal job for the recommendation code. I'll try to add something\non the wiki in the next days.\n\n--sebastian\n\n\nOn 20.01.2012 17:38, Suneel Marthi wrote:\n> I am working on determining document similarity of a corpus I am working with using RowSimilarity.\n> \n> Questions:-\n> \n> a) What do the parameters - 'maxSimilaritiesPerRow' and 'excludeSelfSimilarity' mean?\n> b) Are there any docs available on RowSimilarityJob available, this is the best I could find on Sebastian's blog - http://ssc.io/rowsimilarityjob-on-steroids/ .\n> \n> c) Also do we have any docs on RowIdJob ?\n> \n> Thanks and Regards,\n> Suneel\n> \n\n
<CAK9GPcQdxWLKCzWJPAwynacCA8iOJ6zJT9rnAa+A7M0EzZTewg@mail.gmail.com>	Lance Norskog 	goksron@gmail.com	Re: Help with MatrixDumper	2012-01-20T22:41:36Z	<1327096447.12958.YahooMailNeo@web39402.mail.mud.yahoo.com>	Dumping vectors is a separate job called 'vectordump'.\n\nOn Fri, Jan 20, 2012 at 1:54 PM, Suneel Marthi <suneel_marthi@yahoo.com> wrote:\n> Mahout Version: 0.6-SNAPSHOT from trunk\n>\n>\n> I convert my vectors (generated by seq2sparse) to matrix using the RowIdJob.\n>\n> RowIdJob creates a matrix (IntWritable, VectorWritable) and a docIndex (IntWritable, Text).\n>\n>\n> I am now trying to dump the matrix generated using MatrixDumper and I see this error:-\n>\n> Exception in thread "main" java.lang.ClassCastException: org.apache.mahout.math.VectorWritable cannot be cast to org.apache.mahout.math.MatrixWritable\n>     at org.apache.mahout.utils.MatrixDumper.exportCSV(MatrixDumper.java:70)\n>     at org.apache.mahout.utils.MatrixDumper.run(MatrixDumper.java:63)\n>     at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\n>     at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:79)\n>     at org.apache.mahout.utils.MatrixDumper.main(MatrixDumper.java:50)\n>     at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n>     at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n>     at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n>     at java.lang.reflect.Method.invoke(Method.java:597)\n>     at org.apache.hadoop.util.ProgramDriver$ProgramDescription.invoke(ProgramDriver.java:68)\n>     at org.apache.hadoop.util.ProgramDriver.driver(ProgramDriver.java:139)\n>     at org.apache.mahout.driver.MahoutDriver.main(MahoutDriver.java:188)\n>     at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n>     at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n>     at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n>     at java.lang.reflect.Method.invoke(Method.java:597)\n>     at org.apache.hadoop.util.RunJar.main(RunJar.java:156)\n>\n>\n> Obviously the MatrixDumper is expecting to retrieve a MatrixWritable, but what's being written into the matrix is a VectorWritable.\n>\n> Am I using MatrixDumper the right way or do we have an issue here?\n\n\n\n-- \nLance Norskog\ngoksron@gmail.com\n\n \n
<CAK9GPcRcqRBo6Cc_4sooM=pY_f_7znJuCR-j6spY0z1RrNpwtQ@mail.gmail.com>	Lance Norskog 	goksron@gmail.com	Re: Question on RowSimilarityJob	2012-01-20T22:48:17Z	<1327096923.98475.YahooMailNeo@web39405.mail.mud.yahoo.com>	This can be done as a recommender or with clustering. As a\nrecommender, you would say, "find similar documents". In clustering,\nyou can take one document and find its neighbors in the cluster graph.\n\nHow much data do you have? An important point here is that most of\nMahout's recommender algorithms are in-memory programs. There is only\none Hadoop-based recommender. There are several Hadoop-based\nclustering algorithms.\n\nOn Fri, Jan 20, 2012 at 2:02 PM, Suneel Marthi <suneel_marthi@yahoo.com> wrote:\n> Thanks for the reply Sebastian.\n>\n>\n> For the task I am working on - 'determine similar documents in a collection' - would RowSimilarity be the right approach?\n>\n>\n>\n>\n>\n>\n> ________________________________\n>  From: Sebastian Schelter <ssc@apache.org>\n> To: user@mahout.apache.org\n> Sent: Friday, January 20, 2012 12:58 PM\n> Subject: Re: Question on RowSimilarityJob\n>\n> Hi,\n>\n> 'maxSimilaritiesPerRow' denotes the maximum number of similar rows\n> (documents in your use case) to keep per document.\n> 'excludeSelfSimilarity' means that rows (documents) should not be\n> compared to themselves.\n>\n> Sry for the lack of documentation, RowSimilarityJob was originally only\n> an internal job for the recommendation code. I'll try to add something\n> on the wiki in the next days.\n>\n> --sebastian\n>\n>\n> On 20.01.2012 17:38, Suneel Marthi wrote:\n>> I am working on determining document similarity of a corpus I am working with using RowSimilarity.\n>>\n>> Questions:-\n>>\n>> a) What do the parameters - 'maxSimilaritiesPerRow' and 'excludeSelfSimilarity' mean?\n>> b) Are there any docs available on RowSimilarityJob available, this is the best I could find on Sebastian's blog - http://ssc.io/rowsimilarityjob-on-steroids/ .\n>>\n>> c) Also do we have any docs on RowIdJob ?\n>>\n>> Thanks and Regards,\n>> Suneel\n>>\n\n\n\n-- \nLance Norskog\ngoksron@gmail.com\n\n \n
<17a13b4c-e7d8-4055-92d2-7e3740d2e7d7@mail.cs.ucla.edu>	Daniel Quach 	danquach@cs.ucla.edu	Re: mahout jar in classpath	2012-01-30T22:15:38Z	<7aae9d40-5b57-479c-bbd2-c71d0fd1b70f@mail.cs.ucla.edu>	herp derp, I thought I was using the wrong jar in the classpath, actually I just forgot to put the import into my application. Sorry everyone, still rusty with the java!\n\n----- Original Message -----\nFrom: "Daniel Quach" <danquach@cs.ucla.edu>\nTo: user@mahout.apache.org, "Suneel Marthi" <suneel_marthi@yahoo.com>\nSent: Monday, January 30, 2012 2:04:48 PM\nSubject: Re: mahout jar in classpath\n\nIndeed I did build them from trunk.\n\nRunning that command, it does show DenseVector.class in the jar.\n\nI wonder if I perhaps entered the javac command incorrectly?\nIt looks like this:\n\njavac -cp /usr/local/hadoop/hadoop-core-0.20.203.0.jar:/usr/local/mahout/math/target/mahout-math-0.6.SNAPSHOT.jar -d makevector_classes/ MakeVector.java\n\n\n----- Original Message -----\nFrom: "Suneel Marthi" <suneel_marthi@yahoo.com>\nTo: user@mahout.apache.org\nSent: Monday, January 30, 2012 1:58:21 PM\nSubject: Re: mahout jar in classpath\n\nDenseVector should be in mahout-math jar.\n Did you build your jars from trunk?\n\nWhat do you see when you run this?\n\njar tvf mahout-math-0.6-SNAPSHOT.jar |  grep DenseVector.class\n\n\n\n________________________________\n From: Daniel Quach <danquach@cs.ucla.edu>\nTo: user@mahout.apache.org\nSent: Monday, January 30, 2012 4:52 PM\nSubject: mahout jar in classpath\n\nI am trying to figure out which mahout jar file to include in the classpath if I want to use a DenseVector.\n\nI tried mahout-math-0.6-SNAPSHOT.jar and mahout-core-0.6-SNAPSHOT.jar\n\nBoth times, javac tells me that the DenseVector symbol cannot be found.\n\n \n
<AA267B37-6B7E-44A1-BB39-A9FF89384FFC@gmail.com>	Ted Dunning 	ted.dunning@gmail.com	Re: mahout jar in classpath	2012-01-30T22:52:48Z	<17a13b4c-e7d8-4055-92d2-7e3740d2e7d7@mail.cs.ucla.edu>	It might be helpful to use some tools like maven to help with dependencies. \n\nIn addition there are some very good development tools like intellij that help with the import sorts of issues. \n\nSent from my iPhone\n\nOn Jan 30, 2012, at 14:15, Daniel Quach <danquach@cs.ucla.edu> wrote:\n\n> herp derp, I thought I was using the wrong jar in the classpath, actually I just forgot to put the import into my application. Sorry everyone, still rusty with the java!\n> \n> ----- Original Message -----\n> From: "Daniel Quach" <danquach@cs.ucla.edu>\n> To: user@mahout.apache.org, "Suneel Marthi" <suneel_marthi@yahoo.com>\n> Sent: Monday, January 30, 2012 2:04:48 PM\n> Subject: Re: mahout jar in classpath\n> \n> Indeed I did build them from trunk.\n> \n> Running that command, it does show DenseVector.class in the jar.\n> \n> I wonder if I perhaps entered the javac command incorrectly?\n> It looks like this:\n> \n> javac -cp /usr/local/hadoop/hadoop-core-0.20.203.0.jar:/usr/local/mahout/math/target/mahout-math-0.6.SNAPSHOT.jar -d makevector_classes/ MakeVector.java\n> \n> \n> ----- Original Message -----\n> From: "Suneel Marthi" <suneel_marthi@yahoo.com>\n> To: user@mahout.apache.org\n> Sent: Monday, January 30, 2012 1:58:21 PM\n> Subject: Re: mahout jar in classpath\n> \n> DenseVector should be in mahout-math jar.\n>  Did you build your jars from trunk?\n> \n> What do you see when you run this?\n> \n> jar tvf mahout-math-0.6-SNAPSHOT.jar |  grep DenseVector.class\n> \n> \n> \n> ________________________________\n> From: Daniel Quach <danquach@cs.ucla.edu>\n> To: user@mahout.apache.org\n> Sent: Monday, January 30, 2012 4:52 PM\n> Subject: mahout jar in classpath\n> \n> I am trying to figure out which mahout jar file to include in the classpath if I want to use a DenseVector.\n> \n> I tried mahout-math-0.6-SNAPSHOT.jar and mahout-core-0.6-SNAPSHOT.jar\n> \n> Both times, javac tells me that the DenseVector symbol cannot be found.\n\n \n
<1327964003.66740.YahooMailNeo@web130206.mail.mud.yahoo.com>	Stuart Smith 	stu24mail@yahoo.com	Re: Diagnosing naive bayes results - solved ???	2012-01-30T22:53:23Z	<1327886421.9274.androidMobile@web130206.mail.mud.yahoo.com>	\n\nOk, I changed the \n\n\n-method \n\n\non the testclassifier from \n\n\nsequential -> mapreduce \n\n\n(actually, I just got rid of it, and it defaults to mapreduce).\n\nAnd then I got a properly filled out confusion matrix.\n\nDoes \n\n\n-method sequential\n\nnot understand hdfs?\n\nYes, I'll comb through the TestClassifier code eventually, but thought I'd put up the answer for all to see immediately.\n\nBut, same data, same model, 'sequential' is all zeros, 'mapreduce'.. nice, filled out confusion matrix..\n\n(and sorry for the ee cummings-esque line breaks..., just trying to break up the important lines)\n\n\nTake care,\n  -stu\n\n\n\n________________________________\n From: Stuart Smith <stu24mail@yahoo.com>\nTo: "user@mahout.apache.org" <user@mahout.apache.org> \nSent: Sunday, January 29, 2012 5:20 PM\nSubject: Re: Diagnosing naive bayes results - now I'm really stumped\n \nSent an email to your gmail account... Not comfortable sending it to the whole list..  thanks!\n\nTake care,\n  -stu \n\n
<CAJ1vTB+FkQPufj5HZqU6+YDtCBhUpwX30G9ZtO70xHk_5euVrg@mail.gmail.com>	gaurav redkar 	gauravredkar@gmail.com	Re: Help regarding ClusterOutputPostProcessor	2012-01-31T04:25:35Z	<4F201E05.7040001@windwardsolutions.com>	Hello. As Jeff mentioned, i created a JIRA issue. Kindly check out\nMAHOUT-966 <https://issues.apache.org/jira/browse/MAHOUT-966>  and share\nyour inputs.\n\nThanks,\nGaurav\n\nOn Wed, Jan 25, 2012 at 8:51 PM, Jeff Eastman <jdog@windwardsolutions.com>wrote:\n\n> Mean Shift accumulates the pointIds of every point assigned to a cluster,\n> so I would expect n= to be correct in the cluster dumper output. It is most\n> likely the postprocessor is misbehaving. Please create a JIRA and attach\n> your dataset and we will take a look at it.\n>\n> It would also be useful for you to include the exact CLI commands which\n> you used to duplicate this problem.\n>\n>\n> On 1/25/12 2:41 AM, gaurav redkar wrote:\n>\n>>  Hello,\n>>\n>> I was able to rectify the afore-mentioned problem after i implemented a\n>> custom partitioner instead of using the default hash partitioner.  I have\n>> another issue though. After running the post processor the number of\n>> points\n>> that each cluster contains is not matching the number of points each\n>> cluster should contain as stated by clusterdumper.\n>>\n>>\n>> MSV-287{ n=90 c=[0.05195, 0.05675, 0.07151, 0.05713, 0.06946,...}\n>>\n>> MSV-145{ n=90 c=[0.93685, 0.93071, 0.93641, 0.94629, 0.94409,..}\n>> the n mentioned in clusters-n-final against each cluster is different from\n>> the number of points actually contained in d directory for each cluster.\n>> Any idea why is this happening ...?\n>>\n>> PS: the dataset on which i tested the algorithm has 1000 records with 200\n>> attributes per record. I can share the dataset that i have used if needed.\n>>\n>> Thanks,\n>>\n>> Gaurav\n>>\n>> On Fri, Jan 6, 2012 at 6:12 PM, Paritosh Ranjan<pranjan@xebia.com>\n>>  wrote:\n>>\n>>  ClusterOutputProcessorDriver has options to run either sequentially or\n>>> in\n>>> a mapreduce way.\n>>>\n>>> If the clustering was done sequetially, then ClusterOutputProcessor\n>>> should\n>>> be run sequentially, and if the clustering was done in a mapreduce way,\n>>> then run the ClusterOutputPostProcessor with option mapreduce=true.\n>>>\n>>> If you have already tried this, and its still now working, then filing a\n>>> bug (as Lance mentioned) would be appropriate.\n>>>\n>>>\n>>> On 06-01-2012 17:18, gaurav redkar wrote:\n>>>\n>>>   Hello,\n>>>> wen I ran the ClusterOutputPostProcessor on synthetic_control_data in\n>>>> mapreduce mode, I observed that one directory contained points\n>>>> belonging to\n>>>> 2 other clusters and the directories relating to those 2 clusters were\n>>>> not\n>>>> created as their "part- *" files were empty and the function "**\n>>>> movePartFilesToRespectiveDirec****tories()" was not able to create the\n>>>>\n>>>> directories to put them into. I have converted the sequence file\n>>>> containing\n>>>> the points belonging to those 3 clusters into text file(by changing the\n>>>> output format to TextOutputFormat). Kindly find the attached part-file\n>>>> which can be viewed.\n>>>> Any suggestions as to why this might be happening...?\n>>>> Note: The program runs fine in sequential mode.\n>>>> Thanks.\n>>>>\n>>>>\n>>>> No virus found in this message.\n>>>> Checked by AVG - www.avg.com<http://www.avg.com**>\n>>>> Version: 10.0.1416 / Virus Database: 2109/4125 - Release Date: 01/05/12\n>>>>\n>>>>\n>>>>\n>\n \n\n
<CAPzrt-34JcCngRy=C-daAC=13sgckmeoXknxk51qv-g1Aj4xqA@mail.gmail.com>	Ramprakash Ramamoorthy 	youngestachiever@gmail.com	Naive Bayes Classifier - Scores	2012-01-31T04:52:30Z		Dear all,\n\n             I am performing a sentiment analysis using the naive bayes\nclassifier on apache mahout. Every time when I get the result, I get a\ncategory and a score corresponding to the score.\n\n             Can some one here enlighten me on the score? Like what is the\nmaximum score for a given category(I have two categories - positive and\nnegative). So based on the score can I categorize them as very\npositive,very negative etc. Any input regarding the same would be helpful.\nThank you.\n\n-- \nWith Thanks and Regards,\nRamprakash Ramamoorthy,\nSASTRA University,\n+91 9626975420\n \n\n
<CAJwFCa37S0n9qiYz5PzOPTnqES2yNcThd53mkPK8WWjLmFkSNg@mail.gmail.com>	Ted Dunning 	ted.dunning@gmail.com	Re: Naive Bayes Classifier - Scores	2012-01-31T06:19:24Z	<CAPzrt-34JcCngRy=C-daAC=13sgckmeoXknxk51qv-g1Aj4xqA@mail.gmail.com>	Did you read this paper?\n\nhttp://qwone.com/~jason/papers/sm-thesis.pdf\n\nOn Mon, Jan 30, 2012 at 8:52 PM, Ramprakash Ramamoorthy <\nyoungestachiever@gmail.com> wrote:\n\n> Dear all,\n>\n>             I am performing a sentiment analysis using the naive bayes\n> classifier on apache mahout. Every time when I get the result, I get a\n> category and a score corresponding to the score.\n>\n>             Can some one here enlighten me on the score? Like what is the\n> maximum score for a given category(I have two categories - positive and\n> negative). So based on the score can I categorize them as very\n> positive,very negative etc. Any input regarding the same would be helpful.\n> Thank you.\n>\n> --\n> With Thanks and Regards,\n> Ramprakash Ramamoorthy,\n> SASTRA University,\n> +91 9626975420\n>\n \n\n
<CAJwFCa2cWOBOUnrNjNM0zdeZ9Oicr6vb=bzWvNbMUDsNLOa6xA@mail.gmail.com>	Ted Dunning 	ted.dunning@gmail.com	Re: Naive Bayes Classifier - Scores	2012-01-31T06:19:51Z	<CAJwFCa37S0n9qiYz5PzOPTnqES2yNcThd53mkPK8WWjLmFkSNg@mail.gmail.com>	Or this one?\n\nhttp://people.csail.mit.edu/jrennie/papers/icml03-nb.pdf\n\nOn Mon, Jan 30, 2012 at 10:19 PM, Ted Dunning <ted.dunning@gmail.com> wrote:\n\n> Did you read this paper?\n>\n> http://qwone.com/~jason/papers/sm-thesis.pdf\n>\n>\n> On Mon, Jan 30, 2012 at 8:52 PM, Ramprakash Ramamoorthy <\n> youngestachiever@gmail.com> wrote:\n>\n>> Dear all,\n>>\n>>             I am performing a sentiment analysis using the naive bayes\n>> classifier on apache mahout. Every time when I get the result, I get a\n>> category and a score corresponding to the score.\n>>\n>>             Can some one here enlighten me on the score? Like what is the\n>> maximum score for a given category(I have two categories - positive and\n>> negative). So based on the score can I categorize them as very\n>> positive,very negative etc. Any input regarding the same would be helpful.\n>> Thank you.\n>>\n>> --\n>> With Thanks and Regards,\n>> Ramprakash Ramamoorthy,\n>> SASTRA University,\n>> +91 9626975420\n>>\n>\n>\n \n\n
<05A1978BA4F14540B55E731C54B393F609394258@MAIL-MBX2.ad.renci.org>	Keary Cavin 	kcavin@renci.org	RE: status of hadoop hidden markov model in mahout	2012-01-31T19:14:38Z	<CAFk0LreRuZS480bBT0rVEvYt8ww=KYow-Ukzm=qAAr2qc7PgOg@mail.gmail.com>	Hi Dhruv and Manuel,\n\nThank you for your responses.  I apologize for my late reply.\n\nOne of the immediate goals of our project is to perform imputation over several hundred genomes.  In our next round of imputations, we anticipate incoming data for several thousand genomes.\n\nI don't have a ready answer for the question about the number of observed and hidden states in the HMM.  We do know the best imputation window size our current code supports is between 1 and 5 million base pairs.\n\nWe have a meeting scheduled with the authors of the imputation code we are using and we want to get the details on the parameters and implementation details of its Hidden Markov Model.\n\nWhen I have more information about the algorithm, I will send it to you.\n\nDhruv, I downloaded the MAHOUT-627 patch and applied the files to the current mahout release.  I'll let you know when I have questions.\n\nThank you very much,\n\nKeary\n\n-----Original Message-----\nFrom: dhruv21@gmail.com [mailto:dhruv21@gmail.com] On Behalf Of Dhruv Kumar\nSent: Wednesday, January 25, 2012 5:15 PM\nTo: user@mahout.apache.org\nSubject: Re: status of hadoop hidden markov model in mahout\n\nMAHOUT-627 provides a patch for training the HMM using the Baum Welch Algorithm on MapReduce. If you would like to give it a spin, I will be glad to help you with any questions you may have about its use.\n\nCan you provide more details about your project? I am particularly interested in the number of observed and hidden states you have in your HMM model and the total training set size for understanding your scalability requirements.\n\nOn Wed, Jan 25, 2012 at 1:08 PM, Manuel Blechschmidt < Manuel.Blechschmidt@gmx.de> wrote:\n\n> Hi Keary,\n>\n> On 25.01.2012, at 21:47, Keary Cavin wrote:\n>\n> > Hello,\n> >\n> > We are investigating Mahout as a scalable solution for a hidden \n> > markov\n> model genetic imputation problem we'd like to run on our Hadoop cluster.\n> >\n> > Does the infrastructure exist to run the Mahout HMM code through Hadoop?\n>\n> Actually in the core Mahout SVN there is not parallel implementation \n> of any aspects of the HMM approach yet. Nevertheless Dhruv Kumar \n> implemented during  a google summar of code a paralyzed Baum Welch algorithm.\n>\n> There was a discussion 2 days ago about HMM:\n>\n> http://mail-archives.apache.org/mod_mbox/mahout-user/201201.mbox/%3C5D\n> 1FC0E56861D84086FE034AFD1B223D3CD101E9@mbx1.hosted.exchange-login.net%\n> 3E\n>\n> Here is the patch:\n> https://issues.apache.org/jira/browse/MAHOUT-627\n>\n> Keep in mind that executing machine learning in parallel is on going \n> research.\n>\n> >\n> > Thanks very much,\n> >\n> > Keary\n>\n> /Manuel\n>\n> --\n> Manuel Blechschmidt\n> Dortustr. 57\n> 14467 Potsdam\n> Mobil: 0173/6322621\n> Twitter: http://twitter.com/Manuel_B\n>\n>\n\n \n
<1328046020.48196.YahooMailNeo@web39403.mail.mud.yahoo.com>	Suneel Marthi 	suneel_marthi@yahoo.com	Re: Question on RowSimilarityJob	2012-01-31T21:40:20Z	<4F19AB49.2060000@apache.org>	Sebastian,\n\nQuestion on the RowSimilarity job.\n\na) I created sequence files from my document corpus.\nb) Created vectors from sequence files with ngrams = 3, normalization = 2, min document frequency = 1 and minimum support = 1\nc) Ran the RowId job on the vectors generated in (2) - this gives me an M * N matrix where M = number of  documents in my collection, N = cardinality of the vector - Correct?\n\nd) Ran the Rowsimilarity job on the matrix generated in (3) with Cosine Similarity measure and 'N' as the number of columns - this gives me an M * R matrix where  R < N. \n\n\n    I am not sure I completely understand as to what's happening in the RowSimilarity Job, I did read the paper at http://www.umiacs.umd.edu/~jimmylin/publications/Elsayed_etal_ACL2008_short.pdf  and have been staring at your whiteboard (http://ssc.io/wp-content/uploads/2011/08/v2.jpg) for a while to understand what's happening, but guess I need some help.\n\nI am willing to put some docs on the wiki for RowSimilarityJob once I am done.\n\nThanks for your help.\n\n\n\n________________________________\n From: Sebastian Schelter <ssc@apache.org>\nTo: user@mahout.apache.org \nSent: Friday, January 20, 2012 12:58 PM\nSubject: Re: Question on RowSimilarityJob\n \nHi,\n\n'maxSimilaritiesPerRow' denotes the maximum number of similar rows\n(documents in your use case) to keep per document.\n'excludeSelfSimilarity' means that rows (documents) should not be\ncompared to themselves.\n\nSry for the lack of documentation, RowSimilarityJob was originally only\nan internal job for the recommendation code. I'll try to add something\non the wiki in the next days.\n\n--sebastian\n\n\nOn 20.01.2012 17:38, Suneel Marthi wrote:\n> I am working on determining document similarity of a corpus I am working with using RowSimilarity.\n> \n> Questions:-\n> \n> a) What do the parameters - 'maxSimilaritiesPerRow' and 'excludeSelfSimilarity' mean?\n> b) Are there any docs available on RowSimilarityJob available, this is the best I could find on Sebastian's blog - http://ssc.io/rowsimilarityjob-on-steroids/ .\n> \n> c) Also do we have any docs on RowIdJob ?\n> \n> Thanks and Regards,\n> Suneel\n> \n\n
